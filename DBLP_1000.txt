the combination of flit buffer flow control methods and latency insensitive protocols is an effective solution for networks on chip noc since they both rely on backpressure the two techniques are easy to combine while offering complementary advantages low complexity of router design and the ability to cope with long communication channels via automatic wire pipelining we study various alternative implementations of this idea by considering the combination of three different types of flit buffer flow control methods and two different classes of channel repeaters based respectively on flip flops and relay stations we characterize the area and performance of the two most promising alternative implementations for nocs by completing the rtl design and logic synthesis of the repeaters and routers for different channel parallelisms finally we derive high level abstractions of our circuit designs and we use them to perform system level simulations under various scenarios for two distinct noc topologies and various applications based on our comparative analysis and experimental results we propose noc design approach that combines the reduction of the router queues to minimum size with the distribution of flit buffering onto the channels this approach provides precious flexibility during the physical design phase for many nocs particularly in those systems on chip that must be designed to meet tight constraint on the target clock frequency
we present an easy to use model that addresses the practical issues in designing bus based shared memory multiprocessor systems the model relates the shared bus width bus cycle time cache memory the features of program execution and the number of processors on shared bus to metric called request utilization the request utilization is treated as the scaling factor for the effective average waiting processors in computing the queuing delay cycles simulation study shows that the model performs very well in estimating the shared bus response time using the model system designer can quickly decide the number of the processors that shared bus is able to support effectively the size of the cache memory system should use and the bus cycle time that the main memory system should provide with the model we show that the design favors caching the requests for contention based medium instead of speeding up the transfers although the same performance can be respectively achieved by the two techniques in contention free situation
software product lines spls are used to create tailor made software products by managing and composing reusable assets generating software product from the assets of an spl is possible statically before runtime or dynamically at load time or runtime both approaches have benefits and drawbacks with respect to composition flexibility performance and resource consumption which type of composition is preferable should be decided by taking the application scenario into account current tools and languages however force programmer to decide between static and dynamic composition during development in this paper we present an approach that employs code generation to support static and dynamic composition of features of single code base we offer an implementation on top of featurec an extension of the programming language that supports software composition based on features to simplify dynamic composition and to avoid creation of invalid products we furthermore provide means to validate the correctness of composition at runtime automatically instantiate spls in case of stand alone applications and automatically apply interaction code of crosscutting concerns
experience has proved that interactive applications delivered through digital tv must provide personalized information to the viewers in order to be perceived as valuable service due to the limited computational power of dtv receivers either domestic set top boxes or mobile devices most of the existing systems have opted to place the personalization engines in dedicated servers assuming that return channel is always available for bidirectional communication however in domain where most of the information is transmitted through broadcast there are still many cases of intermittent sporadic or null access to return channel in such situations it is impossible for the servers to learn who is watching tv at the moment and so the personalization features become unavailable to solve this problem without sacrificing much personalization quality this paper introduces solutions to run downsized semantic reasoning process in the dtv receivers supported by pre selection of material driven by audience stereotypes in the head end evaluation results are presented to prove the feasibility of this approach and also to assess the quality it achieves in comparison with previous ones
model based testing techniques play vital role in producing quality software however compared to the testing of functional requirements these techniques are not prevalent that much in testing software security this paper presents model based approach to automatic testing of attack scenarios an attack testing framework is proposed to model attack scenarios and test the system with respect to the modeled attack scenarios the techniques adopted in the framework are applicable in general to the systems where the potential attack scenarios can be modeled in formalism based on extended abstract state machines the attack events ie attack test vectors chosen from the attacks happening in real world are converted to the test driver specific events ready to be tested against the attack signatures the proposed framework is implemented and evaluated using the most common attack scenarios the framework is useful to test software with respect to potential attacks which can significantly reduce the risk of security vulnerabilities
in this paper we address the problem of cache replacement for transcoding proxy caching transcoding proxy is proxy that has the functionality of transcoding multimedia object into an appropriate format or resolution for each client we first propose an effective cache replacement algorithm for transcoding proxy in general when new object is to be cached cache replacement algorithms evict some of the cached objects with the least profit to accommodate the new object our algorithm takes into account of the inter relationships among different versions of the same multimedia object and selects the versions to replace according to their aggregate profit which usually differs from simple summation of their individual profits as assumed in the existing algorithms it also considers cache consistency which is not considered in the existing algorithms we then present complexity analysis to show the efficiency of our algorithm finally we give extensive simulation results to compare the performance of our algorithm with some existing algorithms the results show that our algorithm outperforms others in terms of various performance metrics
distributed proof construction protocols have been shown to be valuable for reasoning about authorization decisions in open distributed environments such as pervasive computing spaces unfortunately existing distributed proof protocols offer only limited support for protecting the confidentiality of sensitive facts which limits their utility in many practical scenarios in this paper we propose distributed proof construction protocol in which the release of fact's truth value can be made contingent upon facts managed by other principals in the system we formally prove that our protocol can safely prove conjunctions of facts without leaking the truth values of individual facts even in the face of colluding adversaries and fact release policies with cyclical dependencies this facilitates the definition of context sensitive release policies that enable the conditional use of sensitive facts in distributed proofs
in this paper we introduce novel approach to image completion which we call structure propagation in our system the user manually specifies important missing structure information by extending few curves or line segments from the known to the unknown regions our approach synthesizes image patches along these user specified curves in the unknown region using patches selected around the curves in the known region structure propagation is formulated as global optimization problem by enforcing structure and consistency constraints if only single curve is specified structure propagation is solved using dynamic programming when multiple intersecting curves are specified we adopt the belief propagation algorithm to find the optimal patches after completing structure propagation we fill in the remaining unknown regions using patch based texture synthesis we show that our approach works well on number of examples that are challenging to state of the art techniques
as text documents are explosively increasing in the internet the process of hierarchical document clustering has been proven to be useful for grouping similar documents for versatile applications however most document clustering methods still suffer from challenges in dealing with the problems of high dimensionality scalability accuracy and meaningful cluster labels in this paper we will present an effective fuzzy frequent itemset based hierarchical clustering ihc approach which uses fuzzy association rule mining algorithm to improve the clustering accuracy of frequent itemset based hierarchical clustering fihc method in our approach the key terms will be extracted from the document set and each document is pre processed into the designated representation for the following mining process then fuzzy association rule mining algorithm for text is employed to discover set of highly related fuzzy frequent itemsets which contain key terms to be regarded as the labels of the candidate clusters finally these documents will be clustered into hierarchical cluster tree by referring to these candidate clusters we have conducted experiments to evaluate the performance based on classic hitech re reuters and wap datasets the experimental results show that our approach not only absolutely retains the merits of fihc but also improves the accuracy quality of fihc
this paper presents novel scheme for maintaining accurate information about distributed data in message passing programs we describe static single assignment ssa based algorithms to build up an intermediate representation of sequential program while targeting code generation for distributed memory machines employing the single program multiple data spmd model of programming this ssa based intermediate representation helps in variety of optimizations performed by our automatic parallelizing compiler paradigm which generates message passing programs and targets distributed memory machines in this paper we concentrate on the semantics and implementation of this ssa form for message passing programs while giving some examples of the kind of optimizations they enable we describe in detail the need for various kinds of merge functions to maintain the single assignment property of distributed data we give algorithms for placement and semantics of these merge functions and show how the requirements are substantially different owing to the presence of distributed data and arbitrary array addressing functions this scheme has been incorporated in our compiler framework which can use uniform methods to compile parallelize and optimize sequential program irrespective of the subscripts used in array addressing functions experimental results for number of benchmarks on an ibm sp show significant improvement in the total runtimes owing to some of the optimizations enabled by the ssa based intermediate representation we have observed up to around ndash reduction in total runtimes in our ssa based schemes compared to non ssa based schemes on processors
super resolution reconstruction of face image is the problem of reconstructing high resolution face image from one or more low resolution face images assuming that high and low resolution images share similar intrinsic geometries various recent super resolution methods reconstruct high resolution images based on weights determined from nearest neighbors in the local embedding of low resolution images these methods suffer disadvantages from the finite number of samples and the nature of manifold learning techniques and hence yield unrealistic reconstructed images to address the problem we apply canonical correlation analysis cca which maximizes the correlation between the local neighbor relationships of high and low resolution images we use it separately for reconstruction of global face appearance and facial details experiments using collection of frontal human faces show that the proposed algorithm improves reconstruction quality over existing state of the art super resolution algorithms both visually and using quantitative peak signal to noise ratio assessment
we consider substantial subset ofc named we develop mathematical specification for by formalizing its abstract syntax execution environment well typedness conditions and operational evaluation semantics based on this specification we prove that is type safe by showing that the execution of programs preserves the types up to subtype relationship
web site presents graph like spatial structure composed of pages connected by hyperlinks this structure may represent an environment in which situated agents associated to visitors of the web site user agents are positioned and moved in order to monitor their navigation this paper presents heterogeneous multi agent system supporting the collection of information related to user's behaviour in web site by specific situated reactive user agents the acquired information is then exploited by interface agents supporting advanced adaptive functionalities based on the history of user's movement in the web site environment interface agents also interact with user agents to acquire information on other visitors of the web site and to support context aware form of interaction among web site visitors
in many advanced database applications eg multimedia databases data objects are transformed into high dimensional points and manipulated in high dimensional space one of the most important but costly operations is the similarity join that combines similar points from multiple datasets in this paper we examine the problem of processing nearest neighbor similarity join knn join knn join between two datasets and returns for each point in its most similar points in we propose new index based knn join approach using the idistance as the underlying index structure we first present its basic algorithm and then propose two different enhancements in the first enhancement we optimize the original knn join algorithm by using approximation bounding cubes in the second enhancement we exploit the reduced dimensions of data space we conducted an extensive experimental study using both synthetic and real datasets and the results verify the performance advantage of our schemes over existing knn join algorithms
caches exploits locality of references to reduce memory access latencies and thereby improve processor performance when an operating system switches application task or performs other kernel services the assumption of locality may be violated because the instructions and data may no longer be in the cache when the preempted operation is resumed thus these operations have an additional cache interference cost that must be taken into account when calculating or estimating the performance and responsiveness of the systemin this paper we present simulation framework suitable for examining the cache interference cost in preemptive real time systems using this framework we measure the interference cost for operating system services and set of embedded benchmarksthe simulations show that there are significant performance gap between the best and worst case execution times even for simple hardware architectures also the worst case performance of some software modules was found to be more or less independent of the cache configuration these results can be used to get better understanding of the execution behavior of preemptive real time systems and can serve as guidelines for choosing suitable cache configurations
in this paper we consider the restless bandit problem which is one of the most well studied generalizations of the celebrated stochastic multi armed bandit problem in decision theory in its ultimate generality the restless bandit problem is known to be pspace hard to approximate to any non trivial factor and little progress has been made on this problem despite its significance in modeling activity allocation under uncertainty we make progress on this problem by showing that for an interesting and general subclass that we term monotone bandits surprisingly simple and intuitive greedy policy yields factor approximation such greedy policies are termed index policies and are popular due to their simplicity and their optimality for the stochastic multi armed bandit problem the monotone bandit problem strictly generalizes the stochastic multi armed bandit problem and naturally models multi project scheduling where the state of project becomes increasingly uncertain when the project is not scheduled we develop several novel techniques in the design and analysis of the index policy our algorithm proceeds by introducing novel balance constraint to the dual of well known lp relaxation to the restless bandit problem this is followed by structural characterization of the optimal solution by using both the exact primal as well as dual complementary slackness conditions this yields an interpretation of the dual variables as potential functions from which we derive the index policy and the associated analysis
we introduce straightforward robust and efficient algorithm for rendering high quality soft shadows in dynamic scenes each frame points in the scene visible from the eye are inserted into spatial acceleration structure shadow umbrae are computed by sampling the scene from the light at the image plane coordinates given by the stored points penumbrae are computed at the same set of points per silhouette edge in two steps first the set of points affected by given edge is estimated from the expected light view screen space bounds of the corresponding penumbra second the actual overlap between these points and the penumbra is computed analytically directly from the occluding geometry the umbral and penumbral sources of occlusion are then combined to determine the degree of shadow at the eye view pixel corresponding to each sample point an implementation of this algorithm for the larrabee architecture yields from to frames per second in simulation for scenes from modern game and produces significantly higher image quality than other recent methods in the real time domain
in this paper we present system using computational linguistic techniques to extract metadata for image access we discuss the implementation functionality and evaluation of an image catalogers toolkit developed in the computational linguistics for metadata building climb research project we have tested components of the system including phrase finding for the art and architecture domain functional semantic labeling using machine learning and disambiguation of terms in domain specific text vis vis rich thesaurus of subject terms geographic and artist names we present specific results on disambiguation techniques and on the nature of the ambiguity problem given the thesaurus resources and domain specific text resource with comparison of domain general resources and text our primary user group for evaluation has been the cataloger expert with specific expertise in the fields of painting sculpture and vernacular and landscape architecture
data mining is new technology that helps businesses to predict future trends and behaviours allowing them to make proactive knowledge driven decisions when data mining tools and techniques are applied on the data warehouse based on customer records they search for the hidden patterns and trends these can be further used to improve customer understanding and acquisition customer relationship management crm systems are adopted by the organisations in order to achieve success in the business and also to formulate business strategies which can be formulated based on the predictions given by the data mining tools basically three major areas of data mining research are identified implementation of crm systems evaluation criteria for data mining software and crm systems and methods to improve data quality for data mining the paper is concluded with proposed integrated model for the crm systems evaluation and implementation this paper focuses on these areas where there is need for more explorations and will provide framework for analysis of the data mining research for crm systems
in this paper we propose the novel concept of probabilistic design for multimedia embedded systems which is motivated by the challenge of how to design but not overdesign such systems while systematically incorporating performance requirements of multimedia application uncertainties in execution time and tolerance for reasonable execution failures unlike most present techniques that are based on either worst or average case execution times of application tasks where the former guarantees the completion of each execution but often leads to overdesigned systems and the latter fails to provide any completion guarantees the proposed probabilistic design method takes advantage of unique features mentioned above of multimedia systems to relax the rigid hardware requirements for software implementation and avoid overdesigning the system in essence this relaxation expands the design space and we further develop an off line on line minimum effort algorithm for quick exploration of the enlarged design space at early design stages this is the first step toward our goal of bridging the gap between real time analysis and embedded software implementation for rapid and economic multimedia system design it is our belief that the proposed method has great potential in reducing system resource while meeting performance requirements the experimental results confirm this as we achieve significant saving in system's energy consumption to provide statistical completion ratio guarantee ie the expected number of completions over large number of iterations is greater than given value
column statistics are an important element of cardinality estimation frameworks more accurate estimates allow the optimizer of rdbms to generate better plans and improve the overall system's efficiency this paper introduces filtered statistics which model value distribution over set of rows restricted by predicate this feature available in microsoft sql server can be used to handle column correlation as well as focus on interesting data ranges in particular it fits well for scenarios with logical subtables like flexible schema or multi tenant applications integration with the existing cardinality estimation infrastructure is presented
to keep up with the explosive internet packet processing demands modern network processors nps employ highly parallel multi threaded and multi core architecture in such parallel paradigm accesses to the shared variables in the external memory and the associated memory latency are contained in the critical sections so that they can be executed atomically and sequentially by different threads in the network processor in this paper we present novel program transformation that is used in the intel auto partitioning compiler for ixp to exploit the inherent finer grained parallelism of those critical sections using the software controlled caching mechanism available in the nps consequently those critical sections can be executed in pipelined fashion by different threads thereby effectively hiding the memory latency and improving the performance of network applications experimental results show that the proposed transformation provides impressive speedup up to and scalability up to threads of the performance for the real world network application lgbps efhernet core metro router
we show that if connected graph with nodes has conductance then rumour spreading also known as randomized broadcast successfully broadcasts message within log many rounds with high probability regardless of the source by using the push pull strategy the notation hides polylog factor this result is almost tight since there exists graph of nodes and conductance with diameter log if in addition the network satisfies some kind of uniformity condition on the degrees our analysis implies that both both push and pull by themselves successfully broadcast the message to every node in the same number of rounds
although database design tools have been developed that attempt to automate or semiautomate the design process these tools do not have the capability to capture common sense knowledge about business applications and store it in context specific manner as result they rely on the user to provide great deal of trivial details and do not function as well as human designer who usually has some general knowledge of how an application might work based on his or her common sense knowledge of the real world common sense knowledge could be used by database design system to validate and improve the quality of an existing design or even generate new designs this requires that context specific information about different database design applications be stored and generalized into information about specific application domains eg pharmacy daycare hospital university manufacturing such information should be stored at the appropriate level of generality in hierarchically structured knowledge base so that it can be inherited by the subdomains below for this to occur two types of learning must take place first knowledge about particular application domain that is acquired from specific applications within that domain are generalized into domain node eg entities relationships and attributes from various hospital applications are generalized to hospital node this is referred to as within domain learning second the information common to two or more related application domain nodes is generalized to higher level node for example knowledge from the car rental and video rental domains may be generalized to rental node this is called across domain learning this paper presents methodology for learning across different application domains based on distance measure the parameters used in this methodology were refined by testing on set of representative cases empirical testing provided further validation
in this paper we address the problem of clustering graphs in object oriented databases unlike previous studies which focused only on workload consisting of single operation this study tackles the problem when the workload is set of operations method and queries that occur with certain probability thus the goal is to minimize the expected cost of an operation in the workload while maintaining similarly low cost for each individual operation classto this end we present new clustering policy based on the nearest neighbor graph partitioning algorithm we then demonstrate that this policy provides considerable gains when compared to suite of well known clustering policies proposed in the literature our results are based on two widely referenced object oriented database benchmarks namely the tektronix hypermodel and oo
integrating several legacy software systems together is commonly performed with multiple applications of the adapter design pattern in oo languages such as java the integration is based on specifying bi directional translations between pairs of apis from different systems yet manual development of wrappers to implement these translations is tedious expensive and error prone in this paper we explore how models aspects and generative techniques can be used in conjunction to alleviate the implementation of multiple wrappers briefly the steps are the automatic reverse engineering of relevant concepts in apis to high level models the manual definition of mapping relationships between concepts in different models of apis using an ad hoc dsl the automatic generation of wrappers from these mapping specifications using aop this approach is weighted against manual development of wrappers using an industrial case study criteria are the relative code length and the increase of automation
higher order abstract syntax is simple technique for implementing languages with functional programming object variables and binders are implemented by variables and binders in the host language by using this technique one can avoid implementing common and tricky routines dealing with variables such as capture avoiding substitution however despite the advantages this technique provides it is not commonly used because it is difficult to write sound elimination forms such as folds or catamorphisms for higher order abstract syntax to fold over such data type one must either simultaneously define an inverse operation which may not exist or show that all functions embedded in the data type are parametric in this paper we show how first class polymorphism can be used to guarantee the parametricity of functions embedded in higher order abstract syntax with this restriction we implement library of iteration operators over data structures containing functionals from this implementation we derive fusion laws that functional programmers may use to reason about the iteration operator finally we show how this use of parametric polymorphism corresponds to the sch√ºrmann despeyroux and pfenning method of enforcing parametricity through modal types we do so by using this library to give sound and complete encoding of their calculus into system private char inline graphic mime subtype gif xlink schar private char this encoding can serve as starting point for reasoning about higher order structures in polymorphic languages
we introduce theoretical framework for discovering relationships between two database instances over distinct and unknown schemata this framework is grounded in the context of data exchange we formalize the problem of understanding the relationship between two instances as that of obtaining schema mapping so that minimum repair of this mapping provides perfect description of the target instance given the source instance we show that this definition yields intuitive results when applied on database instances derived from each other by basic operations we study the complexity of decision problems related to this optimality notion in the context of different logical languages and show that even in very restricted cases the problem is of high complexity
mining association rules is an important technique for discovering meaningful patterns in transaction databases many different measures of interestingness have been proposed for association rules however these measures fail to take the probabilistic properties of the mined data into account we start this paper with presenting simple probabilistic framework for transaction data which can be used to simulate transaction data when no associations are present we use such data and real world database from grocery outlet to explore the behavior of confidence and lift two popular interest measures used for rule mining the results show that confidence is systematically influenced by the frequency of the items in the left hand side of rules and that lift performs poorly to filter random noise in transaction data based on the probabilistic framework we develop two new interest measures hyper lift and hyper confidence which can be used to filter or order mined association rules the new measures show significantly better performance than lift for applications where spurious rules are problematic
in spite of impressive gains by pl fortran and cobol remain the languages in which most of the world's production programs are written and will remain so into the foreseeable future there is great deal of theoretical interest in algol and in extensible languages but so far at least they have had little practical impact problem oriented languages may very well become the most important language development area in the next five to ten years in the operating system area all major computer manufacturers set out to produce very ambitious multiprogramming systems and they all ran into similar problems number of university projects though not directly comparable to those of the manufacturers have contributed greatly to better understanding of operating system principles important trends include the increased interest in the development of system measurement and evaluation techniques and increased use of microprogramming for some programming system functions
this paper describes an ambient intelligent prototype known as socio ec socio ec explores the design and implementation of system for sensing and display user modeling and interaction models based on game structure the game structure includes word puzzles levels body states goals and game skills body states are body movements and positions that players must discover in order to complete level and in turn represent learned game skill the paper provides an overview of background concepts and related research we describe the prototype and game structure provide technical description of the prototype and discuss technical issues related to sensing reasoning and display the paper contributes by providing method for constructing group parameters from individual parameters with real time motion capture data and model for mapping the trajectory of participant's actions in order to determine an intensity level used to manage the experience flow of the game and its representation in audio and visual display we conclude with discussion of known and outstanding technical issues and future research
we describe forward rasterization class of rendering algorithms designed for small polygonal primitives the primitive is efficiently rasterized by interpolation between its vertices the interpolation factors are chosen to guarantee that each pixel covered by the primitive receives at least one sample which avoids holes the location of the samples is recorded with subpixel accuracy using pair of offsets which are then used to reconstruct resample the output image offset reconstruction has good static and temporal antialiasing properties we present two forward rasterization algorithms one that renders quadrilaterals and is suitable for scenes modeled with depth images like in image based rendering by warping and one that renders triangles and is suitable for scenes modeled conventionally when compared to conventional rasterization forward rasterization is more efficient for small primitives and has better temporal antialiasing properties
there is common misconception that the automobile industry is slow to adapt new technologies such as artificial intelligence ai and soft computing the reality is that many new technologies are deployed and brought to the public through the vehicles that they drive this paper provides an overview and sampling of many of the ways that the automotive industry has utilized ai soft computing and other intelligent system technologies in such diverse domains like manufacturing diagnostics on board systems warranty analysis and design
as memory hierarchy becomes deeper and shared by more processors locality increasingly determines system performance as rigorous and precise locality model reuse distance has been used in program optimizations performance prediction memory disambiguation and locality phase prediction however the high cost of measurement has been severely impeding its uses in scenarios requiring high efficiency such as product compilers performance debugging run time optimizationswe recently discovered the statistical connection between time and reuse distance which led to an efficient way to approximate reuse distance using time however not exposed are some algorithmic and implementation techniques that are vital for the efficiency and scalability of the approximation model this paper presents these techniques it describes an algorithm that approximates reuse distance on arbitrary scales it explains portable scheme that employs memory controller to accelerate the measure of time distance it uncovers the algorithm and proof of trace generator that can facilitate various locality studies
retrieving images from large image collection has been an active area of research most of the existing works have focused on content representation in this paper we address the issue of identifying relevant images quickly this is important in order to meet the users performance requirements we propose framework for fast image retrieval based on object shapes extracted from objects within images the framework builds hierarchy of approximations on object shapes such that shape representation at higher level is coarser representation of shape at the lower level in other words multiple shapes at lower level can be mapped into single shape at higher level in this way the hierarchy serves to partition the database at various granularities given query shape by searching only the relevant paths in the hierarchy large portion of the database can thus be pruned away we propose the angle mapping am method to transform shape from one level to another higher level am essentially replaces some edges of shape by smaller number of edges based on the angles between the edges thus reducing the complexity of the original shape based on the framework we also propose two hierarchical structures to facilitate speedy retrieval the first called hierarchical partitioning on shape representation hpsr uses the shape representation as the indexing key the second called hierarchical partitioning on angle vector hpav captures the angle information from the shape representation we conducted an extensive study on both methods to see their quality and efficiency our experiments on sets of images each of which has objects around from to showed that the framework can provide speedy image retrieval without sacrificing on the quality both proposed schemes can improve the efficiency by as much as hundreds of times to sequential scanning the improvement grows as image database size objects per image or object dimension increase
with the advent of extensive wireless networks that blanket physically compact urban enclaves such as office complexes shopping centers or university campuses it is possible to create software applications that provide location based mobile online services one such application is campuswiki which integrates location information into wiki structure in the design science research reported in this paper we employed form of action research in which we engaged users as participants in an iterative process of designing and evaluating campuswiki two qualitative studies were undertaken early in the design process in which semi structured interviews were used to assess potential users reactions to campuswiki through this research the designers were able to assess whether their intentions matched the mental models of potential users of the application the results showed that although many of the perceived benefits were as designed by the developers misunderstanding of the location aware feature led users to unanticipated concerns and expectations these findings are important in guiding designers and implementers on the desirable and possibly undesirable features of such systems
existing template independent web data extraction approaches adopt highly ineffective decoupled strategies attempting to do data record detection and attribute labeling in two separate phases in this paper we propose an integrated web data extraction paradigm with hierarchical models the proposed model is called dynamic hierarchical markov random fields dhmrfs dhmrfs take structural uncertainty into consideration and define joint distribution of both model structure and class labels the joint distribution is an exponential family distribution as conditional model dhmrfs relax the independence assumption as made in directed models since exact inference is intractable variational method is developed to learn the model's parameters and to find the map model structure and label assignments we apply dhmrfs to real world web data extraction task experimental results show that integrated web data extraction models can achieve significant improvements on both record detection and attribute labeling compared to decoupled models in diverse web data extraction dhmrfs can potentially address the blocky artifact issue which is suffered by fixed structured hierarchical models
as the total amount of traffic data in networks has been growing at an alarming rate there is currently substantial body of research that attempts to mine traffic data with the purpose of obtaining useful information for instance there are some investigations into the detection of internet worms and intrusions by discovering abnormal traffic patterns however since network traffic data contain information about the internet usage patterns of users network users privacy may be compromised during the mining process in this paper we propose an efficient and practical method that preserves privacy during sequential pattern mining on network traffic data in order to discover frequent sequential patterns without violating privacy our method uses the repository server model which operates as single mining server and the retention replacement technique which changes the answer to query probabilistically in addition our method accelerates the overall mining process by maintaining the meta tables in each site so as to determine quickly whether candidate patterns have ever occurred in the site or not extensive experiments with real world network traffic data revealed the correctness and the efficiency of the proposed method
this paper describes llvm low level virtual machine compiler framework designed to support transparent lifelongprogram analysis and transformation for arbitrary programs by providing high level information to compilertransformations at compile time link time run time and inidle time between runsllvm defines common low levelcode representation in static single assignment ssa form with several novel features simple language independenttype system that exposes the primitives commonly used toimplement high level language features an instruction fortyped address arithmetic and simple mechanism that canbe used to implement the exception handling features ofhigh level languages and setjmp longjmp in uniformlyand efficientlythe llvm compiler framework and coderepresentation together provide combination of key capabilitiesthat are important for practical lifelong analysis andtransformation of programsto our knowledge no existingcompilation approach provides all these capabilitieswe describethe design of the llvm representation and compilerframework and evaluate the design in three ways thesize and effectiveness of the representation including thetype information it provides compiler performance forseveral interprocedural problems and illustrative examplesof the benefits llvm provides for several challengingcompiler problems
caching frequently accessed data items on the client side is an effective technique for improving performance in mobile environment classical cache invalidation strategies are not suitable for mobile environments due to frequent disconnections and mobility of the clients one attractive cache invalidation technique is based on invalidation reports irs however the ir based cache invalidation solution has two major drawbacks which have not been addressed in previous research first there is long query latency associated with this solution since client cannot answer the query until the next ir interval second when the server updates hot data item all clients have to query the server and get the data from the server separately which wastes large amount of bandwidth in this paper we propose an ir based cache invalidation algorithm which can significantly reduce the query latency and efficiently utilize the broadcast bandwidth detailed analytical analysis and simulation experiments are carried out to evaluate the proposed methodology compared to previous ir based schemes our scheme can significantly improve the throughput and reduce the query latency the number of uplink request and the broadcast bandwidth requirements
physical database design is important for query performance in shared nothing parallel database system in which data is horizontally partitioned among multiple independent nodes we seek to automate the process of data partitioning given workload of sql statements we seek to determine automatically how to partition the base data across multiple nodes to achieve overall optimal or close to optimal performance for that workload previous attempts use heuristic rules to make those decisions these approaches fail to consider all of the interdependent aspects of query performance typically modeled by today's sophisticated query optimizerswe present comprehensive solution to the problem that has been tightly integrated with the optimizer of commercial shared nothing parallel database system our approach uses the query optimizer itself both to recommend candidate partitions for each table that will benefit each query in the workload and to evaluate various combinations of these candidates we compare rank based enumeration method with random based one our experimental results show that the former is more effective
considering the constraint brought by mobility and resources it is important for routing protocols to efficiently deliver data in intermittently connected mobile network icmn different from previous works that use the knowledge of previous encounters to predict the future contact we propose storagefriendly region based protocol namely rena in this paper instead of using temporal information rena builds routing tables based on regional movement history which avoids excessive storage for tracking encounter history we validate the generality of rena through time variant community mobility model with parameters extracted from the mit wlan trace and the vehicular network based on bus routes of the city of helsinki the comprehensive simulation results show that rena is not only storage friendly but also more efficient than the epidemic routing the restricted replication protocol snw and the encounter based protocol rapid under various conditions
we propose distributed on demand power management protocol for collecting data in sensor networks the protocol aims to reduce power consumption while supporting fluctuating demand in the network and provide local routing information and synchronicity without global control energy savings are achieved by powering down nodes during idle times identified through dynamic scheduling we present real implementation on wireless sensor nodes based on novel two level architecture we evaluate our approach through measurements and simulation and show how the protocol allows adaptive scheduling and enables smooth trade off between energy savings and latency an example current measurement shows an energy savings of on an intermediate node
we study new research problem where an implicit information retrieval query is inferred from eye movements measured when the user is reading and used to retrieve new documents in the training phase the user's interest is known and we learn mapping from how the user looks at term to the role of the term in the implicit query assuming the mapping is universal that is the same for all queries in given domain we can use it to construct queries even for new topics for which no learning data is available we constructed controlled experimental setting to show that when the system has no prior information as to what the user is searching the eye movements help significantly in the search this is the case in proactive search for instance where the system monitors the reading behaviour of the user in new topic in contrast during search or reading session where the set of inspected documents is biased towards being relevant stronger strategy is to search for content wise similar documents than to use the eye movements
in recent years network of workstations pcs so called now are becoming appealing vehicles for cost effective parallel computing due to the commodity nature of workstations and networking equipment lan environments are gradually becoming heterogeneous the diverse sources of heterogeneity in now systems pose challenge on the design of efficient communication algorithms for this class of systems in this paper we propose efficient algorithms for multiple multicast on heterogeneous now systems focusing on heterogeneity in processing speeds of workstations pcs multiple multicast is an important operation in many scientific and industrial applications multicast on heterogeneous systems has not been investigated until recently our work distinguishes itself from others in two aspects in contrast to the blocking communication model used in prior works we model communication in heterogeneous cluster more accurately by non blocking communication model and design multicast algorithms that can fully take advantage of non blocking communication while prior works focus on single multicast problem we propose efficient algorithms for general multiple multicast in which single multicast is special case on heterogeneous now systems to our knowledge our work is the earliest effort that addresses multiple multicast for heterogeneous now systems these algorithms are evaluated using network simulator for heterogeneous now systems our experimental results on system of up to nodes show that some of the algorithms outperform others in many cases the best algorithm achieves completion time that is within times of the lower bound
supporting quality of service qos in wireless networks has been very rich and interesting area of research many significant advances have been made in supporting qos in single wireless networks however the support for the qos across multiple heterogeneous wireless networks will be required in the future wireless networks in connections spanning multiple wireless networks the end to end qos will depend on several factors such as mobility and connection patterns of users and the qos policies in each of the wireless networks the end to end qos is also affected by multiple decisions that must be made by several different network entities for resource allocation the paper has two objectives one is to demonstrate the decision making process for resource allocation in multiple heterogeneous wireless networks and the second is to present novel concept of composite qos in such wireless environment more specifically we present an architecture for multiple heterogeneous wireless networks decision making process for resource request and allocation simulation model to study composite qos and several interesting results we also present potential implications of composite qos on users and network service providers we also show how the qos ideas presented in this paper can be used by wireless carriers for improved qos support and management the paper can form the basis for significant further research in dss for emerging wireless networks supporting qos for range of sophisticated and resource intensive mobile applications
this work addresses the problem of optimizing the deployment of sensors in order to ensure the quality of the readings of the value of interest in given critical geographic region as usual we assume that each sensor is capable of reading particular physical phenomenon eg concentration of toxic materials in the air and transmitting it to server or peer however the key assumptions considered in this work are each sensor is capable of moving where the motion may be remotely controlled and the spatial range for which the individual sensor's reading is guaranteed to be of desired quality is limited in scenarios like disaster management and homeland security in case some of the sensors dispersed in larger geographic area report value higher than certain threshold one may want to ensure quality of the readings for the affected region this in turn implies that one may want to ensure that there are enough sensors there and consequently guide subset of the rest of the sensors towards the affected region in this paper we explore variants of the problem of optimizing the guidance of the mobile sensors towards the affected geographic region and we present algorithms for their solutions
we describe in place reconfiguration ipr for lut based fpgas an algorithm that maximizes identical configuration bits for complementary inputs of lut thereby reducing the propagation of faults seen at pair of complementary inputs based on ipr we develop fault tolerant logic resynthesis algorithm which decreases the circuit fault rate while preserving functionality and topology of the lut based logic network since the topology is preserved the resynthesis algorithm can be applied post layout and without changes in physical design compared to the state of the art academic technology mapper berkeley abc ipr reduces the relative fault rate by and increases mttf by with the same area and performance and ipr combined with previous fault tolerant logic resynthesis algorithm rose reduces the relative fault rate by and increases mttf by with less area but same performance the above improvement assumes stochastic single fault and more improvement is expected for multi fault models
in this paper an interactive and realistic virtual head oriented to human computer interaction and social robotics is presented it has been designed following hybrid approach taking robotic characteristics into account and searching for convergence between these characteristics real facial actions and animation techniques an initial head model is first obtained from real person using laser scanner then the model is animated using hierarchical skeleton based procedure the proposed rig structure is close to real facial muscular anatomy and its behaviour follows the facial action coding system speech synthesis and visual human face tracking capabilities are also integrated for providing the head with further interaction ability using the said hybrid approach the head can be readily linked to social robot architecture the opinions of number of persons interacting with this social avatar have been evaluated and are reported in the paper as against their reactions when interacting with social robot with mechatronic face results show the suitability of the avatar for on screen real time interfacing in human computer interaction the proposed technique could also be helpful in the future for designing and parameterizing mechatronic human like heads for social robots
the emerging paradigm of electronic services promises to bring to distributed computation and services the flexibility that the web has brought to the sharing of documents an understanding of fundamental properties of service composition is required in order to take full advantage of the paradigm this paper examines proposals and standards for services from the perspectives of xml data management workflow and process models key areas for study are identified including behavioral service signatures verification and synthesis techniques for composite services analysis of service data manipulation commands and xml analysis applied to service specifications we give sample of the relevant results and techniques in each of these areas
generic database replication algorithms do not scale linearly in throughput as all update deletion and insertion udi queries must be applied to every database replica the throughput is therefore limited to the point where the number of udi queries alone is sufficient to overload one server in such scenarios partial replication of database can help as udi queries are executed only by subset of all servers in this paper we propose globetp system that employs partial replication to improve database throughput globetp exploits the fact that web application's query workload is composed of small set of read and write templates using knowledge of these templates and their respective execution costs globetp provides database table placements that produce significant improvements in database throughput we demonstrate the efficiency of this technique using two different industry standard benchmarks in our experiments globetp increases the throughput by to compared to full replication while using identical hardware configuration furthermore adding single query cache improves the throughput by another to
many artificial intelligence tasks such as automated question answering reasoning or heterogeneous database integration involve verification of semantic category eg coffee is drink red is color while steak is not drink and big is not color we present novel algorithm to automatically validate semantic category contrary to the methods suggested earlier our approach does not rely on any manually codified knowledge but instead capitalizes on the diversity of topics and word usage on the world wide web we have tested our approach within our online fact seeking question answering environment when tested on the trec questions that expect the answer to belong to specific semantic category our approach has improved the accuracy by up to depending on the model and metrics used
in the rank join problem we are given set of relations and scoring function and the goal is to return the join results with the top scores it is often the case in practice that the inputs may be accessed in ranked order and the scoring function is monotonic these conditions allow for efficient algorithms that solve the rank join problem without reading all of the input in this article we present thorough analysis of such rank join algorithms strong point of our analysis is that it is based on more general problem statement than previous work making it more relevant to the execution model that is employed by database systems one of our results indicates that the well known hrjn algorithm has shortcomings because it does not stop reading its input as soon as possible we find that it is np hard to overcome this weakness in the general case but cases of limited query complexity are tractable we prove the latter with an algorithm that infers provably tight bounds on the potential benefit of reading more input in order to stop as soon as possible as result the algorithm achieves cost that is within constant factor of optimal
in this paper we present method for organizing and indexing logo digital libraries like the ones of the patent and trademark offices we propose an efficient queried by example retrieval system which is able to retrieve logos by similarity from large databases of logo images logos are compactly described by variant of the shape context descriptor these descriptors are then indexed by locality sensitive hashing data structure aiming to perform approximate nn search in high dimensional spaces in sub linear time the experiments demonstrate the effectiveness and efficiency of this system on realistic datasets as the tobacco logo database
early applications of smart cards have focused in the area of personal security recently there has been an increasing demand for networked multi application cards in this new scenario enhanced application specific on card java applets and complex cryptographic services are executed through the smart card java virtual machine jvm in order to support such computation intensive applications contemporary smart cards are designed with built in microprocessors and memory as smart cards are highly area constrained environments with memory cpu and peripherals competing for very small die space the vm execution engine of choice is often small slow interpreter in addition support for multiple applications and cryptographic services demands high performance vm execution engine the above necessitates the optimization of the jvm for java cardsin this paper we present the concept of an annotation aware interpreter that optimizes the interpreted execution of java code using java bytecode superoperators sos sos are groups of bytecode operations that are executed as specialized vm instruction simultaneous translation of all the bytecode operations in an so reduces the bytecode dispatch cost and the number of stack accesses data transfer to from the java operand stack and stack pointer updates furthermore sos help improve native code quality without hindering class file portability annotation attributes in the class files mark the occurrences of valuable sos thereby dispensing the expensive task of searching and selecting sos at runtime besides our annotation based approach incurs minimal memory overhead as opposed to just in time jit compilerswe obtain an average speedup of using an interpreter customized with the top sos formed from operation folding patterns further we show that greater speedups could be achieved by statically adding to the interpreter application specific sos formed by top basic blocks the effectiveness of our approach is evidenced by performance improvements of upto obtained using sos formed from optimized basic blocks
engineering knowledge is specific kind of knowledge that is oriented to the production of particular classes of artifacts is typically related to disciplined design methods and takes place in tool intensive contexts as consequence representing engineering knowledge requires the elaboration of complex models that combine functional and structural representations of the resulting artifacts with process and methodological knowledge the different categories used in the engineering domain vary in their status and in the way they should be manipulated when building applications that support engineering processes these categories include artifacts activities methods and models this paper surveys existing models of engineering knowledge and discusses an upper ontology that abstracts the categories that crosscut different engineering domains such an upper model can be reused for particular engineering disciplines the process of creating such elaborations is reported on the particular case study of software engineering as concrete application example
recent works have shown the benefits of keyword proximity search in querying xml documents in addition to text documents for example given query keywords over shakespeare's plays in xml the user might be interested in knowing how the keywords cooccur in this paper we focus on xml trees and define xml keyword proximity queries to return the possibly heterogeneous set of minimum connecting trees mcts of the matches to the individual keywords in the query we consider efficiently executing keyword proximity queries on labeled trees xml in various settings when the xml database has been preprocessed and when no indices are available on the xml database we perform detailed experimental evaluation to study the benefits of our approach and show that our algorithms considerably outperform prior algorithms and other applicable approaches
many current research efforts address the problem of personalizing the web experience for each user with respect to user's identity and or context in this paper we propose new high level model for the specification of web applications that takes into account the manner in which users interact with the application for supplying appropriate contents or gathering profile data we therefore consider entire behaviors rather than single properties as the smallest information units allowing for automatic restructuring of application components for this purpose high level event condition action eca paradigm is proposed which enables capturing arbitrary and timed clicking behaviors also the architecture and components of first prototype implementation are discussed
one reason that researchers may wish to demonstrate that an external software quality attribute can be measured consistently is so that they can validate prediction system for the attribute however attempts at validating prediction systems for external subjective quality attributes have tended to rely on experts indicating that the values provided by the prediction systems informally agree with the experts intuition about the attribute these attempts are undertaken without pre defined scale on which it is known that the attribute can be measured consistently consequently valid unbiased estimate of the predictive capability of the prediction system cannot be given because the experts measurement process is not independent of the prediction system's values usually no justification is given for not checking to see if the experts can measure the attribute consistently it seems to be assumed that subjective measurement isn't proper measurement or subjective measurement cannot be quantified or no one knows the true values of the attributes anyway and they cannot be estimated however even though the classification of software systems or software artefacts quality attributes is subjective it is possible to quantify experts measurements in terms of conditional probabilities it is then possible using statistical approach to assess formally whether the experts measurements can be considered consistent if the measurements are consistent it is also possible to identify estimates of the true values which are independent of the prediction system these values can then be used to assess the predictive capability of the prediction system in this paper we use bayesian inference markov chain monte carlo simulation and missing data imputation to develop statistical tests for consistent measurement of subjective ordinal scale attributes
for robots operating in real world environments the ability to deal with dynamic entities such as humans animals vehicles or other robots is of fundamental importance the variability of dynamic objects however is large in general which makes it hard to manually design suitable models for their appearance and dynamics in this paper we present an unsupervised learning approach to this model building problem we describe an exemplar based model for representing the time varying appearance of objects in planar laser scans as well as clustering procedure that builds set of object classes from given observation sequences extensive experiments in real environments demonstrate that our system is able to autonomously learn useful models for eg pedestrians skaters or cyclists without being provided with external class information
we consider the problem of establishing route and sending packets between source destination pair in ad hoc networks composed of rational selfish nodes whose purpose is to maximize their own utility in order to motivate nodes to follow the protocol specification we use side payments that are made to the forwarding nodes our goal is to design fully distributed algorithm such that node is always better off participating in the protocol execution individual rationality ii node is always better off behaving according to the protocol specification truthfulness iii messages are routed along the most energy efficient least cost path and iv the message complexity is reasonably low we introduce the commit protocol for individually rational truthful and energy efficient routing in ad hoc networks to the best of our knowledge this is the first ad hoc routing protocol with these features commit is based on the vcg payment scheme in conjunction with novel game theoretic technique to achieve truthfulness for the sender node by means of simulation we show that the inevitable economic inefficiency is small as an aside our work demonstrates the advantage of using cross layer approach to solving problems leveraging the existence of an underlying topology control protocol we are able to simplify the design and analysis of our routing protocol and to reduce its message complexity on the other hand our investigation of the routing problem in presence of selfish nodes disclosed new metric under which topology control protocols can be evaluated the cost of cooperation
modern presentation software is still built around interaction metaphors adapted from traditional slide projectors we provide an analysis of the problems in this application genre that presentation authors face and present fly presentation tool that is based on the idea of planar information structures inspired by the natural human thought processes of data chunking association and spatial memory fly explores authoring of presentation documents evaluation of paper prototype showed that the planar ui is easily grasped by users and leads to presentations more closely resembling the information structure of the original content thus providing better authoring support than the slide metaphor our software prototype confirmed these results and outperformed powerpoint in second study for tasks such as prototyping presentations and generating meaningful overviews users reported that this interface helped them better to express their concepts and expressed significant preference for fly over the traditional slide model
we study generalization of the constraint satisfaction problem csp the periodic constraint satisfaction problem an input instance of the periodic csp is finite set of generating constraints over structured variable set that implicitly specifies larger possibly infinite set of constraints the problem is to decide whether or not the larger set of constraints has satisfying assignment this model is natural for studying constraint networks consisting of constraints obeying high degree of regularity or symmetry our main contribution is the identification of two broad polynomial time tractable subclasses of the periodic csp
suppose we are given graph and set of terminals we consider the problem of constructing graph eh that approximately preserves the congestion of every multicommodity flow with endpoints supported in we refer to such graph as flow sparsifier we prove that there exist flow sparsifiers that simultaneously preserve the congestion of all multicommodity flows within an log log log factor where this bound improves to if excludes any fixed minor this is strengthening of previous results which consider the problem of finding graph eh cut sparsifier that approximately preserves the value of minimum cuts separating any partition of the terminals indirectly our result also allows us to give construction for better quality cut sparsifiers and flow sparsifiers thereby we immediately improve all approximation ratios derived using vertex sparsification in we also prove an log log lower bound for how well flow sparsifier can simultaneously approximate the congestion of every multicommodity flow in the original graph the proof of this theorem relies on technique which we refer to as oblivious dual certifcates for proving super constant congestion lower bounds against many multicommodity flows at once our result implies that approximation algorithms for multicommodity flow type problems designed by black box reduction to uniform case on nodes see for examples must incur super constant cost in the approximation ratio
similarity search is important in information retrieval applications where objects are usually represented as vectors of high dimensionality this paper proposes new dimensionality reduction technique and an indexing mechanism for high dimensional datasets the proposed technique reduces the dimensions for which coordinates are less than critical value with respect to each data vector this flexible datawise dimensionality reduction contributes to improving indexing mechanisms for high dimensional datasets that are in skewed distributions in all coordinates to apply the proposed technique to information retrieval cva file compact va file which is revised version of the va file is developed by using cva file the size of index files is reduced further while the tightness of the index bounds is held maximally the effectiveness is confirmed by synthetic and real data
many of today's high level parallel languages support dynamic fine grained parallelism these languages allow the user to expose all the parallelism in the program which is typically of much higher degree than the number of processors hence an efficient scheduling algorithm is required to assign computations to processors at runtime besides having low overheads and good load balancing it is important for the scheduling algorithm to minimize the space usage of the parallel program this paper presents scheduling algorithm that is provably space efficient and time efficient for nested parallel languages in addition to proving the space and time bounds of the parallel schedule generated by the algorithm we demonstrate that it is efficient in practice we have implemented runtime system that uses our algorithm to schedule parallel threads the results of executing parallel programs on this system show that our scheduling algorithm significantly reduces memory usage compared to previous techniques without compromising performance
in this paper we study generalization of standard property testing where the algorithms are required to be more tolerant with respect to objects that do not have but are close to having the property specifically tolerant property testing algorithm is required to accept objects that are close to having given property and reject objects that are far from having for some parameters another related natural extension of standard property testing that we study is distance approximation here the algorithm should output an estimate of the distance of the object to where this estimate is sufficiently close to the true distance of the object to we first formalize the notions of tolerant property testing and distance approximation and discuss the relationship between the two tasks as well as their relationship to standard property testing we then apply these new notions to the study of two problems tolerant testing of clustering and distance approximation for monotonicity we present and analyze algorithms whose query complexity is either polylogarithmic or independent of the size of the input
despite their popularity and importance pointer based programs remain major challenge for program verification in this paper we propose an automated verification system that is concise precise and expressive for ensuring the safety of pointer based programs our approach uses user definable shape predicates to allow programmers to describe wide range of data structures with their associated size properties to support automatic verification we design new entailment checking procedure that can handle well founded inductive predicates using unfold fold reasoning we have proven the soundness and termination of our verification system and have built prototype system
this paper introduces simple real time distributed computing model for message passing systems which reconciles the distributed computing and the real time systems perspective by just replacing instantaneous computing steps with computing steps of non zero duration we obtain model that both facilitates real time scheduling analysis and retains compatibility with classic distributed computing analysis techniques and results we provide general simulations and validity conditions for transforming algorithms from the classic synchronous model to our real time model and vice versa and investigate whether which properties of real systems are inaccurately or even wrongly captured when resorting to zero step time models we revisit the well studied problem of deterministic drift and failure free internal clock synchronization for this purpose and show that no clock synchronization algorithm with constant running time can achieve optimal precision in our real time model since such an algorithm is known for the classic model this is an instance of problem where the standard distributed computing analysis gives too optimistic results we prove that optimal precision is only achievable with algorithms that take time in our model and establish several additional algorithms and lower bounds
advances in microsensor and radio technology will enable small but smart sensors to be deployed for wide range of environmental monitoring applications the low per node cost will allow these wireless networks of sensors and actuators to be densely distributed the nodes in these dense networks will coordinate to perform the distributed sensing and actuation tasks moreover as described in this paper the nodes can also coordinate to exploit the redundancy provided by high density so as to extend overall system lifetime the large number of nodes deployed in these systems will preclude manual configuration and the environmental dynamics will preclude design time preconfiguration therefore nodes will have to self configure to establish topology that provides communication under stringent energy constraints ascent builds on the notion that as density increases only subset of the nodes are necessary to establish routing forwarding backbone in ascent each node assesses its connectivity and adapts its participation in the multihop network topology based on the measured operating region this paper motivates and describes the ascent algorithm and presents analysis simulation and experimental measurements we show that the system achieves linear increase in energy savings as function of the density and the convergence time required in case of node failures while still providing adequate connectivity
an error occurs when software cannot complete requested action as result of some problem with its input configuration or environment high quality error report allows user to understand and correct the problem unfortunately the quality of error reports has been decreasing as software becomes more complex and layered end users take the cryptic error messages given to them by programsand struggle to fix their problems using search engines and support websites developers cannot improve their error messages when they receive an ambiguous or otherwise insufficient error indicator from black box software component we introduce clarify system that improves error reporting by classifying application behavior clarify uses minimally invasive monitoring to generate behavior profile which is summary of the program's execution history machine learning classifier uses the behavior profile to classify the application's behavior thereby enabling more precise error report than the output of the application itself we evaluate prototype clarify system on ambiguous error messages generated by large modern applications like gcc la tex and the linux kernel for performance cost of less than on user applications and on the linux kernel the proto type correctly disambiguates at least of application behaviors that result in ambiguous error reports this accuracy does not degrade significantly with more behaviors clarify classifier for la tex error messages is at most less accurate than classifier for latex error messages finally we show that without any human effort to build classifier clarify can provide nearest neighbor software support where users who experience problem are told about other users who might have had the same problem on average of the users that clarify identifies have experienced the same problem
distributional measures of lexical similarity and kernel methods for classification are well known tools in natural language processing we bring these two methods together by introducing distributional kernels that compare co occurrence probability distributions we demonstrate the effectiveness of these kernels by presenting state of the art results on datasets for three semantic classification compound noun interpretation identification of semantic relations between nominals and semantic classification of verbs finally we consider explanations for the impressive performance of distributional kernels and sketch some promising generalisations
we survey recent results on wireless networks that are based on analogies with various branches of physics we address among others the problems of optimally arranging the flow of traffic in wireless sensor networks finding minimum cost routes performing load balancing optimizing and analyzing cooperative transmissions calculating the capacity finding routes that avoid bottlenecks and developing distributed anycasting protocols the results are based on establishing analogies between wireless networks and settings from various branches of physics such as electrostatics optics percolation theory diffusion and others many of the results we present hinge on the assumption that the network is massive ie it consists of so many nodes that it can be described in terms of novel macroscopic view the macroscopic view is not as detailed as the standard microscopic one but nevertheless contains enough details to permit meaningful optimization
when meeting someone new the first impression is often influenced by someone's physical appearance and other types of prejudice in this paper we present touchmedare an interactive canvas which aims to provide an experience when meeting new people while preventing visual prejudice and lowering potential thresholds the focus of the designed experience was to stimulate people to get acquainted through the interactive canvas touchmedare consists of flexible opaque canvas which plays music when touched simultaneously from both sides dynamic variation of this bodily contact is reflected through real time adaptations of the musical compositions two redesigns were qualitatively and quantitatively evaluated and final version was placed in the lowlands festival as case study evaluation results showed that some explanation was needed for the initial interaction with the installation on the other hand after this initial unfamiliarity passed results showed that making bodily contact through the installation did help people to get acquainted with each other and increased their social interaction
the standard formalism for explaining abstract types is existential quantification while it provides sufficient model for type abstraction in entirely statically typed languages it proves to be too weak for languages enriched with forms of dynamic typing where parametricity is violated as an alternative approach to type abstraction that addresses this shortcoming we present calculus for dynamic type generation it features an explicit construct for generating new type names and relies on coercions for managing abstraction boundaries between generated types and their designated representation sealing is represented as generalized form of these coercions the calculus maintains abstractions dynamically without restricting type analysis
many applications require randomized ordering of input data examples include algorithms for online aggregation data mining and various randomized algorithms most existing work seems to assume that accessing the records from large database in randomized order is not difficult problem however it turns out to be extremely difficult in practice using existing methods randomization is either extremely expensive at the front end as data are loaded or at the back end as data are queried this paper presents simple file structure which supports both efficient online random shuffling of large database as well as efficient online sampling or randomization of the database when it is queried the key innovation of our method is the introduction of small degree of carefully controlled rigorously monitored nonrandomness into the file
moments before the launch of every space vehicle engineering discipline specialists must make critical go no go decision the cost of false positive allowing launch in spite of fault or false negative stopping potentially successful launch can be measured in the tens of millions of dollars not including the cost in morale and other more intangible detriments the aerospace corporation is responsible for providing engineering assessments critical to the go no go decision for every department of defense space vehicle these assessments are made by constantly monitoring streaming telemetry data in the hours before launch we will introduce viztree novel time series visualization tool to aid the aerospace analysts who must make these engineering assessments viztree was developed at the university of california riverside and is unique in that the same tool is used for mining archival data and monitoring incoming live telemetry the use of single tool for both aspects of the task allows natural and intuitive transfer of mined knowledge to the monitoring task our visualization approach works by transforming the time series into symbolic representation and encoding the data in modified suffix tree in which the frequency and other properties of patterns are mapped onto colors and other visual properties we demonstrate the utility of our system by comparing it with state of the art batch algorithms on several real and synthetic datasets
numerous context aware mobile communication systems have emerged for individuals and groups calling for the identification of critical success factors related to the design of such systems at different levels the effective system design cannot be achieved without the understanding of situated user behaviour in using context aware systems drawing on activity theory this article advances cross level but coherent conceptualisations of context awareness as enabled by emerging systems grounded in the activities of using context aware systems these conceptualisations provide implications for system design at individual and group levels in terms of critical success factors including contextualisation interactivity and personalisation
many materials including water plastic and metal have specular surface characteristics specular reflections have commonly been considered nuisance for the recovery of object shape however the way that reflections are distorted across the surface depends crucially on curvature suggesting that they could in fact be useful source of information indeed observers can have vivid impression of shape when an object is perfectly mirrored ie the image contains nothing but specular reflections this leads to the question what are the underlying mechanisms of our visual system to extract this shape information from perfectly mirrored object in this paper we propose biologically motivated recurrent model for the extraction of visual features relevant for the perception of shape information from images of mirrored objects we qualitatively and quantitatively analyze the results of computational model simulations and show that bidirectional recurrent information processing leads to better results than pure feedforward processing furthermore we utilize the model output to create rough nonphotorealistic sketch representation of mirrored object which emphasizes image features that are mandatory for shape perception eg occluding contour and regions of high curvature moreover this sketch illustrates that the model generates representation of object features independent of the surrounding scene reflected in the mirrored object
we address the problem of answering conjunctive queries over extended entity relationship schemata which we call eer extended er schemata with is among entities and relationships and cardinality constraints this is common setting in conceptual data modelling where reasoning over incomplete data with respect to knowledge base is required we adopt semantics for eer schemata based on their relational representation we identify wide class of eer schemata for which query answering is tractable in data complexity the crucial condition for tractability is the separability between maximum cardinality constraints represented as key constraints in relational form and the other constraints we provide by means of graph based representation syntactic condition for separability we show that our conditions is not only sufficient but also necessary thus precisely identifying the class of separable schemata we present an algorithm based on query rewriting that is capable of dealing with such eer schemata while achieving tractability we show that further negative constraints can be added to the eer formalism while still keeping query answering tractable we show that our formalism is general enough to properly generalise the most widely adopted knowledge representation languages
we present set of algorithms and an associated display system capable of producing correctly rendered eye contact between three dimensionally transmitted remote participant and group of observers in teleconferencing system the participant's face is scanned in at hz and transmitted in real time to an autostereoscopic horizontal parallax display displaying him or her over more than deg field of view observable to multiple observers to render the geometry with correct perspective we create fast vertex shader based on lookup table for projecting scene vertices to range of subject angles heights and distances we generalize the projection mathematics to arbitrarily shaped display surfaces which allows us to employ curved concave display surface to focus the high speed imagery to individual observers to achieve two way eye contact we capture video from cross polarized camera reflected to the position of the virtual participant's eyes and display this video feed on large screen in front of the real participant replicating the viewpoint of their virtual self to achieve correct vertical perspective we further leverage this image to track the position of each audience member's eyes allowing the display to render correct vertical perspective for each of the viewers around the device the result is one to many teleconferencing system able to reproduce the effects of gaze attention and eye contact generally missing in traditional teleconferencing systems
common deficiency of discretized datasets is that detail beyond the resolution of the dataset has been irrecoverably lost this lack of detail becomes immediately apparent once one attempts to zoom into the dataset and only recovers blur here we describe method that generates the missing detail from any available and plausible high resolution data using texture synthesis since the detail generation process is guided by the underlying image or volume data and is designed to fill in plausible detail in accordance with the coarse structure and properties of the zoomed in neighborhood we refer to our method as constrained texture synthesis regular zooms become semantic zooms where each level of detail stems from data source attuned to that resolution we demonstrate our approach by medical application the visualization of human liver but its principles readily apply to any scenario as long as data at all resolutions are available we will first present viewing application called the virtual microscope and then extend our technique to volumetric viewing
we present new technique that employs support vector machines svms and gaussian mixture densities gmds to create generative discriminative object classification technique using local image features in the past several approaches to fuse the advantages of generative and discriminative approaches were presented often leading to improved robustness and recognition accuracy support vector machines are well known discriminative classification framework but similar to other discriminative approaches suffer from lack of robustness with respect to noise and overfitting gaussian mixtures on the contrary are widely used generative technique we present method to directly fuse both approaches effectively allowing to fully exploit the advantages of both the fusion of svms and gmds is done by representing svms in the framework of gmds without changing the training and without changing the decision boundary the new classifier is evaluated on the pascal voc data additionally we perform experiments on the usps dataset and on four tasks from the uci machine learning repository to obtain additional insights into the properties of the proposed approach it is shown that for the relatively rare cases where svms have problems the combined method outperforms both individual ones
in this paper processor scheduling policies that save processors are introduced and studied in multiprogrammed parallel system processor saving scheduling policy purposefully keeps some of the available processors idle in the presence of work to be done the conditions under which processor saving policies can be more effective than their greedy counterparts ie policies that never leave processors idle in the presence of work to be done are examined sensitivity analysis is performed with respect to application speedup system size coefficient of variation of the applications execution time variability in the arrival process and multiclass workloads analytical simulation and experimental results show that processor saving policies outperform their greedy counterparts under variety of system and workload characteristics
principles of the unitesk test development technology based on the use of formal models of target software are presented this technology was developed by the redverst group in the institute for system programming russian academy of sciences ispras lsqb rsqb which obtained rich experience in testing and verification of complex commercial software
recovering from malicious attacks in survival database systems is vital in mission critical information systems traditional rollback and re execute techniques are too time consuming and can not be applied in survival environments in this paper two efficient approaches transaction dependency based and data dependency based are proposed comparing to transaction dependency based approach data dependency recovery approaches need not undo innocent operations in malicious and affected transactions even some benign blind writes on bad data item speed up recovery process
information system engineering has become under increasing pressure to come up with software solutions that endow systems with the agility that is required to evolve in continually changing business and technological environment in this paper we suggest that software engineering has contribution to make in terms of concepts and techniques that have been recently developed for parallel program design and software architectures we show how such mechanisms can be encapsulated in new modelling primitive coordination contract that can be used for extending component based development approaches in order to manage such levels of change
this article provides detailed implementation study on the behavior of web serves that serve static requests where the load fluctuates over time transient overload various external factors are considered including wan delays and losses and different client behavior models we find that performance can be dramatically improved via kernel level modification to the web server to change the scheduling policy at the server from the standard fair processor sharing scheduling to srpt shortest remaining processing time scheduling we find that srpt scheduling induces no penalties in particular throughput is not sacrificed and requests for long files experience only negligibly higher response times under srpt than they did under the original fair scheduling
recent work demonstrates the potential for extracting patterns from users behavior as detected by sensors since there is currently no generalized framework for reasoning about activity aware applications designers can only rely on the existing systems for guidance however these systems often use custom domain specific definition of activity pattern consequently the guidelines designers can extract from individual systems are limited to the specific application domains of those applications in this paper we introduce five high level guidelines or commandments for designing activity aware applications by considering the issues we outlined in this paper designers will be able to avoid common mistakes inherent in designing activity aware applications
cast shadows are an informative cue to the shape of objects they are particularly valuable for discovering object's concavities which are not available from other cues such as occluding boundaries we propose new method for recovering shape from shadows which we call shadow carving given conservative estimate of the volume occupied by an object it is possible to identify and carve away regions of this volume that are inconsistent with the observed pattern of shadows we prove theorem that guarantees that when these regions are carved away from the shape the shape still remains conservative shadow carving overcomes limitations of previous studies on shape from shadows because it is robust with respect to errors in shadows detection and it allows the reconstruction of objects in the round rather than just bas reliefs we propose reconstruction system to recover shape from silhouettes and shadow carving the silhouettes are used to reconstruct the initial conservative estimate of the object's shape and shadow carving is used to carve out the concavities we have simulated our reconstruction system with commercial rendering package to explore the design parameters and assess the accuracy of the reconstruction we have also implemented our reconstruction scheme in table top system and present the results of scanning of several objects
information from which knowledge can be discovered is frequently distributed due to having been recorded at different times or to having arisen from different sources such information is often subject to both imprecision and uncertainty the dempster shafer representation of evidence offers way of representing uncertainty in the presence of imprecision and may therefore be used to provide mechanism for storing imprecise and uncertain information in databases we consider an extended relational data model that allows the imprecision and uncertainty associated with attribute values to be quantified using mass function distribution when query is executed it may be necessary to combine imprecise and uncertain data from distributed sources in order to answer that query mechanism is therefore required both for combining the data and for generating measures of uncertainty to be attached to the imprecise combined data in this paper we provide such mechanism based on aggregation of evidence we show first how this mechanism can be used to resolve inconsistencies and hence provide an essential database capability to perform the operations necessary to respond to queries on imprecise and uncertain data we go on to exploit the aggregation operator in an attribute driven approach to provide information on properties of and patterns in the data this is fundamental to rule discovery and hence such an aggregation operator provides facility that is central requirement in providing distributed information system with the capability to perform the operations necessary for knowledge discovery
one view of computational learning theory is that of learner acquiring the knowledge of teacher we introduce formal model of learning capturing the idea that teachers may have gaps in their knowledge the goal of the learner is still to acquire the knowledge of the teacher but now the learner must also identify the gaps this is the notion of learning from consistently ignorant teacher we consider the impact of knowledge gaps on learning for example monotone dnf and dimensional boxes and show that learning is still possible negatively we show that knowledge gaps make learning conjunctions of horn clauses as hard as learning dnf we also present general results describing when known learning algorithms can be used to obtain learning algorithms using consistently ignorant teacher
in this paper we study the challenges and evaluate the effectiveness of data collected from the web for recommendations we provide experimental results including user study showing that our methods produce good recommendations in realistic applications we propose new evaluation metric that takes into account the difficulty of prediction we show that the new metric aligns well with the results from user study
this paper proposes new clothing segmentation method using foreground clothing and background non clothing estimation based on the constrained delaunay triangulation cdt without any pre defined clothing model in our method the clothing is extracted by graph cuts where the foreground seeds and background seeds are determined automatically the foreground seeds are found by torso detection based on dominant colors determination and the background seeds are estimated based on cdt with the determined seeds the color distributions of the foreground and background are modeled by gaussian mixture models and filtered by cdt based noise suppression algorithm for more robust and accurate segmentation experimental results show that our clothing segmentation method is able to extract different clothing from static images with variations in backgrounds and lighting conditions
in this paper we address the issue of feeding future superscalar processor cores with enough instructions hardware techniques targeting an increase in the instruction fetch bandwidth have been proposed such as the trace cache microarchitecture we present microarchitecture solution based on register file holding basic blocks of instructions this solution places the instruction memory hierarchy out of the cycle determining path we call our approach instruction register file irf we estimate our approach with simplescalar based simulator run on the mediabench benchmark suite and compare to the trace cache performance on the same benchmarks we show that on this benchmark suite an irf based processor fetching up to three basic blocks per cycle outperforms trace cache based processor fetching instructions long traces by on the average
the authors propose method for personalizing the flexible widget layout fwl by adjusting the desirability of widgets with pairwise comparison method and show its implementation and that it actually works personalization of graphical user interfaces guis is important from perspective of usability and it is challenge in the field of model based user interface designs the fwl is model and optimization based layout framework of guis offering possibility for personalization but it has not actually realized it with any concrete method yet in this paper the authors implement method for personalization as dialog box and incorporate it into the existing system of the fwl thus users can personalize layouts generated by the fwl system at run time
within the context of the relational model general technique for establishing that the translation of view update defined by constant complement is independent of the choice of complement is presented in contrast to previous results the uniqueness is not limited to order based updates those constructed from insertions and deletions nor is it limited to those well behaved complements which define closed update strategies rather the approach is based upon optimizing the change of information in the main schema which the view update entails the only requirement is that the view and its complement together possess property called semantic bijectivity relative to the information measure it is furthermore established that very wide range of views have this property this results formalizes the intuition long observed in examples that it is difficult to find different complements which define distinct but reasonable update strategies
in wireless networks bandwidth is relatively scarce especially for supporting on demand media streaming in wired networks multicast stream merging is well known technique for scalable on demand streaming also caching proxies are widely used on the internet to offload servers and reduce network traffic this paper uses simulation to examine caching hierarchy for wireless streaming video distribution in combination with multicast stream merging the main purpose is to gain insight into the filtering effects caused by caching and merging using request frequencies entropy and inter reference times as metrics we illustrate how merging caching and traffic aggregation affect the traffic characteristics at each level the simulation results provide useful insights into caching performance in video streaming hierarchy
the power of high level languages lies in their abstraction over hardware and software complexity leading to greater security better reliability and lower development costs however opaque abstractions are often show stoppers for systems programmers forcing them to either break the abstraction or more often simply give up and use different language this paper addresses the challenge of opening up high level language to allow practical low level programming without forsaking integrity or performance the contribution of this paper is three fold we draw together common threads in diverse literature we identify framework for extending high level languages for low level programming and we show the power of this approach through concrete case studies our framework leverages just three core ideas extending semantics via intrinsic methods extending types via unboxing and architectural width primitives and controlling semantics via scoped semantic regimes we develop these ideas through the context of rich literature and substantial practical experience we show that they provide the power necessary to implement substantial artifacts such as high performance virtual machine while preserving the software engineering benefits of the host language the time has come for high level low level programming to be taken more seriously more projects now use high level languages for systems programming increasing architectural heterogeneity and parallelism heighten the need for abstraction and new generation of high level languages are under development and ripe to be influenced
collaboration and information sharing between organizations that share common goal is becoming increasingly important effective sharing promotes efficiency and productivity as well as enhances customer service with internet connectivity widely available sharing and access to information is relatively simple to implement however the abundance causes another problem the difficulty of determining where truly useful and relevant information is housed information resources such as data documents multimedia objects and services stored in different agencies need to be easily discovered and shared we propose collaborative semantic and pragmatic annotation environment where resources of each agency can be annotated by users in the government social network this collaborative annotation captures not only the semantics but also the pragmatics of the resources such as who when where how and why the resources are used the benefits of semantic and pragmatic annotation tags will include an ability to filter discover and search new and dynamic as well as hidden resources to navigate between resources in search by traversing semantic relationships and to recommend the most relevant government information distributed over different agencies distributed architecture of tagging system is shown and tag based search is illustrated
number of supervised learning methods have been introduced in the last decade unfortunately the last comprehensive empirical evaluation of supervised learning was the statlog project in the early we present large scale empirical comparison between ten supervised learning methods svms neural nets logistic regression naive bayes memory based learning random forests decision trees bagged trees boosted trees and boosted stumps we also examine the effect that calibrating the models via platt scaling and isotonic regression has on their performance an important aspect of our study is the use of variety of performance criteria to evaluate the learning methods
model driven development using languages such as uml and bon often makes use of multiple diagrams eg class and sequence diagrams when modeling systems these diagrams presenting different views of system of interest may be inconsistent metamodel provides unifying framework in which to ensure and check consistency while at the same time providing the means to distinguish between valid and invalid models that is conformance two formal specifications of the metamodel for an object oriented modeling language are presented and it is shown how to use these specifications for model conformance and multiview consistency checking comparisons are made in terms of completeness and the level of automation each provide for checking multiview consistency and model conformance the lessons learned from applying formal techniques to the problems of metamodeling model conformance and multiview consistency checking are summarized
disk caching algorithm is presented that uses an adaptive prefetching scheme to optimize the system performance in disk controllers for traces with different data localities the algorithm uses on line measurements of disk transfer times and of inter page fault rates to adjust the level of prefetching dynamically and its performance is evaluated through trace driven simulations using real workloads the results confirm the effectiveness and efficiency of the new adaptive prefetching algorithm
grids offer dramatic increase in the number of available processing and storing resources that can be delivered to applications however efficient job submission and management continue being far from accessible to ordinary scientists and engineers due to their dynamic and complex nature this paper describes new globus based framework that allows an easier and more efficient execution of jobs in submit and forget fashion the framework automatically performs the steps involved in job submission and also watches over its efficient execution in order to obtain reasonable degree of performance job execution is adapted to dynamic resource conditions and application demands adaptation is achieved by supporting automatic application migration following performance degradation better resource discovery requirement change owner decision or remote resource failure the framework is currently functional on any grid testbed based on globus because it does not require new system software to be installed in the resources the paper also includes practical experiences of the behavior of our framework on the trgp and ucm cab testbeds
debugging refers to the laborious process of finding causes of program failures often such failures are introduced when program undergoes changes and evolves from stable version to new modified version in this paper we propose an automated approach for debugging evolving programs given two programs reference stable program and new modified program and an input that fails on the modified program our approach uses concrete as well as symbolic execution to synthesize new inputs that differ marginally from the failing input in their control flow behavior comparison of the execution traces of the failing input and the new inputs provides critical clues to the root cause of the failure notable feature of our approach is that it handles hard to explain bugs like code missing errors by pointing to the relevant code in the reference program we have implemented our approach in tool called darwin we have conducted experiments with several real life case studies including real world web servers and the libpng library for manipulating png images our experience from these experiments points to the efficacy of darwin in pinpointing bugs moreover while localizing given observable error the new inputs synthesized by darwin can reveal other undiscovered errors
os kernels have been written in weakly typed or non typed programming languages for example therefore it is extremely hard to verify even simple memory safety of the kernels the difficulty could be resolved by writing os kernels in strictly typed programming languages but existing strictly typed languages are not flexible enough to implement important os facilities eg memory management and multi thread management facilities to address the problem we designed and implemented talk new strictly and statically typed assembly language which is flexible enough to implement os facilities and wrote an os kernel with talk in our approach the safety of the kernel can be verified automatically through static type checking at the level of binary executables without source code
existing research has most often relied on simulation and considered the uniform traffic distribution when investigating the performance properties of multicomputer networks eg the torus however there are numerous parallel applications that generate non uniform traffic patterns such as hot spot furthermore much more attention has been paid to capturing the impact of non uniform traffic on network performance resulting in the development of number of analytical models for predicting message latency in the presence of hot spots in the network for instance analytical models have been reported for the adaptively routed torus with uni directional as well as bi directional channels however models for the deterministically routed torus have considered uni directional channels only in an effort to fill in this gap this paper describes an analytical model for the deterministically routed torus with bi directional channels when subjected to hot spot traffic the modelling approach adopted for deterministic routing is totally different from that for adaptive routing due to the inherently different nature of the two types of routing the validity of the model is demonstrated by comparing analytical results against those obtained through extensive simulation experiments
the task of obtaining an optimal set of parameters to fit mixture model has many applications in science and engineering domains and is computationally challenging problem novel algorithm using convolution based smoothing approach to construct hierarchy or family of smoothed log likelihood surfaces is proposed this approach smooths the likelihood function and applies the em algorithm to obtain promising solution on the smoothed surface using the most promising solutions as initial guesses the em algorithm is applied again on the original likelihood though the results are demonstrated using only two levels the method can potentially be applied to any number of levels in the hierarchy theoretical insight demonstrates that the smoothing approach indeed reduces the overall gradient of modified version of the likelihood surface this optimization procedure effectively eliminates extensive searching in non promising regions of the parameter space results on some benchmark datasets demonstrate significant improvements of the proposed algorithm compared to other approaches empirical results on the reduction in the number of local maxima and improvements in the initialization procedures are provided
an object detection method from line drawings is presented the method adopts the local neighborhood structure as the elementary descriptor which is formed by grouping several nearest neighbor lines curves around one reference with this representation both the appearance and the geometric structure of the line drawing are well described the detection algorithm is hypothesis test scheme the top most similar local structures in the drawing are firstly obtained for each local structure of the model and the transformation parameters are estimated for each of the candidates such as object center scale and rotation factors by treating each estimation result as point in the parameter space dense region around the ground truth is then formed provided that there exist model in the drawing the mean shift method is used to detect the dense regions and the significant modes are accepted as the occurrence of object instances
we describe our experiences of designing digital community display with members of rural community these experiences are highlighted by the development of printed and digital postcard features for the wray photo display public photosharing display designed with the community which was trialled during popular village fair where both local residents and visitors interacted with the system this trial allowed us to examine the relative popularity and differences in usage between printed and digital postcard and offer insights into the uses of these features with community generated content and potential problems encountered
the interconnection network considered in this paper is the generalized base hypercube that is an attractive variance of the well known hypercube the generalized base hypercube is superior to the hypercube in many criteria such as diameter connectivity and fault diameter in this paper we study the hamiltonian connectivity and pancyclicity of the generalized base hypercube by the algorithmic approach we show that generalized base hypercube is hamiltonian connected for that is there exists hamiltonian path joining each pair of vertices in generalized base hypercube for we also show that generalized base hypercube is pancyclic for that is it embeds cycles of all lengths ranging from to the order of the graph for
in this paper we approach the problem of constructing ensembles of classifiers from the point of view of instance selection instance selection is aimed at obtaining subset of the instances available for training capable of achieving at least the same performance as the whole training set in this way instance selection algorithms try to keep the performance of the classifiers while reducing the number of instances in the training set meanwhile boosting methods construct an ensemble of classifiers iteratively focusing each new member on the most difficult instances by means of biased distribution of the training instances in this work we show how these two methodologies can be combined advantageously we can use instance selection algorithms for boosting using as objective to optimize the training error weighted by the biased distribution of the instances given by the boosting method our method can be considered as boosting by instance selection instance selection has mostly been developed and used for nearest neighbor nn classifiers so as first step our methodology is suited to construct ensembles of nn classifiers constructing ensembles of classifiers by means of instance selection has the important feature of reducing the space complexity of the final ensemble as only subset of the instances is selected for each classifier however the methodology is not restricted to nn classifier other classifiers such as decision trees and support vector machines svms may also benefit from smaller training set as they produce simpler classifiers if an instance selection algorithm is performed before training in the experimental section we show that the proposed approach is able to produce better and simpler ensembles than random subspace method rsm method for nn and standard ensemble methods for and svms
we present an expectation maximization learning algorithm em for estimating the parameters of partially constrained bayesian trees the bayesian trees considered here consist of an unconstrained subtree and set of constrained subtrees in this tree structure constraints are imposed on some of the parameters of the parametrized conditional distributions such that all conditional distributions within the same subtree share the same constraint we propose learning method that uses the unconstrained subtree to guide the process of discovering set of relevant constrained substructures substructure discovery and constraint enforcement are simultaneously accomplished using an em algorithm we show how our tree substructure discovery method can be applied to the problem of learning representative pose models from set of unsegmented video sequences our experiments demonstrate the potential of the proposed method for human motion classification
in multimedia retrieval query is typically interactively refined towards the optimal answers by exploiting user feedback however in existing work in each iteration the refined query is re evaluated this is not only inefficient but fails to exploit the answers that may be common between iterations furthermore it may also take too many iterations to get the optimal answers in this paper we introduce new approach called optrfs optimizing relevance feedback search by query prediction for iterative relevance feedback search optrfs aims to take users to view the optimal results as fast as possible it optimizes relevance feedback search by both shortening the searching time during each iteration and reducing the number of iterations optrfs predicts the potential candidates for the next iteration and maintains this small set for efficient sequential scan by doing so repeated candidate accesses ie random accesses can be saved hence reducing the searching time for the next iteration in addition efficient scan on the overlap before the next search starts also tightens the search space with smaller pruning radius as step forward optrfs also predicts the optimal query which corresponds to optimal answers based on the early executed iterations queries by doing so some intermediate iterations can be saved hence reducing the total number of iterations by taking the correlations among the early executed iterations into consideration optrfs investigates linear regression exponential smoothing and linear exponential smoothing to predict the next refined query so as to decide the overlap of candidates between two consecutive iterations considering the special features of relevance feedback optrfs further introduces adaptive linear exponential smoothing to self adjust the parameters for more accurate prediction we implemented optrfs and our experimental study on real life data sets show that it can reduce the total cost of relevance feedback search significantly some interesting features of relevance feedback search are also discovered and discussed
automated software engineering methods support the construction maintenance and analysis of both new and legacy systems their application is commonplace in desktop and enterprise class systems due to the productivity and reliability benefits they afford the contribution of this article is to present an applied foundation for extending the use of such methods to the flourishing domain of wireless sensor networks the objective is to enable developers to construct tools that aid in understanding both the static and dynamic properties of reactive event based systems we present static analysis and instrumentation toolkit for the nesc language the defacto standard for sensor network development we highlight the novel aspects of the toolkit analyze its performance and provide representative case studies that illustrate its use
real scale semantic web applications such as knowledge portals and marketplaces require the management of large volumes of metadata ie information describing the available web content and services better knowledge about their meaning usage accessibility or quality will considerably facilitate an automated processing of web resources the resource description framework rdf enables the creation and exchange of metadata as normal web data although voluminous rdf descriptions are already appearing sufficiently expressive declarative languages for querying both rdf descriptions and schemas are still missing in this paper we propose new rdf query language called rql it is typed functional language la oql and relies on formal model for directed labeled graphs permitting the interpretation of superimposed resource descriptions by means of one or more rdf schemas rql adapts the functionality of semistructured xml query languages to the peculiarities of rdf but foremost it enables to uniformly query both resource descriptions and schemas we illustrate the rql syntax semantics and typing system by means of set of example queries and report on the performance of our persistent rdf store employed by the rql interpreter
the web is now being used as general platform for hosting distributed applications like wikis bulletin board messaging systems and collaborative editing environments data from multiple applications originating at multiple sources all intermix in single web browser making sensitive data stored in the browser subject to broad milieu of attacks cross site scripting cross site request forgery and others the fundamental problem is that existing web infrastructure provides no means for enforcing end to end security on data to solve this we design an architecture using mandatory access control mac enforcement we overcome the limitations of traditional mac systems implemented solely at the operating system layer by unifying mac enforcement across virtual machine operating system networking and application layers we implement our architecture using xen virtual machine management selinux at the operating system layer labeled ipsec for networking and our own label enforcing web browser called flowwolf we tested our implementation and find that it performs well supporting data intermixing while still providing end to end security guarantees
paper augmented digital documents padds are digital documents that can be manipulated either on computer screen or on paper padds and the infrastructure supporting them can be seen as bridge between the digital and the paper worlds as digital documents padds are easy to edit distribute and archive as paper documents padds are easy to navigate annotate and well accepted in social settings the chimeric nature of padds make them well suited for many tasks such as proofreading editing and annotation of large format document like blueprintswe are presenting an architecture which supports the seamless manipulation of padds using today's technologies and reports on the lessons we learned while implementing the first padd system
we present statistical method called covering topic score cts to predict query performance for information retrieval estimation is based on how well the topic of user's query is covered by documents retrieved from certain retrieval system our approach is conceptually simple and intuitive and can be easily extended to incorporate features beyond bag of words such as phrases and proximity of terms experiments demonstrate that cts significantly correlates with query performance in variety of trec test collections and in particular cts gains more prediction power benefiting from features of phrases and proximity of terms we compare cts with previous state of the art methods for query performance prediction including clarity score and robustness score our experimental results show that cts consistently performs better than or at least as well as these other methods in addition to its high effectiveness cts is also shown to have very low computational complexity meaning that it can be practical for real applications
sensor networks consist of many small sensing devices that monitor an environment and communicate using wireless links the lifetime of these networks is severely curtailed by the limited battery power of the sensors one line of research in sensor network lifetime management has examined sensor selection techniques in which applications judiciously choose which sensors data should be retrieved and are worth the expended energy in the past many ad hoc approaches for sensor selection have been proposed in this paper we argue that sensor selection should be based upon tradeoff between application perceived benefit and energy consumption of the selected sensor setwe propose framework wherein the application can specify the utility of measuring data nearly concurrently at each set of sensors he goal is then to select sequence of sets to measure whose total utility is maximized while not exceeding the available energy alternatively we may look for the most cost effective sensor set maximizing the product of utility and system lifetimethis approach is very generic and permits us to model many applications of sensor networks we proceed to study two important classes of utility functions submodular and supermodular functions we show that the optimum solution for submodular functions can be found in polynomial time while optimizing the costeffectiveness of supermodular functions is np hard for practically important subclass of supermodular functions we present an lp based solution if nodes can send for different amounts of time and show that we can achieve an logn approximation ratio if each node has to send for the same amount of timefinally we study scenarios in which the quality of measurements is naturally expressed in terms of distances from targets we show that the utility based approach is analogous to penalty based approach in those scenarios and present preliminary results on some practically important special cases
while total order broadcast or atomic broadcast primitives have received lot of attention this paper concentrates on total order multicast to multiple groups in the context of asynchronous distributed systems in which processes may suffer crash failures multicast to multiple groups means that each message is sent to subset of the process groups composing the system distinct messages possibly having distinct destination groups total order means that all message deliveries must be totally ordered this paper investigates consensus based approach to solve this problem and proposes corresponding protocol to implement this multicast primitive this protocol is based on two underlying building blocks namely uniform reliable multicast and uniform consensus its design characteristics lie in the two following properties the first one is minimality property more precisely only the sender of message and processes of its destination groups have to participate in the total order multicast of the message the second property is locality property no execution of consensus has to involve processes belonging to distinct groups ie consensus is executed on per group basis this locality property is particularly useful when one is interested in using the total order multicast primitive in large scale distributed systems in addition to correctness proof an improvement that reduces the cost of the protocol is also suggested
as collaboration in virtual environments becomes more object focused and closely coupled the frequency of conflicts in accessing shared objects can increase in addition two kinds of concurrency control surprises become more disruptive to the collaboration undo surprises can occur when previously visible change is undone because of an access conflict intention surprises can happen when concurrent action by remote session changes the structure of shared object at the same perceived time as local access of that object such that the local user might not get what they expect because they have not had time to visually process the change hierarchy of three concurrency control mechanisms is presented in descending order of collaborative surprises which allows the concurrency scheme to be tailored to the tolerance for such surprises one mechanism is semioptimistic the other two are pessimistic designed for peer to peer vitual environments in which several threads have access to the shared scene graph these algorithms are straightforward and relatively simple they can be implemented using and java under windows and unix on both desktop and immersive systems in series of usability experiments the average performance of the most conservative concurrency control mechanism on local lan was found to be quite acceptable
using moving parabolic approximations mpa we reconstruct an improved point based model of curve or surface represented as an unorganized point cloud while also estimating the differential properties of the underlying smooth manifold we present optimization algorithms to solve these mpa models and examples which show that our reconstructions of the curve or surface and estimates of the normals and curvature information are accurate for precise point clouds and robust in the presence of noise
transactional memory tm is promising paradigm for concurrent programming this paper is an overview of our recent theoretical work on defining theory of tm we first recall some tm correctness properties and then overview results on the inherent power and limitations of tms
time sequences which are ordered sets of observations have been studied in various database applications in this paper we introduce new class of time sequences where each observation is represented by an interval rather than number such sequences may arise in many situations for instance we may not be able to determine the exact value at time point due to uncertainty or aggregation such observation may be represented better by range of possible values similarity search with interval time sequences as both query and data sequences poses new challenge for research we first address the issue of dis similarity measures for interval time sequences we choose an norm based measure because it effectively quantifies the degree of overlapping and remoteness between two intervals and is invariant irrespective of the position of an interval when it is enclosed within another interval we next propose an efficient indexing technique for fast retrieval of similar interval time sequences from large databases more specifically we propose to extract segment based feature vector for each sequence and to map each feature vector to either point or hyper rectangle in multi dimensional feature space we then show how we can use existing multi dimensional index structures such as the tree for efficient query processing the proposed method guarantees no false dismissals experimental results show that for synthetic and real stock data it is superior to sequential scanning in performance and scales well with the data size
application integration can be carried out on three different levels the data source level the business logic level and the user interface level with ontologies based integration on the data source level dating back to the and semantic web services for integrating on the business logic level coming of age it is time for the next logical step employing ontologies for integration on the user interface level such an approach supports both the developer in terms of reduced development times and the user in terms of better usability of integrated applications in this paper we introduce framework employing ontologies for integrating applications on the user interface level the acm portal is published by the association for computing machinery copyright acm inc terms of usage privacy policy code of ethics contact us useful downloads adobe acrobat quicktime windows media player real player
the handling of user preferences is becoming an increasingly important issue in present day information systems among others preferences are used for information filtering and extraction to reduce the volume of data presented to the user they are also used to keep track of user profiles and formulate policies to improve and automate decision makingwe propose here simple logical framework for formulating preferences as preference formulas the framework does not impose any restrictions on the preference relations and allows arbitrary operation and predicate signatures in preference formulas it also makes the composition of preference relations straightforward we propose simple natural embedding of preference formulas into relational algebra and sql through single winnow operator parameterized by preference formula the embedding makes possible the formulation of complex preference queries for example involving aggregation by piggybacking on existing sql constructs it also leads in natural way to the definition of further preference related concepts like ranking finally we present general algebraic laws governing the winnow operator and its interactions with other relational algebra operators the preconditions on the applicability of the laws are captured by logical formulas the laws provide formal foundation for the algebraic optimization of preference queries we demonstrate the usefulness of our approach through numerous examples
block correlations are common semantic patterns in storage systems these correlations can be exploited for improving the effectiveness of storage caching prefetching data layout and disk scheduling unfortunately information about block correlations is not available at the storage system level previous approaches for discovering file correlations in file systems do not scale well enough to be used for discovering block correlations in storage systems in this paper we propose miner an algorithm which uses data mining technique called frequent sequence mining to discover block correlations in storage systems miner runs reasonably fast with feasible space requirement indicating that it is practical tool for dynamically inferring correlations in storage system moreover we have also evaluated the benefits of block correlation directed prefetching and data layout through experiments our results using real system workloads show that correlation directed prefetching and data layout can reduce average response time by compared to the base case and compared to the commonly used sequential prefetching scheme
this paper describes onechip third generation reconfigurable processor architecture that integrates reconfigurable functional unit rfu into superscalar reduced instruction set computer risc processor's pipeline the architecture allows dynamic scheduling and dynamic reconfiguration it also provides support for pre loading configurations and for least recently used lru configuration managementto evaluate the performance of the onechip architecture several off the shelf software applications were compiled and executed on sim onechip an architecture simulator for onechip that includes software environment for programming the system the architecture is compared to similar one but without dynamic scheduling and without an rfu onechip achieves performance improvement and shows speedup range from up to for the different applications and data sizes used the results show that dynamic scheduling helps performance the most on average and that the rfu will always improve performance the best when most of the execution is in the rfu
most correlation clustering algorithms rely on principal component analysis pca as correlation analysis tool the correlation of each cluster is learned by applying pca to set of sample points since pca is rather sensitive to outliers if small fraction of these points does not correspond to the correct correlation of the cluster the algorithms are usually misled or even fail to detect the correct results in this paper we evaluate the influence of outliers on pca and propose general framework for increasing the robustness of pca in order to determine the correct correlation of each cluster we further show how our framework can be applied to pca based correlation clustering algorithms thorough experimental evaluation shows the benefit of our framework on several synthetic and real world data sets
existing dram controllers employ rigid non adaptive scheduling and buffer management policies when servicing prefetch requests some controllers treat prefetch requests the same as demand requests others always prioritize demand requests over prefetch requests however none of these rigid policies result in the best performance because they do not take into account the usefulness of prefetch requests if prefetch requests are useless treating prefetches and demands equally can lead to significant performance loss and extra bandwidth consumption in contrast if prefetch requests are useful prioritizing demands over prefetches can hurt performance by reducing dram throughput and delaying the service of useful requests this paper proposes new low cost memory controller called prefetch aware dram controller padc that aims to maximize the benefit of useful prefetches and minimize the harm caused by useless prefetches to accomplish this padc estimates the usefulness of prefetch requests and dynamically adapts its scheduling and buffer management policies based on the estimates the key idea is to adaptively prioritize between demand and prefetch requests and drop useless prefetches to free up memory system resources based on the accuracy of the prefetcher our evaluation shows that padc significantly outperforms previous memory controllers with rigid prefetch handling policies on both single and multi core systems with variety of prefetching algorithms across wide range of multiprogrammed spec cpu workloads it improves system performance by on core system and by on an core system while reducing dram bandwidth consumption by and respectively
most hardware predictors are table based eg two level branch predictors and have exponential size growth in the number of input bits or features eg previous branch outcomes this growth severely limits the amount of predictive information that such predictors can use to avoid exponential growth we introduce the idea of dynamic feature selection for building hardware predictors that can use large amount of predictive information based on this idea we design the dynamic decision tree ddt predictor which exhibits only linear size growth in the number of features our initial evaluation in branch prediction shows that the general purpose ddt using only branch history features is comparable on average to conventional branch predictors opening the door to practically using large numbers of additional features
referential integrity is an essential global constraint in relational database that maintains it in complete and consistent state in this work we assume the database may violate referential integrity and relations may be denormalized we propose set of quality metrics defined at four granularity levels database relation attribute and value that measure referential completeness and consistency quality metrics are efficiently computed with standard sql queries that incorporate two query optimizations left outer joins on foreign keys and early foreign key grouping experiments evaluate our proposed metrics and sql query optimizations on real and synthetic databases showing they can help in detecting and explaining referential errors
cluster based replication solutions are an attractive mechanism to provide both high availability and scalability for the database backend within the multi tier information systems of service oriented businesses an important issue that has not yet received sufficient attention is how database replicas that have failed can be reintegrated into the system or how completely new replicas can be added in order to increase the capacity of the system ideally recovery takes place online ie while transaction processing continues at the replicas that are already running in this paper we present complete online recovery solution for database clusters one important issue is to find an efficient way to transfer the data the joining replica needs in this paper we present two data transfer strategies the first transfers the latest copy of each data item the second transfers the updates rejoining replica has missed during its downtime second challenge is to coordinate this transfer with ongoing transaction processing such that the joining node does not miss any updates we present coordination protocol that can be used with postgres replication tool which uses group communication system for replica control we have implemented and compared our transfer solutions against set of parameters and present heuristics which allow an automatic selection of the optimal strategy for given configuration
influence of items on some other items might not be the same as the association between these sets of items many tasks of data analysis are based on expressing influence of items on other items in this paper we introduce the notion of an overall influence of set of items on another set of items we also propose an extension to the notion of overall association between two items in database using the notion of overall influence we have designed two algorithms for influence analysis involving specific items in database as the number of databases increases on yearly basis we have adopted incremental approach in these algorithms experimental results are reported for both synthetic and real world databases
the growing number of information security breaches in electronic and computing systems calls for new design paradigms that consider security as primary design objective this is particularly relevant in the embedded domain where the security solution should be customized to the needs of the target system while considering other design objectives such as cost performance and power due to the increasing complexity and shrinking design cycles of embedded software most embedded systems present host of software vulnerabilities that can be exploited by security attacks many attacks are initiated by causing violation in the properties of data eg integrity privacy access control rules etc associated with trusted program that is executing on the system leading to range of undesirable effectsin this work we develop general framework that provides security assurance against wide class of security attacks our work is based on the observation that program's permissible behaviorwith respect to data accesses can be characterized by certain properties we present hardware software approach wherein such properties can be encoded as data attributes and enforced as security policies during program execution these policies may be application specific eg access control for certain data structures compiler generated eg enforcing that variables are accessed only within their scope or universally applicable to all programs eg disallowing writes to unallocated memory we show how an embedded system architecture can support such policies by enhancing the memory hierarchy to represent the attributes of each datum as security tags that are linked to it through its lifetime and ii adding configurable hardware checker that interprets the semantics of the tags and enforces the desired security policies we evaluated the effectiveness of the proposed architecture in enforcing various security policies for several embedded benchmarks our experiments in the context of the simplescalar framework demonstrate that the proposed solution ensures run time validation of program data properties with minimal execution time overheads
given two sets of moving objects future timestamp tq and distance threshold spatio temporal join retrieves all pairs of objects that are within distance at tq the selectivity of join equals the number of retrieved pairs divided by the cardinality of the cartesian product this paper develops model for spatio temporal join selectivity estimation based on rigorous probabilistic analysis and reveals the factors that affect the selectivity initially we solve the problem for id point and rectangle objects whose location and velocities distribute uniformly and then extend the results to multi dimensional spaces finally we deal with non uniform distributions using specialized spatio temporal histogram extensive experiments confirm that the proposed formulae are highly accurate average error below
we give semantics to polymorphic effect analysis that tracks possibly thrown exceptions and possible non termination for higher order language the semantics is defined using partial equivalence relations over standard monadic domain theoretic model of the original language and establishes the correctness of both the analysis itself and of the contextual program transformations that it enables
the need to visualize large social networks is growing as hardware capabilities make analyzing large networks feasible and many new data sets become available unfortunately the visualizations in existing systems do not satisfactorily resolve the basic dilemma of being readable both for the global structure of thenetwork and also for detailed analysis of local communities to address this problem we present nodetrix hybrid representation for networks that combines the advantages of two traditional representations node link diagrams are used to show the global structure of network while arbitrary portions of the network can be shown as adjacency matrices to better support the analysis of communities key contribution is set of interaction techniques these allow analysts to create nodetrix visualization by dragging selections to and from node link and matrix forms and to flexibly manipulate the nodetrix representation to explore the dataset andcreate meaningful summary visualizations of their findings finally we present case study applying nodetrix to the analysis of the infovis coauthorship dataset to illustrate the capabilities of nodetrix as both an exploration tool and an effective means of communicating results
the artifacts constituting software system often drift apart over time we have developed the software reflexion model technique to help engineers perform various software engineering tasks by exploiting rather than removing the drift between design and implementation more specifically the technique helps an engineer compare artifacts by summarizing where one artifact such as design is consistent with and inconsistent with another artifact such as source the technique can be applied to help software engineer evolve structural mental model of system to the point that it is good enough to be used for reasoning about task at hand the software reflexion model technique has been applied to support variety of tasks including design conformance change assessment and an experimental reengineering of the million lines of code microsoft excel product in this paper we provide formal characterization of the reflexion model technique discuss practical aspects of the approach relate experiences of applying the approach and tools and place the technique into the context of related work
abstraction and slicing are both techniques for reducing the size of the state space to be inspected during verification in this paper we present new model checking procedure for infinite state concurrent systems that interleaves automatic abstraction refinement which splits states according to new predicates obtained by craig interpolation with slicing which removes irrelevant states and transitions from the abstraction the effects of abstraction and slicing complement each other as the refinement progresses the increasing accuracy of the abstract model allows for more precise slice the resulting smaller representation gives room for additional predicates in the abstraction the procedure terminates when an error path in the abstraction can be concretized which proves that the system is erroneous or when the slice becomes empty which proves that the system is correct
compressing the instructions of an embedded program is important for cost sensitive low power control oriented embedded computing number of compression schemes have been proposed to reduce program size however the increased instruction density has an accompanying performance cost because the instructions must be decompressed before execution in this paper we investigate the performance penalty of hardware managed code compression algorithm recently introduced in ibm's powerpc this scheme is the first to combine many previously proposed code compression techniques making it an ideal candidate for study we find that code compression with appropriate hardware optimizations does not have to incur much performance loss furthermore our studies show this holds for architectures with wide range of memory configurations and issue widths surprisingly we find that performance increase over native code is achievable in many situations
detecting code clones has many software engineering applications existing approaches either do not scale to large code bases or are not robust against minor code modifications in this paper we present an efficient algorithm for identifying similar subtrees and apply it to tree representations of source code our algorithm is based on novel characterization of subtrees with numerical vectors in the euclidean space mathbb and an efficient algorithm to cluster these vectors wrt the euclidean distance metric subtrees with vectors in one cluster are considered similar we have implemented our tree similarity algorithm as clone detection tool called deckard and evaluated it on large code bases written in and java including the linux kernel and jdk our experiments show that deckard is both scalable and accurate it is also language independent applicable to any language with formally specified grammar
this paper presents an initial study of multimodal collaborative platform concerning user preferences and interaction technique adequacy towards task true collaborative interactions are missing aspect of the majority of nowadays multi user system on par with the lack of support towards impaired users in order to surpass these obstacles we provide an accessible platform for co located collaborative environments which aims at not only improving the ways users interact within them but also at exploring novel interaction patterns brief study regarding set of interaction techniques and tasks was conducted in order to assess the most suited modalities in certain settings we discuss the results drawn from this study detail some related conclusions and present future work directions
password authenticated key exchange pake protocols allow parties to share secret keys in an authentic manner based on an easily memorizable password recently lu and cao proposed three party password authenticated key exchange protocol so called pake based on ideas of the abdalla and pointcheval two party spake extended to three parties pake can be seen to have structure alternative to that of another three party pake protocol pake by abdalla and pointcheval furthermore simple improvement to pake was proposed very recently by chung and ku to resist the kind of attacks that applied to earlier versions of pake in this paper we show that pake falls to unknown key share attacks by any other client and undetectable online dictionary attacks by any adversary the latter attack equally applies to the recently improved pake indeed the provable security approach should be taken when designing pakes and furthermore our results highlight that extra cautions still be exercised when defining models and constructing proofs in this direction
efficient construction of inverted indexes is essential to provision of search over large collections of text data in this article we review the principal approaches to inversion analyze their theoretical cost and present experimental results we identify the drawbacks of existing inversion approaches and propose single pass inversion method that in contrast to previous approaches does not require the complete vocabulary of the indexed collection in main memory can operate within limited resources and does not sacrifice speed with high temporary storage requirements we show that the performance of the single pass approach can be improved by constructing inverted files in segments reducing the cost of disk accesses during inversion of large volumes of data
materialized xpath access control views are commonly used for enforcing access control when access control rules defining materialized xml access control view change the view must be adapted to reflect these changes the process of updating materialized view after its definition changes is referred to as view adaptation while xpath security views have been widely reported in literature the problem of view adaptation for xpath security views has not been addressed view adaptation results in view downtime during which users are denied access to security views to prevent unauthorized access thus efficient view adaptation is important for making xpath security views pragmatic in this work we show how to adapt an xpath access control view incrementally by re using the existing view which reduces computation and communication costs significantly and results in less downtime for the end user empirical evaluations confirm that the incremental view adaptation algorithms presented in this paper are efficient and scalable
class imbalance where the classes in dataset are not represented equally is common occurrence in machine learning classification models built with such datasets are often not practical since most machine learning algorithms would tend to perform poorly on the minority class instances we present unique evolutionary computing based data sampling approach as an effective solution for the class imbalance problem the genetic algorithm based approach evolutionary sampling works as majority undersampling technique where instances from the majority class are selectively removed this preserves the relative integrity of the majority class while maintaining the original minority class group our research prototype evann also implements genetic algorithm based optimization of modeling parameters for the machine learning algorithms considered in our study an extensive empirical investigation involving four real world datasets is performed comparing the proposed approach to other existing data sampling techniques that target the class imbalance problem our results demonstrate that evolutionary sampling both with and without learner optimization performs relatively better than other data sampling techniques detailed coverage of our case studies in this paper lends itself toward empirical replication
high level languages are growing in popularity however decades of software development have produced large libraries of fast time tested meritorious code that are impractical to recreate from scratch cross language bindings can expose low level code to high level languages unfortunately writing bindings by hand is tedious and error prone while mainstream binding generators require extensive manual annotation or fail to offer the language features that users of modern languages have come to expect we present an improved binding generation strategy based on static analysis of unannotated library source code we characterize three high level idioms that are not uniquely expressible in c's low level type system array parameters resource managers and multiple return values we describe suite of interprocedural analyses that recover this high level information and we show how the results can be used in binding generator for the python programming language in experiments with four large libraries we find that our approach avoids the mistakes characteristic of hand written bindings while offering level of python integration unmatched by prior automated approaches among the thousands of functions in the public interfaces of these libraries roughly exhibit the behaviors detected by our static analyses
the ability to predict at compile time the likelihood of particular branch being taken provides valuable information for several optimizations including global instruction scheduling code layout function inlining interprocedural register allocation and many high level optimizations previous attempts at static branch prediction have either used simple heuristics which can be quite inaccurate or put the burden onto the programmer by using execution profiling data or source code hintsthis paper presents new approach to static branch prediction called value range propagation this method tracks the weighted value ranges of variables through program much like constant propagation these value ranges may be either numeric of symbolic in nature branch prediction is then performed by simply consulting the value range of the appropriate variable heuristics are used as fallback for cases where the value range of the variable cannot be determined statically in the process value range propagationsubsumes both constant propagation and copy propagationexperimental results indicate that this approach produces significantly more accurate predictions than the best existing heuristic techniques the value range propagation method can be implemented over any ldquo factored rdquo dataflow representation with static single assignment property such as ssa form or dependence flow graph where the variables have been renamed to achieve single assignment experimental results indicate that the technique maintains the linear runtime behavior of constant propagation experienced in practice
in this paper we study the problem of effective keyword search over xml documents we begin by introducing the notion of valuable lowest common ancestor vlca to accurately and effectively answer keyword queries over xml documents we then propose the concept of compact vlca cvlca and compute the meaningful compact connected trees rooted as cvlcas as the answers of keyword queries to efficiently compute cvlcas we devise an effective optimization strategy for speeding up the computation and exploit the key properties of cvlca in the design of the stack based algorithm for answering keyword queries we have conducted an extensive experimental study and the experimental results show that our proposed approach achieves both high efficiency and effectiveness when compared with existing proposals
decision support systems help the decision making process with the use of olap on line analytical processing and data warehouses these systems allow the analysis of corporate data as olap and data warehousing evolve more and more complex data is being used xml extensible markup language is flexible text format allowing the interchange and the representation of complex data finding an appropriate model for an xml data warehouse tends to become complicated as more and more solutions appear hence in this survey paper we present an overview of the different proposals that use xml within data warehousing technology these proposals range from using xml data sources for regular warehouses to those using full xml warehousing solutions some researches merely focus on document storage facilities while others present adaptations of xml technology for olap even though there are growing number of researches on the subject many issues still remain unsolved
in this paper we propose an extension algorithm to closet one of the most efficient algorithms for mining frequent closed itemsets in static transaction databases to allow it to mine frequent closed itemsets in dynamic transaction databases in dynamic transaction database transactions may be added deleted and modified with time based on two variant tree structures our algorithm retains the previous mined frequent closed itemsets and updates them by considering the changes in the transaction databases only hence the frequent closed itemsets in the current transaction database can be obtained without rescanning the entire changed transaction database the performance of the proposed algorithm is compared with closet showing performance improvements for dynamic transaction databases compared to using mining algorithms designed for static transaction databases
this paper explores the suitability of dense circulant graphs of degree four for the design of on chip interconnection networks networks based on these graphs reduce the torus diameter in factor which translates into significant performance gains for unicast traffic in addition they are clearly superior to tori when managing collective communications this paper introduces new two dimensional node's labeling of the networks explored which simplifies their analysis and exploitation in particular it provides simple and optimal solutions to two important architectural issues routing and broadcasting other implementation issues such as network folding and scalability by using hierarchical networks are also explored in this work
wireless sensors are very small computers and understanding the timing and behavior of software written for them is crucial to ensuring that they perform correctly this paper outlines lightweight method for gathering behavioral and timing information from simulated executions of software written in the nesc tinyos environment the resulting data is used to generate both behavioral and timing profiles of the software using uml sequence diagrams to visualize the behavior and to present the timing information
repeated elements are ubiquitous and abundant in both manmade and natural scenes editing such images while preserving the repetitions and their relations is nontrivial due to overlap missing parts deformation across instances illumination variation etc manually enforcing such relations is laborious and error prone we propose novel framework where user scribbles are used to guide detection and extraction of such repeated elements our detection process which is based on novel boundary band method robustly extracts the repetitions along with their deformations the algorithm only considers the shape of the elements and ignores similarity based on color texture etc we then use topological sorting to establish partial depth ordering of overlapping repeated instances missing parts on occluded instances are completed using information from other instances the extracted repeated instances can then be seamlessly edited and manipulated for variety of high level tasks that are otherwise difficult to perform we demonstrate the versatility of our framework on large set of inputs of varying complexity showing applications to image rearrangement edit transfer deformation propagation and instance replacement
we investigate the problem of optimizing the routing performance of virtual network by adding extra random links our asynchronous and distributed algorithm ensures by adding single extra link per node that the resulting network is navigable small world ie in which greedy routing using the distance in the original network computes paths of polylogarithmic length between any pair of nodes with probability previously known small world augmentation processes require the global knowledge of the network and centralized computations which is unrealistic for large decentralized networks our algorithm based on careful multi layer sampling of the nodes and the construction of light overlay network bypasses these limitations for bounded growth graphs ie graphs where for any node and any radius the number of nodes within distance from is at most constant times the number of nodes within distance our augmentation process proceeds with high probability in log log communication rounds with log log messages of size log bits sent per node and requiring only log log bit space in each node where is the number of nodes and the diameter in particular with the only knowledge of original distances greedy routing computes between any pair of nodes in the augmented network path of length at most log log with probability and of expected length log log hence we provide distributed scheme to augment any bounded growth graph into small world with high probability in polylogarithmic time while requiring polylogarithmic memory we consider that the existence of such lightweight process might be first step towards the definition of more general construction process that would validate kleinberg's model as plausible explanation for the small world phenomenon in large real interaction networks
to achieve interoperability modern information systems and commerce applications use mappings to translate data from one representation to another in dynamic environments like the web data sources may change not only their data but also their schemas their semantics and their query capabilities such changes must be reflected in the mappings mappings left inconsistent by schema change have to be detected and updated as large complicated schemas become more prevalent and as data is reused in more applications manually maintaining mappings even simple mappings like view definitions is becoming impractical we present novel framework and tool tomas for automatically adapting mappings as schemas evolve our approach considers not only local changes to schema but also changes that may affect and transform many components of schema we consider comprehensive class of mappings for relational and xml schemas with choice types and nested constraints our algorithm detects mappings affected by structural or constraint change and generates all the rewritings that are consistent with the semantics of the mapped schemas our approach explicitly models mapping choices made by user and maintains these choices whenever possible as the schemas and mappings evolve we describe an implementation of mapping management and adaptation tool based on these ideas and compare it with mapping generation tool
in this paper we discuss the energy efficient multicast problem in ad hoc wireless networks each node in the network is assumed to have fixed level of transmission power the problem of our concern is given an ad hoc wireless network and multicast request how to find multicast tree such that the total energy cost of the multicast tree is minimized we first prove this problem is np hard and it is unlikely to have an approximation algorithm with constant performance ratio of the number of nodes in the network we then propose an algorithm based on the directed steiner tree method that has theoretically guaranteed approximation performance ratio we also propose two efficient heuristics node join tree njt and tree join tree tjt algorithms the njt algorithm can be easily implemented in distributed fashion extensive simulations have been conducted to compare with other methods and the results have shown significant improvement on energy efficiency of the proposed algorithms
the key notion in service oriented architecture is decoupling clients and providers of service based on an abstract service description which is used by the service broker to point clients to suitable service implementation client then sends service requests directly to the service implementation problem with the current architecture is that it does not provide trustworthy means for clients to specify service brokers to verify and service implementations to prove that certain desired non functional properties are satisfied during service request processing an example of such non functional property is access and persistence restrictions on the data received as part of the service requests in this work we propose an extension of the service oriented architecture that provides these facilities we also discuss prototype implementation of this architecture and report preliminary results that demonstrate the potential practical value of the proposed architecture in real world software applications
virtual humans are being increasingly used in different domains virtual human modeling requires to consider aspects belonging to different levels of abstractions for example at lower levels one has to consider aspects concerning the geometric definition of the virtual human model and appearance while at higher levels one should be able to define how the virtual human behaves into an environment anim the standard for representing humanoids in xd vrml worlds is mainly concerned with low level modeling aspects as result the developer has to face the problem of defining the virtual human behavior and translating it into lower levels eg geometrical and kinematic aspects in this paper we propose vha virtual human architecture software architecture that allows one to easily manage an interactive anim virtual human into xd vrml worlds the proposed solution allows the developer to focus mainly on high level aspects of the modeling process such as the definition of the virtual human behavior
robotic tape libraries are popular for applications with very high storage requirements such as video servers here we study the throughput of tape library system we design new scheduling algorithm the so called relief and compare it against some older straightforward ones like fcfs maximum queue length mql and an unfair one bypass roughly equivalent to shortest job first the proposed algorithm incorporates an aging mechanism in order to attain fairness and we prove that under certain assumptions it minimizes the average start up latency extensive simulation experiments show that relief outperforms its competitors fair and unfair alike with up to improvement in throughput for the same rejection ratio
this paper demonstrates the advantages of using controlled mobility in wireless sensor networks wsns for increasing their lifetime ie the period of time the network is able to provide its intended functionalities more specifically for wsns that comprise large number of statically placed sensor nodes transmitting data to collection point the sink we show that by controlling the sink movements we can obtain remarkable lifetime improvements in order to determine sink movements we first define mixed integer linear programming milp analytical model whose solution determines those sink routes that maximize network lifetime our contribution expands further by defining the first heuristics for controlled sink movements that are fully distributed and localized our greedy maximum residual energy gmre heuristic moves the sink from its current location to new site as if drawn toward the area where nodes have the highest residual energy we also introduce simple distributed mobility scheme random movement or rm according to which the sink moves uncontrolled and randomly throughout the network the different mobility schemes are compared through extensive ns based simulations in networks with different nodes deployment data routing protocols and constraints on the sink movements in all considered scenarios we observe that moving the sink always increases network lifetime in particular our experiments show that controlling the mobility of the sink leads to remarkable improvements which are as high as sixfold compared to having the sink statically and optimally placed and as high as twofold compared to uncontrolled mobility
the method described in this article evaluates case similarity in the retrieval stage of case based reasoning cbr it thus plays key role in deciding which case to select and therefore in deciding which solution will be eventually applied in cbr there are many retrieval techniques one feature shared by most is that case retrieval is based on attribute similarity and importance however there are other crucial factors that should be considered such as the possible consequences of given solution in other words its potential loss and gain as their name clearly implies these concepts are defined as functions measuring loss and gain when given retrieval case solution is applied moreover these functions help the user to choose the best solution so that when mistake is made the resulting loss is minimal in this way the highest benefit is always obtained
activity centric computing acc systems seek to address the fragmentation of office work across tools and documents by allowing users to organize work around the computational construct of an activity defining and structuring appropriate activities within system poses challenge for users that must be overcome in order to benefit from acc support we know little about how knowledge workers appropriate the activity construct to address this we studied users appropriation of production quality acc system lotus activities for everyday work by employees in large corporation we contribute to better understanding of how users articulate their individual and collaborative work in the system by providing empirical evidence of their patterns of appropriation we conclude by discussing how our findings can inform the design of other acc systems for the workplace
we present an interactive system for synthesizing urban layouts by example our method simultaneously performs both structure based synthesis and an image based synthesis to generate complete urban layout with plausible street network and with aerial view imagery our approach uses the structure and image data of real world urban areas and synthesis algorithm to provide several high level operations to easily and interactively generate complex layouts by example the user can create new urban layouts by sequence of operations such as join expand and blend without being concerned about low level structural details further the ability to blend example urban layout fragments provides powerful way to generate new synthetic content we demonstrate our system by creating urban layouts using example fragments from several real world cities each ranging from hundreds to thousands of city blocks and parcels
dimensionality reduction is an essential data preprocessing technique for large scale and streaming data classification tasks it can be used to improve both the efficiency and the effectiveness of classifiers traditional dimensionality reduction approaches fall into two categories feature extraction and feature selection techniques in the feature extraction category are typically more effective than those in feature selection category however they may break down when processing large scale data sets or data streams due to their high computational complexities similarly the solutions provided by the feature selection approaches are mostly solved by greedy strategies and hence are not ensured to be optimal according to optimized criteria in this paper we give an overview of the popularly used feature extraction and selection algorithms under unified framework moreover we propose two novel dimensionality reduction algorithms based on the orthogonal centroid algorithm oc the first is an incremental oc ioc algorithm for feature extraction the second algorithm is an orthogonal centroid feature selection ocfs method which can provide optimal solutions according to the oc criterion both are designed under the same optimization criterion experiments on reuters corpus volume data set and some public large scale text data sets indicate that the two algorithms are favorable in terms of their effectiveness and efficiency when compared with other state of the art algorithms
most large public displays have been used for providing information to passers by with the primary purpose of acting as one way information channels to individual users we have developed large public display to which users can send their own media content using mobile devices the display supports multi touch interaction thus enabling collaborative use of the display this display called citywall was set up in city center with the goal of showing information of events happening in the city we observed two user groups who used mobile phones with upload capability during two large scale events happening in the city our findings are that this kind of combined use of personal mobile devices and large public display as publishing forum used collaboratively with other users creates unique setting that extends the group's feeling of participation in the events we substantiate this claim with examples from user data
abstract almost all semantics for logic programs with negation identify set sem of models of program as the intended semantics of and any model in this class is considered possible meaning of with regard to the semantics the user has in mind thus for example in the case of stable models check end of sentence choice models check end of sentence answer sets check end of sentence etc different possible models correspond to different ways of completing the incomplete information in the logic program however different end users may have different ideas on which of these different models in sem is reasonable one from their point of view for instance given sem user may prefer model in sem to model in sem based on some evaluation criterion that she has in this paper we develop logic program semantics based on optimal models this semantics does not add yet another semantics to the logic programming arena it takes as input an existing semantics sem and user specified objective function obj and yields new semantics underline rm opt subseteq sem that realizes the objective function within the framework of preferred models identified already by sem thus the user who may or may not know anything about logic programming has considerable flexibility in making the system reflect her own objectives by building on top of existing semantics known to the system in addition to the declarative semantics we provide complete complexity analysis and algorithms to compute optimal models under varied conditions when sem is the stable model semantics the minimal models semantics and the all models semantics
the growing nature of databases and the flexibility inherent in the sql query language that allows arbitrarily complex formulations can result in queries that take inordinate amount of time to complete to mitigate this problem strategies that are optimized to return the first few rows or top rows in case of sorted results are usually employed however both these strategies can lead to unpredictable query processing times thus in this paper we propose supporting time constrained sql queries specifically user issues sql query as before but additionally provides nature of constraint soft or hard an upper bound for query processing time and acceptable nature of results partial or approximate the dbms takes the criteria constraint type time limit quality of result into account in generating the query execution plan which is expected guaranteed to complete in the allocated time for soft hard time constraint if partial results are acceptable then the technique of reducing result set cardinality ie returning first few or top rows is used whereas if approximate results are acceptable then sampling is used to compute query results within the specified time limit for the latter case we argue that trading off quality of results for predictable response time is quite useful however for this case we provide additional aggregate functions to estimate the aggregate values and to compute the associated confidence interval this paper presents the notion of time constrained sql queries discusses the challenges in supporting such construct describes framework for supporting such queries and outlines its implementation in oracle database by exploiting oracle's cost based optimizer and extensibility capabilities
scenarios have been advocated as means of improving requirements engineering yet few methods or tools exist to support scenario based re the paper reports method and software assistant tool for scenario based re that integrates with use case approaches to object oriented development the method and operation of the tool are illustrated with financial system case study scenarios are used to represent paths of possible behavior through use case and these are investigated to elaborate requirements the method commences by acquisition and modeling of use case the use case is then compared with library of abstract models that represent different application classes each model is associated with set of generic requirements for its class hence by identifying the class es to which the use case belongs generic requirements can be reused scenario paths are automatically generated from use cases then exception types are applied to normal event sequences to suggest possible abnormal events resulting from human error generic requirements are also attached to exceptions to suggest possible ways of dealing with human error and other types of system failure scenarios are validated by rule based frames which detect problematic event patterns the tool suggests appropriate generic requirements to deal with the problems encountered the paper concludes with review of related work and discussion of the prospects for scenario based re methods and tools
the new hybrid clone detection tool nicad combines the strengths and overcomes the limitations of both text based and ast based clone detection techniques and exploits novel applications of source transformation system to yield highly accurate identification of cloned code in software systems in this paper we present an in depth study of near miss function clones in open source software using nicad we examine more than open source java and num systems including the entire linux kernel apache httpd jsdk swing and dbo and compare their use of cloned code in several different dimensions including language clone size clone similarity clone location and clone density both by proportion of cloned functions and lines of cloned code we manually verify all detected clones and provide complete catalogue of different clones in an online repository in variety of formats these validated results can be used as cloning reference for these systems and as benchmark for evaluating other clone detection tools copyright copy john wiley sons ltd in this paper we provide an empirical study of function clones in more than open source java and num systems of varying kinds and sizes including the entire linux kernel using the new hybrid clone detection method nicad we manually verify all the detected clones and provide complete catalogue of the different clones in an online repository in variety of formats our studies show that there are large number of near miss function clones in those systems copyright copy john wiley sons ltd
mobile agents are becoming increasingly important in the highly distributed applications frameworks seen today their routing dispatching from node to node is very important issue as we need to safeguard application efficiency achieve better load balancing and resource utilization throughout the underlying network selecting the best target server for dispatching mobile agent is therefore multi faceted problem that needs to be carefully tackled in this paper we propose distributed adaptive routing schemes next node selection for mobile agents the proposed schemes overcome risks like load oscillations ie agents simultaneously abandoning congested node in search for other less saturated node we try to induce different routing decisions taken by agents to achieve load balancing and better utilization of network resources we consider five different algorithms and evaluate them through simulations our findings are quite promising both from the user application and the network infrastructure perspective
the evolution of geographic phenomena has been one of the concerns of spatiotemporal database research however in large spectrum of geographical applications users need more than mere representation of data evolution for instance in urban management applications mdash eg cadastral evolution mdash users often need to know why how and by whom certain changes have been performed as well as their possible impact on the environment answers to such queries are not possible unless supplementary information concerning real world events is associated with the corresponding changes in the database and is managed efficiently this paper proposes solution to this problem which is based on extending spatiotemporal database with mechanism for managing documentation on the evolution of geographic information this solution has been implemented in gis based prototype which is also discussed in the paper
we describe an efficient top down strategy for overlap removal and floorplan repair which repairs overlaps in floorplans produced by placement algorithms or rough floorplanning methodologies the algorithmic framework that we propose incorporates novel geometric shifting technique within top down flow the effect of our algorithm is quantified across broad range of floorplans produced by multiple tools our method succeeds in producing valid placements in almost all cases moreover compared to leading methods it requires only one fifth the run time and produces placements with to less hpwl and up to less cell movement
in this paper we describe conceptual framework and address the related issues and solutions in the identification of three major challenges for the development and evaluation of immersive digital educational games idegs these challenges are advancing adaptive educational technologies to shape learning experience ensuring the individualization of learning experiences adaptation to personal aims needs abilities and prerequisites ii providing technological approaches to reduce the development costs for idegs by enabling the creation of entirely different stories and games for variety of different learning domains each based more or less on the same pool of story units patterns and structures iii developing robust evaluation methodologies for idegs by the extension of iso to include user satisfaction motivation and learning progress and other user experience ux attributes while our research and development is by no means concluded we believe that we have arrived at stage where conclusions may be drawn which will be of considerable use to other researchers in this domain
recently lot of work has been done on formalization of business process specification in particular using petri nets and process algebra however these efforts usually do not explicitly address complex business process development which necessitates the specification coordination and synchronization of large number of business steps it is imperative that these atomic tasks are associated correctly and monitored for countless dependencies moreover as these business processes grow they become critically reliant on large number of split and merge points which additionally increases modeling complexity therefore one of the central challenges in complex business process modeling is the composition of dependent business steps we address this challenge and introduce formally correct method for automated composition of algebraic expressions in complex business process modeling based on acyclic directed graph reductions we show that our method generates an equivalent algebraic expression from an appropriate acyclic directed graph if the graph is well formed and series parallel additionally we encapsulate the reductions in an algorithm that transforms business step dependencies described by users into digraphs recognizes structural conflicts identifies wheatstone bridges and finally generates algebraic expressions
to narrow the semantic gap in content based image retrieval cbir relevance feedback is utilized to explore knowledge about the user's intention in finding target image or image category users provide feedback by marking images returned in response to query image as relevant or irrelevant existing research explores such feedback to refine querying process select features or learn image classifier however the vast amount of unlabeled images is ignored and often substantially limited examples are engaged into learning in this paper we address the two issues and propose novel effective method called relevance aggregation projections rap for learning potent subspace projections in semi supervised way given relevances and irrelevances specified in the feedback rap produces subspace within which the relevant examples are aggregated into single point and the irrelevant examples are simultaneously separated by large margin regarding the query plus its feedback samples as labeled data and the remainder as unlabeled data rap falls in special paradigm of imbalanced semi supervised learning through coupling the idea of relevance aggregation with semi supervised learning we formulate constrained quadratic optimization problem to learn the subspace projections which entail semantic mining and therefore make the underlying cbir system respond to the user's interest accurately and promptly experiments conducted over large generic image database show that our subspace approach outperforms existing subspace methods for cbir even with few iterations of user feedback
applications that analyze mine and visualize large datasets are considered an important class of applications in many areas of science engineering and business queries commonly executed in data analysis applications often involve user defined processing of data and application specific data structures if data analysis is employed in collaborative environment the data server should execute multiple such queries simultaneously to minimize the response time to clients in this paper we present the design of runtime system for executing multiple query workloads on shared memory machine we describe experimental results using an application for browsing digitized microscopy images
in recent years classification learning for data streams has become an important and active research topic major challenge posed by data streams is that their underlying concepts can change over time which requires current classifiers to be revised accordingly and timely to detect concept change common methodology is to observe the online classification accuracy if accuracy drops below some threshold value concept change is deemed to have taken place an implicit assumption behind this methodology is that any drop in classification accuracy can be interpreted as symptom of concept change unfortunately however this assumption is often violated in the real world where data streams carry noise that can also introduce significant reduction in classification accuracy to compound this problem traditional noise cleansing methods are incompetent for data streams those methods normally need to scan data multiple times whereas learning for data streams can only afford one pass scan because of data's high speed and huge volume another open problem in data stream classification is how to deal with missing values when new instances containing missing values arrive how learning model classifies them and how the learning model updates itself according to them is an issue whose solution is far from being explored to solve these problems this paper proposes novel classification algorithm flexible decision tree flexdt which extends fuzzy logic to data stream classification the advantages are three fold first flexdt offers flexible structure to effectively and efficiently handle concept change second flexdt is robust to noise hence it can prevent noise from interfering with classification accuracy and accuracy drop can be safely attributed to concept change third it deals with missing values in an elegant way extensive evaluations are conducted to compare flexdt with representative existing data stream classification algorithms using large suite of data streams and various statistical tests experimental results suggest that flexdt offers significant benefit to data stream classification in real world scenarios where concept change noise and missing values coexist
as more and more human motion data are becoming widely used to animate computer graphics figures in many applications the growing need for compact storage and fast transmission makes it imperative to compress motion data we propose data driven method for efficient compression of human motion sequences by exploiting both spatial and temporal coherences of the data we first segment motion sequence into subsequences such that the poses within subsequence lie near low dimensional linear space we then compress each segment using principal component analysis our method achieves further compression by storing only the key frames projections to the principal component space and interpolating the other frames in between via spline functions the experimental results show that our method can achieve significant compression rate with low reconstruction errors
wireless mesh networks wmns can provide seamless broadband connectivity to network users with low setup and maintenance costs to support next generation applications with real time requirements however these networks must provide improved quality of service guarantees current mesh protocols use techniques that fail to accurately predict the performance of end to end paths and do not optimize performance based on knowledge of mesh network structures in this paper we propose quorum routing protocol optimized for wmns that provides accurate qos properties by correctly predicting delay and loss characteristics of data traffic quorum integrates novel end to end packet delay estimation mechanism with stability aware routing policies allowing it to more accurately follow qos requirements while minimizing misbehavior of selfish nodes
this paper presents method for acquiring concession strategy of an agent in multi issue negotiation this method learns how to make concession to an opponent for realizing win win negotiation to learn the concession strategy we adopt reinforcement learning first an agent receives proposal from an opponent the agent recognizes negotiation state using the difference between their proposals and the difference between their concessions according to the state the agent makes proposal by reinforcement learning reward of the learning is profit of an agreement and punishment of negotiation breakdown the experimental results showed that the agents could acquire the negotiation strategy that avoids negotiation breakdown and increases profits of an agreement as result agents can acquire the action policy that strikes balance between cooperation and competition
the capabilities of current mobile devices especially pdas are making it possible to design and develop mobile applications that employ visual techniques for using geographic data in the field these applications can be extremely useful in areas as diverse as tourism business natural resources management and homeland security in this paper we present system aimed at supporting users in the exploratory analysis of geographic data on pdas through highly interactive interface based on visual dynamic queries we propose alternative visualizations to display query results and present an experimental evaluation aimed at comparing their effectiveness on pda in tourist scenario our findings provide an experimental confirmation of the unsuitability of the typical visualization employed by classic dynamic query systems which displays only those results that fully satisfy query in those cases where only sub optimal results are obtainable for such cases the results of our study highlight the usefulness of visualizations that display all results and their degree of satisfaction of the query
the problem confronted in the content based image retrieval research is the semantic gap between the low level feature representing and high level semantics in the images this paper describes way to bridge such gap by learning the similar images given from the user the system extracts the similar region pairs and classifies those similar region pairs either as object or non object semantics and either as object relation or non object relation semantics automatically which are obtained from comparing the distances and spatial relationships in the similar region pairs by themselves the system also extracts interesting parts of the features from the similar region pair and then adjusts each interesting feature and region pair weight dynamically using those objects and object relation semantics as well as the dynamic weights adjustment from the similar images the semantics of those similar images can be mined and used for searching the similar images the experiments show that the proposed system can retrieve the similar images well and efficient
in this paper some studies have been made on the essence of fuzzy linear discriminant analysis lda algorithm and fuzzy support vector machine fsvm classifier respectively as kernel based learning machine fsvm is represented with the fuzzy membership function while realizing the same classification results with that of the conventional pair wise classification it outperforms other learning machines especially when unclassifiable regions still remain in those conventional classifiers however serious drawback of fsvm is that the computation requirement increases rapidly with the increase of the number of classes and training sample size to address this problem an improved fsvm method that combines the advantages of fsvm and decision tree called dt fsvm is proposed firstly furthermore in the process of feature extraction reformative lda algorithm based on the fuzzy nearest neighbors fknn is implemented to achieve the distribution information of each original sample represented with fuzzy membership grade which is incorporated into the redefinition of the scatter matrices in particular considering the fact that the outlier samples in the patterns may have some adverse influence on the classification result we developed novel lda algorithm using relaxed normalized condition in the definition of fuzzy membership function thus the classification limitation from the outlier samples is effectively alleviated finally by making full use of the fuzzy set theory complete lda cf lda framework is developed by combining the reformative lda rf lda feature extraction method and dt fsvm classifier this hybrid fuzzy algorithm is applied to the face recognition problem extensive experimental studies conducted on the orl and nust face images databases demonstrate the effectiveness of the proposed algorithm
the region analysis of tofte and talpin is an attempt to determine statically the life span of dynamically allocated objects but the calculus is at once intuitively simple yet deceptively subtle and previous theoretical analyses have been frustratingly complex no analysis has revealed and explained in simple terms the connection between the subleties of the calculus and the imperative features it builds on we present novel approach for proving safety and correctness of simplified version of the region calculus we give stratified operational semantics composed of highlevel semantics dealing with the conceptual difficulties of effect annotations and low level one with explicit operations on region indexed store the main results of the paper are proof simpler than previous ones and modular approach to type safety and correctness the flexibility of this approach is demonstrated by the simplicity of the extension to the full calculus with type and region polymorphism
ontologies enable to directly encode domain knowledge in software applications so ontology based systems can exploit the meaning of information for providing advanced and intelligent functionalities one of the most interesting and promising application of ontologies is information extraction from unstructured documents in this area the extraction of meaningful information from pdf documents has been recently recognized as an important and challenging problem this paper proposes an ontology based information extraction system for pdf documents founded on well suited knowledge representation approach named self populating ontology spo the spo approach combines object oriented logic based features with formal grammar capabilities and allows expressing knowledge in term of ontology schemas instances and extraction rules called descriptors aimed at extracting information having also tabular form the novel aspect of the spo approach is that it allows to represent ontologies enriched by rules that enable them to populate them self with instances extracted from unstructured pdf documents in the paper the tractability of the spo approach is proven moreover features and behavior of the prototypical implementation of the spo system are illustrated by means of running example
using wireless peer to peer interactions between portable devices it is possible to locally share information and maintain spatial temporal knowledge emanating from the surroundings we consider the prospects for unleashing ambient data from the surrounding environment for information provision using two biological phenomena human mobility and human social interaction this leads to analogies with epidemiology and is highly relevant to future technology rich environments here embedded devices in the physical environment such as sensors and wireless enabled appliances represent information sources that can provide extensive situated information in this paper we address candidate scenario where isolated sensors in the environment provide real time data from fixed locations using simulation we examine what happens when information is greedily acquired and shared by mobile participants through peer to peer interaction this is assessed taking into account availability of source nodes and the effects of mobility with respect to temporal accuracy of information the results reaffirm the need to consider range of mobility models in testing and validating protocols
we study adding aggregate operators such as summing up elements of column of relation to logics with counting mechanisms the primary motivation comes from database applications where aggregate operators are present in all real life query languages unlike other features of query languages aggregates are not adequately captured by the existing logical formalisms consequently all previous approaches to analyzing the expressive power of aggregation were only capable of producing partial results depending on the allowed class of aggregate and arithmetic operationswe consider powerful counting logic and extend it with the set of all aggregate operators we show that the resulting logic satisfies analogs of hanf's and gaifman's theorems meaning that it can only express local properties we consider database query language that expresses all the standard aggregates found in commercial query languages and show how it can be translated into the aggregate logic thereby providing number of expressivity bounds that do not depend on particular class of arithmetic functions and that subsume all those previously known we consider restricted aggregate logic that gives us tighter capture of database languages and also use it to show that some questions on expressivity of aggregation cannot be answered without resolving some deep problems in complexity theory
while there have been advances in visualization systems particularly in multi view visualizations and visual exploration the process of building visualizations remains major bottleneck in data exploration we show that provenance metadata collected during the creation of pipelines can be reused to suggest similar content in related visualizations and guide semi automated changes we introduce the idea of query by example in the context of an ensemble of visualizations and the use of analogies as first class operations in system to guide scalable interactions we describe an implementation of these techniques in vistrails publicly available open source system
since the first results published in by liu and layland on the rate monotonic rm and earliest deadline first edf algorithms lot of progress has been made in the schedulability analysis of periodic task sets unfortunately many misconceptions still exist about the properties of these two scheduling methods which usually tend to favor rm more than edf typical wrong statements often heard in technical conferences and even in research papers claim that rm is easier to analyze than edf it introduces less runtime overhead it is more predictable in overload conditions and causes less jitter in task executionsince the above statements are either wrong or not precise it is time to clarify these issues in systematic fashion because the use of edf allows better exploitation of the available resources and significantly improves system's performancethis paper compares rm against edf under several aspects using existing theoretical results specific simulation experiments or simple counterexamples to show that many common beliefs are either false or only restricted to specific situations
we present methodology for data warehouse design and its application within the telecom italia information system the methodology is based on conceptual representation of the enterprise which is exploited both in the integration phase of the warehouse information sources and during the knowledge discovery activity on the information stored in the warehouse the application of the methodology in the telecom italia framework has been supported by prototype software tools both for conceptual modeling and for data integration and reconciliation
sprint is middleware infrastructure for high performance and high availability data management it extends the functionality of standalone in memory database imdb server to cluster of commodity shared nothing servers applications accessing an imdb are typically limited by the memory capacity of the machine running the imdb sprint partitions and replicates the database into segments and stores them in several data servers applications are then limited by the aggregated memory of the machines in the cluster transaction synchronization and commitment rely on total order multicast differently from previous approaches sprint does not require accurate failure detection to ensure strong consistency allowing fast reaction to failures experiments conducted on cluster with data servers using tpc and micro benchmark showed that sprint can provide very good performance and scalability
the effort in software process support has focused so far on modeling and enacting processes certain amount of work has been done but little has reached satisfactory level of maturity and acceptance in our opinion this is due to the difficulty for system to accommodate the very numerous aspects involved in software processes complete process support should cover topics ranging from low level tasks like compiling to organizational and strategic tasks this includes process enhancement resource management and control cooperative work etc the environment must also be convenient for software engineers team leaders managers and so on it must be able to describe details for efficient execution and be high level for capturing understanding etc as matter of fact the few tools that have reached sufficient maturity have focussed on single topic and addressed single class of usersit is our claim that no single system can provide satisfactory solution except in clearly defined subdomain thus we shifted our attention from finding the universal system to finding ways to make many different systems cooperate with their associated formalisms and process enginesthis paper presents novel approach for software process support environments based on federation of heterogeneous and autonomous components the approach has been implemented and experimented in the apel environment it is shown which architecture and technology is involved how it works which interoperability paradigms have been used which problems we have solved and which issues are still under study
as the internet grows in size it becomes crucial to understand how the speeds of links in the network must improve in order to sustain the pressure of new end nodes being added each day although the speeds of links in the core and at the edges improve roughly according to moore's law this improvement alone might not be enough indeed the structure of the internet graph and routing in the network might necessitate much faster improvements in the speeds of key links in the network in this paper using combination of analysis and extensive simulations we show that the worst congestion in the internet as level graph in fact scales poorly with the network size sup omega sup where is the number of nodes when shortest path routing is used to route traffic between ases we also show somewhat surprisingly that policy based routing does not exacerbate the maximum congestion when compared to shortest path routing our results show that it is crucial to identify ways to alleviate this congestion to avoid some links from being perpetually congested to this end we show that the congestion scaling properties of internet like graphs can be improved dramatically by introducing moderate amounts of redundancy in the graph in terms of parallel edges between pairs of adjacent nodes
challenge involved in applying density based clustering to categorical biomedical data is that the cube of attribute values has no ordering defined making the search for dense subspaces slow we propose the hierdenc algorithm for hierarchical density based clustering of categorical data and complementary index for searching for dense subspaces efficiently the hierdenc index is updated when new objects are introduced such that clustering does not need to be repeated on all objects the updating and cluster retrieval are efficient comparisons with several other clustering algorithms showed that on large datasets hierdenc achieved better runtime scalability on the number of objects as well as cluster quality by fast collapsing the bicliques in large networks we achieved an edge reduction of as much as hierdenc is suitable for large and quickly growing datasets since it is independent of object ordering does not require re clustering when new data emerges and requires no user specified input parameters
this paper surveys how the maximum adjacency ma ordering of the vertices in graph can be used to solve various graph problems we first explain that the minimum cut problem can be solved efficiently by utilizing the ma ordering the idea is then extended to fundamental operation of graph edge splitting based on this the edge connectivity augmentation problem for given and also for the entire range of can be solved efficiently by making use of the ma ordering where it is asked to add the smallest number of new edges to given graph so that its edge connectivity is increased to other related topics are also surveyed
this paper examines algorithmic aspects of searching for approximate functional dependencies in database relation the goal is to avoid exploration of large parts of the space of potential rules this is accomplished by leveraging found rules to make finding other rules more efficient the overall strategy is an attribute at time iteration which uses local breadth first searches on lattices that increase in width and height in each iteration the resulting algorithm provides many opportunities to apply heuristics to tune the search for particular data sets and or search objectives the search can be tuned at both the global iteration level and the local search level number of heuristics are developed and compared experimentally
current recommender systems attempt to identify appealing items for user by applying syntactic matching techniques which suffer from significant limitations that reduce the quality of the offered suggestions to overcome this drawback we have developed domain independent personalization strategy that borrows reasoning techniques from the semantic web elaborating recommendations based on the semantic relationships inferred between the user's preferences and the available items our reasoning based approach improves the quality of the suggestions offered by the current personalization approaches and greatly reduces their most severe limitations to validate these claims we have carried out case study in the digital tv field in which our strategy selects tv programs interesting for the viewers from among the myriad of contents available in the digital streams our experimental evaluation compares the traditional approaches with our proposal in terms of both the number of tv programs suggested and the users perception of the recommendations finally we discuss concerns related to computational feasibility and scalability of our approach
despite the effectiveness of search engines the persistently increasing amount of web data continuously obscures the search task efforts have thus concentrated on personalized search that takes account of user preferences new concept is introduced towards this direction search based on ranking of local set of categories that comprise user search profile new algorithms are presented that utilize web page categories to personalize search results series of user based experiments show that the proposed solutions are efficient finally we extend the application of our techniques in the design of topic focused crawlers which can be considered an alternative personalized search
we discuss some basic issues of interactive computations in the framework of rough granular computing among these issues are hierarchical modeling of granule structures and interactions between granules of different complexity interactions between granules on which computations are performed are among the fundamental concepts of wisdom technology wistech wistech is encompassing such areas as interactive computations multiagent systems cognitive computation natural computing complex adaptive and autonomous systems or knowledge representation and reasoning about knowledge
the tremendous growth of system memories has increased the capacities and capabilities of memory resident embedded databases yet current embedded databases need to be tuned in order to take advantage of new memory technologies in this paper we study the implications of hosting memory resident databases and propose hardware and software query driven techniques to improve their performance and energy consumption we exploit the structured organization of memories which enables selective mode of operation in which banks are accessed selectively unused banks are placed in lower power mode based on access pattern information we propose hardware techniques that dynamically control the memory by making the system adapt to the access patterns that arise from queries we also propose software query directed scheme that directly modifies the queries to reduce the energy consumption by ensuring uniform bank accesses our results show that these optimizations could lead to at the least reduction in memory energy we also show that query directed schemes better utilize the low power modes achieving up to improvement
modern database systems provide not only powerful data models but also complex query languages supporting powerful features such as the ability to create new database objects and invocation of arbitrary methods possibly written in third party programming language in this sense query languages have evolved into powerful programming languages surprisingly little work exists utilizing techniques from programming language research to specify and analyse these query languages this paper provides formal high level operational semantics for complex value oql like query language that can create fresh database objects and invoke external methods we define type system for our query language and prove an important soundness propertywe define simple effect typing discipline to delimit the computational effects within our queries we prove that this effect system is correct and show how it can be used to detect cases of non determinism and to define correct query optimizations
we present practical technique for pointing and selection using combination of eye gaze and keyboard triggers eyepoint uses two step progressive refinement process fluidly stitched together in look press look release action which makes it possible to compensate for the accuracy limitations of the current state of the art eye gaze trackers while research in gaze based pointing has traditionally focused on disabled users eyepoint makes gaze based pointing effective and simple enough for even able bodied users to use for their everyday computing tasks as the cost of eye gaze tracking devices decreases it will become possible for such gaze based techniques to be used as viable alternative for users who choose not to use mouse depending on their abilities tasks and preferences
there has been tremendous growth in the amount and range of information available on the internet the users requests for online information can be captured by long tail model few popular websites enjoy high number of visitations while the majority of the rest are less frequently requested in this study we use real world data to investigate this phenomenon and show that both users physical location and time of access affect the heterogeneity of website requests the effect can partially be explained by differences in demographic characteristics at locations and diverse user browsing behavior in weekdays and weekends these results can be used to design better online marketing strategies affiliate advertising models and internet caching algorithms with sensitivities to user location and time of access differences
leakage energy reduction for caches has been the target of many recent research efforts in this work we propose novel compiler directed approach to reduce the data cache leakage energy by exploiting the program behavior the proposed approach is based on the observation that only small portion of the data are active at runtime and the program spends lot of time in loops so large portion of data cache lines which are not accessed by the loop can be placed into the leakage control mode to reduce leakage energy consumption the compiler directed approach does not require hardware counters to monitor the access patterns of the cache lines and it is adaptive to the program behavior the experimental results show that the compiler directed approach is very competitive in terms of energy consumption and energy delay product compared to the recently proposed pure hardware based approach we also show that the utilization of loop transformations can increase the effectiveness of our strategy
with an increasing use of data mining tools and techniques we envision that knowledge discovery and data mining system kddms will have to support and optimize for the following scenarios sequence of queries user may analyze one or more datasets by issuing sequence of related complex mining queries and multiple simultaneous queries several users may be analyzing set of datasets concurrently and may issue related complex queriesthis paper presents systematic mechanism to optimize for the above cases targeting the class of mining queries involving frequent pattern mining on one or multiple datasets we present system architecture and propose new algorithms to simultaneously optimize multiple such queries and use knowledgeable cache to store and utilize the past query results we have implemented and evaluated our system with both real and synthetic datasets our experimental results show that our techniques can achieve speedup of up to factor of compared with the systems which do not support caching or optimize for multiple queries
collaborative brainstorming can be challenging but important part of creative group problem solving mind mapping has the potential to enhance the brainstorming process but has its own challenges when used in group we introduce groupmind collaborative mind mapping tool that addresses these challenges and opens new opportunities for creative teamwork including brainstorming we present semi controlled evaluation of groupmind and its impact on teamwork problem solving and collaboration for brainstorming activities groupmind performs better than using traditional whiteboard in both interaction group and nominal group settings for the task involving memory recall the hierarchical mind map structure also imposes important framing effects on group dynamics and idea organization during the brainstorming process we also present design ideas to assist in the development of future tools to support creative problem solving in groups
new paradigm for mobile service chain's competitive and collaborative mechanism is proposed in this study the main idea of the proposed approach is based on multi agent system with optimal profit of the pull push and collaborative models among the portal access service provider pasp the product service provider psp and the mobile service provider msp to address the running mechanism for the multi agent system an integrated system framework is proposed based on the agent evolution algorithm aea which could resolve all these modes to examine the feasibility of the framework prototype system based on java repast is implemented the simulation experiments show that this system can help decision makers take the appropriate strategies with higher profits by analyzing the expectations and variances or risks of each player's profit the interaction between and among entities in the chain is well understood it is found that in the situation where collaborative mechanism is applied the performance of players is better as compared to the other two situations where competitive mechanism is implemented if some constraints are applied the risk will be kept at low level
the common abstraction of xml schema by unranked regular tree languages is not entirely accurate to shed some light on the actual expressive power of xml schema intuitive semantical characterizations of the element declarations consistent edc rule are provided in particular it is obtained that schemas satisfying edc can only reason about regular properties of ancestors of nodes hence with respect to expressive power xml schema is closer to dtds than to tree automata these theoretical results are complemented with an investigation of the xml schema definitions xsds occurring in practice revealing that the extra expressiveness of xsds over dtds is only used to very limited extent as this might be due to the complexity of the xml schema specification and the difficulty of understanding the effect of constraints on typing and validation of schemas simpler formalism equivalent to xsds is proposed it is based on contextual patterns rather than on recursive types and it might serve as light weight front end for xml schema next the effect of edc on the way xml documents can be typed is discussed it is argued that cleaner more robust larger but equally feasible class is obtained by replacing edc with the notion of pass preorder typing ppt schemas that allow one to determine the type of an element of streaming document when its opening tag is met this notion can be defined in terms of grammars with restrained competition regular expressions and there is again an equivalent syntactical formalism based on contextual patterns finally algorithms for recognition simplification and inclusion of schemas for the various classes are given
we propose method to handle approximate searching by image content in medical image databases image content is represented by attributed relational graphs holding features of objects and relationships between objects the method relies on the assumption that fixed number of labeled or expected objects eg heart lungs etc are common in all images of given application domain in addition to variable number of unexpected or unlabeled objects eg tumor hematoma etc the method can answer queries by example such as find all rays that are similar to smith's ray the stored images are mapped to points in multidimensional space and are indexed using state of the art database methods trees the proposed method has several desirable propertiesdatabase search is approximate so that all images up to prespecified degree of similarity tolerance are retrievedit has no false dismissals ie all images qualifying query selection criteria are retrieved it is much faster than sequential scanning for searching in the main memory and on the disk ie by up to an order of magnitude thus scaling up well for large databases
this paper proposes novel method using constant inter frame motion for self calibration from an image sequence of an object rotating around single axis with varying camera internal parameters our approach makes use of the facts that in many commercial systems rotation angles are often controlled by an electromechanical system and that the inter frame essential matrices are invariant if the rotation angles are constant but not necessary known therefore recovering camera internal parameters is possible by making use of the equivalence of essential matrices which relate the unknown calibration matrices to the fundamental matrices computed from the point correspondences we also describe linear method that works under restrictive conditions on camera internal parameters the solution of which can be used as the starting point of the iterative non linear method with looser constraints the results are refined by enforcing the global constraints that the projected trajectory of any point should be conic after compensating for the focusing and zooming effects finally using the bundle adjustment method tailored to the special case ie static camera and constant object rotation the structure of the object is recovered and the camera parameters are further refined simultaneously to determine the accuracy and the robustness of the proposed algorithm we present the results on both synthetic and real sequences
discriminative reranking is one method for constructing high performance statistical parsers collins discriminative reranker requires source of candidate parses for each sentence this paper describes simple yet novel method for constructing sets of best parses based on coarse to fine generative parser charniak this method generates best lists that are of substantially higher quality than previously obtainable we used these parses as the input to maxent reranker johnson et al riezler et al that selects the best parse from the set of parses for each sentence obtaining an score of on sentences of length or less
we have proposed the extent system for automated photograph annotation using image content and context analysis key component of extent is landmark recognition system called landmarker in this paper we present the architecture of landmarker the content of query photograph is analyzed and compared against database of sample landmark images to recognize any landmarks it contains an algorithm is presented for comparing query image with sample image context information may be used to assist landmark recognition also we show how landmarker deals with scalability to allow recognition of large number of landmarks we have implemented prototype of the system and present empirical results on large dataset
this article describes our research on spoken language translation aimed toward the application of computer aids for second language acquisition the translation framework is incorporated into multilingual dialogue system in which student is able to engage in natural spoken interaction with the system in the foreign language while speaking query in their native tongue at any time to obtain spoken translation for language assistance thus the quality of the translation must be extremely high but the domain is restricted experiments were conducted in the weather information domain with the scenario of native english speaker learning mandarin chinese we were able to utilize large corpus of english weather domain queries to explore and compare variety of translation strategies formal example based and statistical translation quality was manually evaluated on test set of spontaneous utterances the best speech translation performance percnt correct percnt incorrect and percnt rejected is achieved by system which combines the formal and example based methods using parsability by domain specific chinese grammar as rejection criterion
preventive measures sometimes fail to defect malicious attacks with cyber attacks on data intensive applications becoming an ever more serious threat intrusion tolerant database systems are significant concern the main objective of intrusion tolerant database systems is to detect attacks and to assess and repair the damage caused by the attacks in timely manner such that the database will not be damaged to such degree that is unacceptable or useless this paper focuses on efficient damage assessment and repair in resilient distributed database systems the complexity of distributed database systems caused by data partition distributed transaction processing and failures makes damage assessment and repair much more challenging than in centralized database systems this paper identifies the key challenges and presents an efficient algorithm for distributed damage assessment and repair
this paper presents new algorithm that detects set of dominant points on the boundary of an eight connected shape to obtain polygonal approximation of the shape itself the set of dominant points is obtained from the original break points of the initial boundary where the integral square is zero for this goal most of the original break points are deleted by suppressing those whose perpendicular distance to an approximating straight line is lower than variable threshold value the proposed algorithm iteratively deletes redundant break points until the required approximation which relies on decrease in the length of the contour and the highest error is achieved comparative experiment with another commonly used algorithm showed that the proposed method produced efficient and effective polygonal approximations for digital planar curves with features of several sizes
multisource data flow problems involve information which may enter nodes independently through different classes of edges in some cases dissimilar meet operations appear to be used for different types of nodes these problems include bidirectional and flow sensitive problems as well as many static analyses of concurrent programs with synchronization tuple frameworks type of standard data flow framework provide natural encoding for multisource problems using single meet operator previously the solution of these problems has been described as the fixed point of set of data flow equations using our tuple representation we can access the general results of standard data flow frameworks concerning convergence time and solution precision for these problems we demonstrate this for the bidirectional component of partial redundancy suppression and two problems on the program summary graph an interesting subclass of tuple frameworks the join of meets frameworks is useful for reachability problems especially those stemming from analyses of explicitly parallel programs we give results on function space properties for join of meets frameworks that indicate precise solutions for most of them will be difficult to obtain
in this paper we analyze the node spatial distribution of mobile wireless ad hoc networks characterizing this distribution is of fundamental importance in the analysis of many relevant properties of mobile ad hoc networks such as connectivity average route length and network capacity in particular we have investigated under what conditions the node spatial distribution resulting after large number of mobility steps resembles the uniform distribution this is motivated by the fact that the existing theoretical results concerning mobile ad hoc networks are based on this assumption in order to test this hypothesis we performed extensive simulations using two well known mobility models the random waypoint model which resembles intentional movement and brownian like model which resembles nonintentional movement our analysis has shown that in brownian like motion the uniformity assumption does hold and that the intensity of the concentration of nodes in the center of the deployment region that occurs in the random waypoint model heavily depends on the choice of some mobility parameters for extreme values of these parameters the uniformity assumption is impaired
we present tool that predicts whether the software under development inside an ide has bug an ide plugin performs this prediction using the change classification technique to classify source code changes as buggy or clean during the editing session change classification uses support vector machines svm machine learning classifier algorithm to classify changes to projects mined from their configuration management repository this technique besides being language independent and relatively accurate can classify change immediately upon its completion and use features extracted solely from the change delta added deleted and the source code to predict buggy changes thus integrating change classification within an ide can predict potential bugs in the software as the developer edits the source code ideally reducing the amount of time spent on fixing bugs later to this end we have developed change classification plugin for eclipse based on client server architecture described in this paper
weak pseudorandom function wprf is cryptographic primitive similar to but weaker than pseudorandom function for wprfs one only requires that the output is pseudorandom when queried on random inputs we show that unlike normal prfs wprfs are seed incompressible in the sense that the output of wprf is pseudorandom even if bounded amount of information about the key is leakedas an application of this result we construct simple mode of operation which when instantiated with any wprf gives leakage resilient stream cipher the implementation of such cipher is secure against every side channel attack as long as the amount of information leaked per round is bounded but overall can be arbitrary large the construction is simpler than the previous one dziembowski pietrzak focs as it only uses single primitive wprf in straight forward manner
we describe framework for finding and tracking trails for autonomous outdoor robot navigation through combination of visual cues and ladar derived structural information the algorithm is able to follow paths which pass through multiple zones of terrain smoothness border vegetation tread material and illumination conditions our shape based visual trail tracker assumes that the approaching trail region is approximately triangular under perspective it generates region hypotheses from learned distribution of expected trail width and curvature variation and scores them using robust measure of color and brightness contrast with flanking regions the structural component analogously rewards hypotheses which correspond to empty or low density regions in groundstrike filtered ladar obstacle map our system's performance is analyzed on several long sequences with diverse appearance and structural characteristics ground truth segmentations are used to quantify performance where available and several alternative algorithms are compared on the same data
after reviewing number of internet tools and technologies originating in the field of logic programming and discussing promissing directions of ongoing research we describe logic programming based networking infrastructure which combines reasoning and knowledge processing with flexible coordination of dynamic state changes and computation mobility as well as and its use for the design of intelligent mobile agent programs lightweight logic programming language jinni implemented in java is introduced as flexible scripting tool for gluing together knowledge processing components and java objects in networked client server applications and thin client environments as well as through applets over the web mobile threads implemented by capturing first order continuations in compact data structure sent over the network allow jinni to interoperate with remote high performance binprolog servers for cpu intensive knowledge processing controlled natural language to prolog translator with support of third party speech recognition and text to speech translation allows interaction with users not familiar with logic programming
science projects of various disciplines face fundamental challenge thousands of users want to obtain new scientific results by application specific and dynamic correlation of data from globally distributed sources considering the involved enormous and exponentially growing data volumes centralized data management reaches its limits since scientific data are often highly skewed and exploration tasks exhibit large degree of spatial locality we propose the locality aware allocation of data objects onto distributed network of interoperating databases hisbase is an approach to data management in scientific federated data grids that addresses the scalability issue by combining established techniques of database research in the field of spatial data structures quadtrees histograms and parallel databases with the scalable resource sharing and load balancing capabilities of decentralized peer to peer pp networks the proposed combination constitutes complementary science infrastructure enabling load balancing and increased query throughput
the web and especially major web search engines are essential tools in the quest to locate online information for many people this paper reports results from research that examines characteristics and changes in web searching from nine studies of five web search engines based in the us and europe we compare interactions occurring between users and web search engines from the perspectives of session length query length query complexity and content viewed among the web search engines the results of our research shows users are viewing fewer result pages searchers on us based web search engines use more query operators than searchers on european based search engines there are statistically significant differences in the use of boolean operators and result pages viewed and one cannot necessary apply results from studies of one particular web search engine to another web search engine the wide spread use of web search engines employment of simple queries and decreased viewing of result pages may have resulted from algorithmic enhancements by web search engine companies we discuss the implications of the findings for the development of web search engines and design of online content
in this paper we formulate two classes of problems the colored range query problems and the colored point enclosure query problems to model multi dimensional range and point enclosure queries in the presence of categorical information many of these problems are difficult to solve using traditional data structural techniques based on new framework of combining sketching techniques and traditional data structures we obtain two sets of results in solving the problems approximately and efficiently in addition the framework can be employed to attack other related problems by finding the appropriate summary structures
in this paper we show how pattern matching can be seen to arise from proof term assignment for the focused sequent calculus this use of the curry howard correspondence allows us to give novel coverage checking algorithm and makes it possible to give rigorous correctness proof for the classical pattern compilation strategy of building decision trees via matrices of patterns
in this paper we propose complete model handling the physical simulation of deformable objects we formulate continuous expressions for stretching bending and twisting energies these expressions are mechanically rigorous and geometrically exact both elastic and plastic deformations are handled to simulate wide range of materials we validate the proposed model in several classical test configurations the use of geometrical exact energies with dynamic splines provides very accurate results as well as interactive simulation times which shows the suitability of the proposed model for constrained cad applications we illustrate the application potential of the proposed model by describing virtual system for cable positioning which can be used to test compatibility between planned fixing clip positions and mechanical cable properties
emerging applications in the area of emergency response and disaster management are increasingly demanding interactive capabilities to allow for the quick understanding of critical situation in particular in urban environments key component of these interactive simulations is how to recreate the behavior of crowd in real time while supporting individual behaviors crowds can often be unpredictable and present mixed behaviors such as panic or aggression that can very rapidly change based on unexpected new elements introduced into the environment we present preliminary research specifically oriented towards the simulation of large crowds for emergency response and rescue planning situations our approach uses highly scalable architecture integrated with an efficient rendering architecture and an immersive visualization environment for interaction in this environment users can specify complex scenarios plug in crowd behavior algorithms and interactively steer the simulation to analyze and evaluate multiple what if situations
we introduce an algorithm for space variant filtering of video based on spatio temporal laplacian pyramid and use this algorithm to render videos in order to visualize prerecorded eye movements spatio temporal contrast and colour saturation are reduced as function of distance to the nearest gaze point of regard ie non fixated distracting regions are filtered out whereas fixated image regions remain unchanged results of an experiment in which the eye movements of an expert on instructional videos are visualized with this algorithm so that the gaze of novices is guided to relevant image locations show that this visualization technique facilitates the novices perceptual learning
this paper presents an investigation into dynamic self adjustment of task deployment and other aspects of self management through the embedding of multiple policiesnon dedicated loosely coupled computing environments such as clusters and grids are increasingly popular platforms for parallel processing these abundant systems are highly dynamic environments in which many sources of variability affect the run time efficiency of tasks the dynamism is exacerbated by the incorporation of mobile devices and wireless communicationthis paper proposes an adaptive strategy for the flexible run time deployment of tasks to continuously maintain efficiency despite the environmental variability the strategy centres on policy based scheduling which is informed by contextual and environmental inputs such as variance in the round trip communication time between client and its workers and the effective processing performance of each workera self management framework has been implemented for evaluation purposes the framework integrates several policy controlled adaptive services with the application code enabling the run time behaviour to be adapted to contextual and environmental conditions using this framework an exemplar self managing parallel application is implemented and used to investigate the extent of the benefits of the strategy
in the area of health care and sports in recent years variety of mobile applications have been established mobile devices are of emerging interest due to their high availability and increasing computing power in many different health scenarios in this paper we present scalable secure sensor monitoring platform ssmp which collects vital data of users vital parameters can be collected by just one single sensor or in multi sensor configuration nowadays wide spectrum of sensors is available which provide wireless connectivity eg bluetooth vital data can then easily be transmitted to mobile device which subsequently transmits these data to an ehealth portal there are already solutions implementing these capabilities however privacy aspects of users are very often neglected since health data may enable people to draw potentially compromising conclusions eg insurance companies it is absolutely necessary to design an enhanced security concept in this context to complicate matters further the trustworthiness of providers which are operating with user's health data can not be determined by users priori this means that the security concept implemented by the provider may bear security flaws additionally there is no guarantee that the provider preserves the users privacy claims in this work we propose security concept incorporating privacy aspects using mobile devices for transferring and storing health data at portal in addition the concept guarantees anonymity in the transfer process as well as for stored data at service provider hence insider attacks based on stored data can be prevented
next generation system designs are challenged by multiple walls among them the inter related impediments offered by power dissipation limits and reliability are particularly difficult ones that all current chip system design teams are grappling with in this paper we first describe the attendant challenges in integrated multi dimensional pre silicon modeling and the solution approaches being pursued later we focus on leading edge solutions for power thermal and failure rate mitigation that have been proposed in our work over the past decade
the main goal of this paper is to provide routing table free online algorithms for wireless sensor networks wsns to select cost eg node residual energies and delay efficient paths as basic information to drive the routing process both node costs and hop count distances are considered particular emphasis is given to greedy routing schemes due to their suitability for resource constrained and highly dynamic networks for what concerns greedy forwarding we present the statistically assisted routing algorithm sara where forwarding decisions are driven by statistical information on the costs of the nodes within coverage and in the second order neighborhood by analysis we prove that an optimal online policy exists we derive its form and we exploit it as the core of sara besides greedy techniques sub optimal algorithms where node costs can be partially propagated through the network are also presented these techniques are based on real time learning lrta algorithms which through an initial exploratory phase converge to quasi globally optimal paths all the proposed schemes are then compared by simulation against globally optimal solutions discussing the involved trade offs and possible performance gains the results show that the exploitation of second order cost information in sara substantially increases the goodness of the selected paths with respect to fully localized greedy routing finally the path quality can be further increased by lrta schemes whose convergence can be considerably enhanced by properly setting real time search parameters however these solutions fail in highly dynamic scenarios as they are unable to adapt the search process to time varying costs
peer to peer pp networks are beginning to form the infrastructure of future applications computers are organized in pp overlay networks to facilitate search queries with reasonable cost so scalability is major aim in design of pp networks in this paper to obtain high factor of scalability we partition network search space using consistent static shared upper ontology we name our approach semantic partition tree spt all resources and queries are annotated using the upper ontology and queries are semantically routed in the overlay network also each node indexes addresses of other nodes that possess contents expressible by the concept it maintains so our approach can be conceived as an ontology based distributed hash table dht also we introduce lookup service for the network which is very scalable and independent of the network size and just depends on depth of the ontology tree further we introduce broadcast algorithm on the network we present worst case analysis of both lookup and broadcast algorithms and measure their performance using simulation the results show that our scheme is highly scalable and can be used in real pp applications
stacked wafer integration has the potential to improve multiprocessor system on chip mpsoc integration density performance and power efficiency however the power density of mpsocs increases with the number of active layers resulting in high chip temperatures this can reduce system reliability reduce performance and increase cooling cost thermal optimization for mpsocs imposes numerous challenges it is difficult to manage assignment and scheduling of heterogeneous workloads to maintain thermal safety in addition the thermal characteristics of mpsocs differ from those of mpsocs because each stacked layer has different thermal resistance to the ambient and vertically adjacent processors have strong temperature correlation we propose mpsoc thermal optimization algorithm that conducts task assignment scheduling and voltage scaling power balancing algorithm is initially used to distribute tasks among cores and active layers detailed thermal analysis is used to guide hotspot mitigation algorithm that incrementally reduces the peak mpsoc temperature by appropriately adjusting task execution times and voltage levels the proposed algorithm considers leakage power consumption and adapts to inter layer thermal heterogeneity performance evaluation on set of multiprogrammed and multithreaded benchmarks indicates that the proposed techniques can optimize dmpsoc power consumption power profile and chip peak temperature
the test scheduling problem is one of the major issues in the test integration of system on chip soc and test schedule is usually influenced by the test access mechanism tam in this paper we propose graph based approach to power constrained test scheduling with tam assignment and test conflicts also considered by mapping test schedule to subgraph of the test compatibility graph an interval graph recognition method can be used to determine the order of the core tests we then present heuristic algorithm that can effectively assign tam wires to the cores given the test order with the help of the tabu search method and the test compatibility graph the proposed algorithm allows rapid exploration of the solution space experimental results for the itc benchmarks show that short test length is achieved within reasonable computation time
commercial workloads form an important class of applications and have performance characteristics that are distinct from scientific and technical benchmarks such as spec cpu however due to the prohibitive simulation time of commercial workloads it is extremely difficult to use them in computer architecture research in this paper we study the efficacy of using statistical sampling based simulation methodology for two classes of commercial workloads java server benchmark specjbb and an online transaction processing oltp benchmark dbt our results show that although specjbb shows distinct garbage collection phases there are no large scale phases in the oltp benchmark we take advantage of this stationary behavior in steady phase and propose statistical sampling based simulation technique dynasim with two dynamic stopping rules in this approach the simulation terminates once the target accuracy has been met we apply dynasim to simulate commercial workloads and show that with the simulation of only few million total instructions the error can be within at confidence level of dynasim compares favorably with random sampling and representative sampling in terms of the total number of instructions simulated time cost and with representative sampling in terms of the number of checkpoints storage cost dynasim increases the usability of sampling based simulation approach for commercial workloads and will encourage the use of commercial workloads in computer architecture research
we describe new data format for storing triangular symmetric and hermitian matrices called rectangular full packed format rfpf the standard two dimensional arrays of fortran and also known as full format that are used to represent triangular and symmetric matrices waste nearly half of the storage space but provide high performance via the use of level blas standard packed format arrays fully utilize storage array space but provide low performance as there is no level packed blas we combine the good features of packed and full storage using rfpf to obtain high performance via using level blas as rfpf is standard full format representation also rfpf requires exactly the same minimal storage as packed the format each lapack full and or packed triangular symmetric and hermitian routine becomes single new rfpf routine based on eight possible data layouts of rfpf this new rfpf routine usually consists of two calls to the corresponding lapack full format routine and two calls to level blas routines this means no new software is required as examples we present lapack routines for cholesky factorization cholesky solution and cholesky inverse computation in rfpf to illustrate this new work and to describe its performance on several commonly used computer platforms performance of lapack full routines using rfpf versus lapack full routines using the standard format for both serial and smp parallel processing is about the same while using half the storage performance gains are roughly one to factor of for serial and one to factor of for smp parallel times faster using vendor lapack full routines with rfpf than with using vendor and or reference packed routines
application resource usage models can be used in the decision making process for ensuring quality of service as well as for capacity planning apart from their general use in performance modeling optimization and systems management current solutions for modeling application resource usage tend to address parts of the problem by either focusing on specific application or specific platform or on small subset of system resources we propose simple and flexible approach for modeling application resource usage in platform independent manner that enables the prediction of application resource usage on unseen platforms the technique proposed is application agnostic requiring no modification to the application binary or source and no knowledge of application semantics we implement linux based prototype and evaluate it using four different workloads including real world applications and benchmarks our experiments reveal prediction errors that are bound within of the observed for these workloads when using the proposed approach
similarity search is core module of many data analysis tasks including search by example classification and clustering for time series data dynamic time warping dtw has been proven very effective similarity measure since it minimizes the effects of shifting and distortion in time however the quadratic cost of dtw computation to the length of the matched sequences makes its direct application on databases of long time series very expensive we propose technique that decomposes the sequences into number of segments and uses cheap approximations thereof to compute fast lower bounds for their warping distances we present several progressively tighter bounds relying on the existence or not of warping constraints finally we develop an index and multi step technique that uses the proposed bounds and performs two levels of filtering to efficiently process similarity queries thorough experimental study suggests that our method consistently outperforms state of the art methods for dtw similarity search
various known models of probabilistic xml can be represented as instantiations of the abstract notion of documents in addition to ordinary nodes documents have distributional nodes that specify the possible worlds and their probabilistic distribution particular families of documents are determined by the types of distributional nodes that can be used as well as by the structural constraints on the placement of those nodes in document some of the resulting families provide natural extensions and combinations of previously studied probabilistic xml models the focus of the paper is on the expressive power of families of documents in particular two main issues are studied the first is the ability to efficiently translate given document of one family into another family the second is closure under updates namely the ability to efficiently represent the result of updating the instances of document of given family as another document of that family for both issues we distinguish two variants corresponding to value based and object based semantics of documents
we discuss the parallelization of algorithms for solving poly nomial systems symbolically by way of triangular decompositions we introduce component level parallelism for which the number of processors in use depends on the geometry of the solution set of the input system our long term goal is to achieve an efficient multi level parallelism coarse grained component level for tasks computing geometric objects in the solution sets and medium fine grained level for polynomial arithmetic such as gcd resultant computation within each task component level parallelization of triangular decompositions belongs to the class of dynamic irregular parallel applications which leads us to address the following question how to exploit geometrical information at an early stage of the solving process that would be favorable to parallelization we report on the effectiveness of the approaches that we have applied including modular methods solving by decreasing order of dimension task pool with dimension and rank guided scheduling we have extended the aldor programming language to support multiprocessed parallelism on smps and realized preliminary implementation our experimentation shows promising speedups for some well known problems and proves that our component level parallelization is practically efficient we expect that this speedup would add multiplicative factor to the speedup of medium fine grained level parallelization as parallel gcd and resultant computations
transactional coherence and consistency tcc offers way to simplify parallel programming by executing all code within transactions in tcc systems transactions serve as the fundamental unit of parallel work communication and coherence as each transaction completes it writes all of its newly produced state to shared memory atomically while restarting other processors that have speculatively read stale data with this mechanism tcc based system automatically handles data synchronization correctly without programmer intervention to gain the benefits of tcc programs must be decomposed into transactions we describe two basic programming language constructs for decomposing programs into transactions loop conversion syntax and general transaction forking mechanism with these constructs writing correct parallel programs requires only small incremental changes to correct sequential programs the performance of these programs may then easily be optimized based on feedback from real program execution using few simple techniques
modern stream applications such as sensor monitoring systems and publish subscription services necessitate the handling of large numbers of continuous queries specified over high volume data streams efficient sharing of computations among multiple continuous queries especially for the memory and cpu intensive window based operations is critical novel challenge in this scenario is to allow resource sharing among similar queries even if they employ windows of different lengths this paper first reviews the existing sharing methods in the literature and then illustrates the significant performance shortcomings of these methodsthis paper then presents novel paradigm for the sharing of window join queries namely we slice window states of join operator into fine grained window slices and form chain of sliced window joins by using an elaborate pipelining methodology the number of joins after state slicing is reduced from quadratic to linear this novel sharing paradigm enables us to push selections down into the chain and flexibly select subsequences of such sliced window joins for computation sharing among queries with different window sizes based on the state slice sharing paradigm two algorithms are proposed for the chain buildup one minimizes the memory consumption while the other minimizes the cpu usage the algorithms are proven to find the optimal chain with respect to memory or cpu usage for given query workload we have implemented the slice share paradigm within the data stream management system cape the experimental results show that our strategy provides the best performance over diverse range of workload settings among all alternate solutions in the literature
in ad hoc networks the performance is significantly degraded as the size of the network grows the network clustering by which the nodes are hierarchically organized on the basis of the proximity relieves this performance degradation finding the weakly connected dominating set wcds is promising approach for clustering the wireless ad hoc networks finding the minimum wcds in the unit disk graph is an np hard problem and host of approximation algorithms has been proposed in this article we first proposed centralized approximation algorithm called dla cc based on distributed learning automata dla for finding near optimal solution to the minimum wcds problem then we propose dla based clustering algorithm called dla dc for clustering the wireless ad hoc networks the proposed cluster formation algorithm is distributed implementation of dla cc in which the dominator nodes and their closed neighbors assume the role of the cluster heads and cluster members respectively in this article we compute the worst case running time and message complexity of the clustering algorithm for finding near optimal cluster head set we argue that by proper choice of the learning rate of the clustering algorithm trade off between the running time and message complexity of algorithm with the cluster head set size clustering optimality can be made the simulation results show the superiority of the proposed algorithms over the existing methods
traditional compilers compile and optimize files separately making worst case assumptions about the program context in which file is to be linked more aggressive compilation architectures perform cross file interprocedural or whole program analyses potentially producing much faster programs but substantially increasing the cost of compilation even more radical are systems that perform all compilation and optimization at run time such systems can optimize programs based on run time program and system properties as well as static whole program properties however run time compilers also called dynamic compilers or just in time compilers suffer under severe constraints on allowable compilation time since any time spent compiling steals from time spent running the program none of these compilation models dominates the others each has unique strengths and weaknesses not present in the other modelswe are developing new staged compilation model which strives to combine high run time code quality with low compilation overhead compilation is organized as series of stages with stages corresponding to for example separate compilation library linking program linking and run time execution any given optimization can be performed at any of these stages to reduce compilation time while maintaining high effectiveness an optimization should be performed at the earliest stage that provides the necessary program context information to carry out the optimization effectively moreover single optimization can itself be spread across multiple stages with earlier stages performing preplanning work that enables the final stage to complete the optimization quickly in this way we hope to produce highly optimized programs nearly as good as what could be done with purely run time compiler that had an unconstrained compilation time budget but at much more practical compile time costwe are building the whirlwind optimizing compiler as the concrete embodiment of this staged compilation model initially targeting object oriented languages key component of whirlwind is set of techniques for automatically constructing staged compilers from traditional unstaged compilers including aggressive applications of specialization and other partial evaluation technology
many cache management schemes designed for mobile environments are based on invalidation reports irs however ir based approach suffers from long query latency and it cannot efficiently utilize the broadcast bandwidth in this paper we propose techniques to address these problems first by replicating small fraction of the essential information related to cache invalidation the query latency can be reduced then we propose techniques to efficiently utilize the broadcast bandwidth based on counters associated with each data item novel techniques are designed to maintain the accuracy of the counter in case of server failures client failures and disconnections extensive simulations are provided and used to evaluate the proposed methodology compared to previous ir based algorithms the proposed solution can significantly reduce the query latency improve the bandwidth utilization and effectively deal with disconnections and failures
recently many natural language processing nlp applications have improved the quality of their output by using various machine learning techniques to mine information extraction ie patterns for capturing information from the input text currently to mine ie patterns one should know in advance the type of the information that should be captured by these patterns in this work we propose novel methodology for corpus analysis based on cross examination of several document collections representing different instances of the same domain we show that this methodology can be used for automatic domain template creation as the problem of automatic domain template creation is rather new there is no well defined procedure for the evaluation of the domain template quality thus we propose methodology for identifying what information should be present in the template using this information we evaluate the automatically created domain templates through the text snippets retrieved according to the created templates
model transformations provide powerful capability to automate model refinements however the use of model transformation languages may present challenges to those who are unfamiliar with specific transformation language this paper presents an approach called model transformation by demonstration mtbd which allows an end user to demonstrate the exact transformation desired by actually editing source model and demonstrating the changes that evolve to target model an inference engine built into the underlying modeling tool records all editing operations and infers transformation pattern which can be reused in other models the paper motivates the need for the approach and discusses the technical contributions of mtbd case study with several sample inferred transformations serves as concrete example of the benefits of mtbd
spectral clustering refers to flexible class of clustering procedures that can produce high quality clusterings on small data sets but which has limited applicability to large scale problems due to its computational complexity of in general with the number of data points we extend the range of spectral clustering by developing general framework for fast approximate spectral clustering in which distortion minimizing local transformation is first applied to the data this framework is based on theoretical analysis that provides statistical characterization of the effect of local distortion on the mis clustering rate we develop two concrete instances of our general framework one based on local means clustering kasp and one based on random projection trees rasp extensive experiments show that these algorithms can achieve significant speedups with little degradation in clustering accuracy specifically our algorithms outperform means by large margin in terms of accuracy and run several times faster than approximate spectral clustering based on the nystrom method with comparable accuracy and significantly smaller memory footprint remarkably our algorithms make it possible for single machine to spectral cluster data sets with million observations within several minutes
many software systems suffer from missing support for behavioral runtime composition and configuration of software components the concern behavioral composition and configuration is not treated as first class entity but instead it is hard coded in different programming styles leading to tangled composition and configuration code that is hard to understand and maintain we propose to embed dynamic language with tailorable object and class concept into the host language in which the components are written and use the tailorable language for behavioral composition and configuration tasks using this approach we can separate the concerns behavioral composition and configuration from the rest of the software system leading to more reusable understandable and maintainable composition and configuration of software components
deep packet inspection dpi has been widely adopted in detecting network threats such as intrusion viruses and spam it is challenging however to achieve high speed dpi due to the expanding rule sets and ever increasing line rates key issue is that the size of the finite automata falls beyond the capacity of on chip memory thus incurring expensive off chip accesses in this paper we present dpico hardware based dpi engine that utilizes novel techniques to minimize the storage requirements for finite automata the techniques proposed are modified content addressable memory mcam interleaved memory banks and data packing the experiment results show the scalable performance of dpico can achieve up to gbps throughput using contemporary fpga chip experiment data also show that dpico based accelerator can improve the pattern matching performance of dpi server by up to times
an accurate tractable analytic cache model for time shared systems is presented which estimates the overall cache miss rate of multiprocessing system with any cache size and time quanta the input to the model consists of the isolated miss rate curves for each process the time quanta for each of the executing processes and the total cache size the output is the overall miss rate trace driven simulations demonstrate that the estimated miss rate is very accurate since the model provides fast and accurate way to estimate the effect of context switching it is useful for both understanding the effect of context switching on caches and optimizing cache performance for time shared systems cache partitioning mechanism is also presented and is shown to improve the cache miss rate up to over the normal lru replacement policy
in this paper we investigate technique for fusing approximate knowledge obtained from distributed heterogeneous information sources this issue is substantial eg in modeling multiagent systems where group of loosely coupled heterogeneous agents cooperate in achieving common goal information exchange leading ultimately to knowledge fusion is natural and vital ingredient of this process we use generalization of rough sets and relations which depends on allowing arbitrary similarity relations the starting point of this research is where framework for knowledge fusion in multiagent systems is introduced agents individual perceptual capabilities are represented by similarity relations further aggregated to express joint capabilities of teams this aggregation expressing shift from individual to social level of agents activity has been formalized by means of dynamic logic the approach of doherty et al uses the full propositional dynamic logic which does not guarantee tractability of reasoning our idea is to adapt the techniques of nguyen to provide an engine for tractable approximate database querying restricted to horn fragment of serial dynamic logic we also show that the obtained formalism is quite powerful in applications
the problem of modeling memory locality of applications to guide compiler optimizations in systematic manner is an important unsolved problem made even more significant with the advent of multi core and many core architectures we describe an approach based on novel source level metric called static reuse distance to model the memory behavior of applications written in matlab we use matlab as representative language that lets end users express their algorithms precisely but at relatively high level matlab's high level characteristics allow the static analysis to focus on large objects such as arrays without losing accuracy due to processor specific layout of scalar values in memory we present an efficient algorithm to compute static reuse distances using an extended version of dependence graphs our approach differs from earlier similar attempts in three important aspects it targets high level programming systems characterized by heavy use of libraries it works on full programs instead of being confined to loops and it integrates practical mechanisms to handle separately compiled procedures as well as pre compiled library procedures that are only available in binary form we study matlab code taken from real programs to demonstrate the effectiveness of our model finally we present some applications of our approach to program transformations that are known to be important in matlab but are expected to be relevant to other similar high level languages as well
the problem of frequent item discovery in streaming data has attracted lot of attention lately while the above problem has been studied extensively and several techniques have been proposed for its solution these approaches treat all the values of the data stream equally nevertheless not all values are of equal importance in several situations we are interested more in the new values that have appeared in the stream rather than in the older onesin this paper we address the problem of finding recent frequent items in data stream given small bounded memory and present novel algorithms to this direction we propose basic algorithm that extends the functionality of existing approaches by monitoring item frequencies in recent windows subsequently we present an improved version of the algorithm with significantly improved performance in terms of accuracy at no extra memory cost finally we perform an extensive experimental evaluation and show that the proposed algorithms can efficiently identify the frequent items in ad hoc recent windows of data stream
the past two decades have presented significant technological developments of mobile information and communication technology ict such as portable technologies eg mobile phones notebook computers personal digital assistants and associated wireless infrastructures eg wireless local area networks mobile telecommunications infrastructures bluetooth personal area networks mobile ict offers range of technical opportunities for organisations and their members to implement enterprise mobility however the challenges of unlocking the opportunities of enterprise mobility are not well understood one of the key issues is to establish systems and associated working practices that are deemed usable by both individuals and the organisation the aim of this paper is to show that the concept of organisational usability can enrich the understanding of mobile ict in organisations as an addition to the traditional understanding of individual usability organisational usability emphasises the role of mobile ict beyond individual support large scale study of four different ways of organising foreign exchange trading in middle eastern bank serves as the concrete foundation for the discussion the empirical study showed how the final of the four attempts at establishing trading deployed mobile ict to enable mobile trading and by providing solution which was deemed usable for both the organisation and the traders the paper contributes to the understanding of how usability of mobile ict critically depends on carefully balancing individual and organisational requirements it also demonstrates the need for research in enterprise mobility to embrace both individual and organisational concerns in order to grasp the complexity of the phenomena
high performance clusters have been growing rapidly in scale most of these clusters deploy high speed interconnect such as infini band to achieve higher performance most scientific applications executing on these clusters use the message passing interface mpi as the parallel programming model thus the mpi library has key role in achieving application performance by consuming as few resources as possible and enabling scalable performance state of the art mpi implementations over infiniband primarily use the reliable connection rc transport due to its good performance and attractive features however the rc transport requires connection between every pair of communicating processes with each requiring several kb of memory as clusters continue to scale memory requirements in rc based implementations increase the connection less unreliable datagram ud transport is an attractive alternative which eliminates the need to dedicate memory for each pair of processes in this paper we present high performance ud based mpi design we implement our design and compare the performance and resource usage with the rc based mvapich we evaluate npb smg sweepd and sppm up to processes on an core infiniband cluster for smg our prototype shows speedup and seven fold reduction in memory for processes additionally based on our model our design has an estimated times reduction in memory over mvapich at processes when all connections are created to the best of our knowledge this is the first research work that presents high performance mpi design over infiniband that is completely based on ud and can achieve near identical or better application performance than rc
outlier detection has been used for centuries to detect and where appropriate remove anomalous observations from data outliers arise due to mechanical faults changes in system behaviour fraudulent behaviour human error instrument error or simply through natural deviations in populations their detection can identify system faults and fraud before they escalate with potentially catastrophic consequences it can identify errors and remove their contaminating effect on the data set and as such to purify the data for processing the original outlier detection methods were arbitrary but now principled and systematic techniques are used drawn from the full gamut of computer science and statistics in this paper we introduce survey of contemporary techniques for outlier detection we identify their respective motivations and distinguish their advantages and disadvantages in comparative review
this paper presents novel image feature representation method called multi texton histogram mth for image retrieval mth integrates the advantages of co occurrence matrix and histogram by representing the attribute of co occurrence matrix using histogram it can be considered as generalized visual attribute descriptor but without any image segmentation or model training the proposed mth method is based on julesz's textons theory and it works directly on natural images as shape descriptor meanwhile it can be used as color texture descriptor and leads to good performance the proposed mth method is extensively tested on the corel dataset with natural images the results demonstrate that it is much more efficient than representative image feature descriptors such as the edge orientation auto correlogram and the texton co occurrence matrix it has good discrimination power of color texture and shape features
it is widely believed that distributed software development is riskier and more challenging than collocated development prior literature on distributed development in software engineering and other fields discuss various challenges including cultural barriers expertise transfer difficulties and communication and coordination overhead we evaluate this conventional belief by examining the overall development of windows vista and comparing the post release failures of components that were developed in distributed fashion with those that were developed by collocated teams we found negligible difference in failures this difference becomes even less significant when controlling for the number of developers working on binary we also examine component characteristics such as code churn complexity dependency information and test code coverage and find very little difference between distributed and collocated components to investigate if less complex components are more distributed further we examine the software process and phenomena that occurred during the vista development cycle and present ways in which the development process utilized may be insensitive to geography by mitigating the difficulties introduced in prior work in this area
seed based framework for textual information extraction allows for weakly supervised extraction of named entities from anonymized web search queries the extraction is guided by small set of seed named entities without any need for handcrafted extraction patterns or domain specific knowledge allowing for the acquisition of named entities pertaining to various classes of interest to web search users inherently noisy search queries are shown to be highly valuable albeit little explored resource for web based named entity discovery
making the structure of software visible during system development helps build shared understanding of the context for each piece of work ii identify progress with implementation and iii highlight any conflict between individual development activities finding an adequate representation for such information is not straightforward especially for large applications this paper describes an implementation of such visualization system designed to explore some of the issues involved the approach is based on war room command console metaphor and uses bank of eight linked consoles to present information the tool was applied to several industrial software systems written in mixture of java and one of which was over million lines of code in size
this paper is concerned with the parallel evaluation of datalog rule programs mainly by processors that are interconnected by communication network we introduce paradigm called data reduction for the parallel evaluation of general datalog program several parallelization strategies discussed previously in cw gst ws are special cases of this paradigm the paradigm parallelizes the evaluation by partitioning among the processors the instantiations of the rules after presenting the paradigm we discuss the following issues that we see fundamental for parallelization strategies derived from the paradigm properties of the strategies that enable reduction in the communication overhead decomposability load balancing and application to programs with negation we prove that decomposability concept introduced previously in ws cw is undecidable
dynamic load balancing is key factor in achieving high performance for large scale distributed simulations on grid infrastructures in grid environment the available resources and the simulation's computation and communication behavior may experience run time critical imbalances consequently an initial static partitioning should be combined with dynamic load balancing scheme to ensure the high performance of the distributed simulation many improved or novel dynamic load balancing designs have been proposed in recent years which aim to improve the distributed simulation performance such designs are in general non formalized and the realizations of the designs are highly time consuming and error prone practices in this paper we propose formal dynamic load balancing design approach using discrete event system specification devs we discuss the feasibility of using devs and as an additional step we consider studying recently proposed design through formalized devs model system our focus is how devs component based formalized design approach can predict some of the key design factors before the design is realized or can further validate and consolidate realized dynamic load balancing designs
the glasgow haskell compiler ghc has quite sophisticated support for concurrency in its runtime system which is written in low level code as ghc evolves the runtime system becomes increasingly complex error prone difficult to maintain and difficult to add new concurrency features this paper presents an alternative approach to implement concurrency in ghc rather than hard wiring all kinds of concurrency features the runtime system is thin substrate providing only small set of concurrency primitives and the remaining concurrency features are implemented in software libraries written in haskell this design improves the safety of concurrency support it also provides more customizability of concurrency features which can be developed as haskell library packages and deployed modularly
for large scale and residual software like network service reliability is critical requirement recent research has shown that most of network software still contains number of bugs methods for automated detection of bugs in software can be classified into static analysis based on formal verification and runtime checking based on fault injection in this paper framework for checking software security vulnerability is proposed the framework is based on automated bug detection technologies ie static analysis and fault injection which are complementary each other the proposed framework provides new direction in which various kinds of software can be checked its vulnerability by making use of static analysis and fault injection technology in experiment on proposed framework we find unknown vulnerability as well as known vulnerability in windows network module
we present the design implementation and evaluation of promise novel peer to peer media streaming system encompassing the key functions of peer lookup peer based aggregated streaming and dynamic adaptations to network and peer conditions particularly promise is based on new application level pp service called collectcast collectcast performs three main functions inferring and leveraging the underlying network topology and performance information for the selection of senders monitoring the status of peers and connections and reacting to peer connection failure or degradation with low overhead dynamically switching active senders and standby senders so that the collective network performance out of the active senders remains satisfactory based on both real world measurement and simulation we evaluate the performance of promise and discuss lessons learned from our experience with respect to the practicality and further optimization of promise
several mesh like coarse grained reconfigurable architectures have been devised in the last few years accompanied with their corresponding mapping flows one of the major bottlenecks in mapping algorithms on these architectures is the limited memory access bandwidth only few mapping methodologies encountered the problem of the limited bandwidth while none has explored how the performance improvements are affected from the architectural characteristics we study in this paper the impact that the architectural parameters have on performance speedups achieved when the pes local rams are used for storing the variables with data reuse opportunities the data reuse values are transferred in the internal interconnection network instead of being fetched from external memories in order to reduce the data transfer burden on the bus network novel mapping algorithm is also proposed that uses list scheduling technique the experimental results quantified the trade offs that exist between the performance improvements and the memory access latency the interconnection network and the processing element's local ram size for this reason our mapping methodology targets on flexible architecture template which permits such an exploration more specifically the experiments showed that the improvements increase with the memory access latency while richer interconnection topology can improve the operation parallelism by factor of on average finally for the considered set of benchmarks the operation parallelism has been improved from to from the application of our methodology and by having each pe's local ram size of words
operational transformation ot is technique originally invented for supporting consistency maintenance in collaborative text editors word processors have much richer data types and more comprehensive operations than plain text editors among others the capability of updating attributes of any types of object is an essential feature of all word processors in this paper we report an extension of ot for supporting generic update operation in addition to insert and delete operations for collaborative word processing we focus on technical issues and solutions involved in transforming updates for both consistency maintenance and group undo novel technique called multi version single display mvsd has been devised to resolve conflict between concurrent updates and integrated into the framework of ot this work has been motivated by and conducted in the coword project which aims to convert ms word into real time collaborative word processor without changing its source code this ot extension is relevant not only to word processors but also to range of interactive applications that can be modelled as editors
we present fully automatic method for content selection evaluation in summarization that does not require the creation of human model summaries our work capitalizes on the assumption that the distribution of words in the input and an informative summary of that input should be similar to each other results on large scale evaluation from the text analysis conference show that input summary comparisons are very effective for the evaluation of content selection our automatic methods rank participating systems similarly to manual model based pyramid evaluation and to manual human judgments of responsiveness the best feature jensen shannon divergence leads to correlation as high as with manual pyramid and with responsiveness evaluations
software comprehension understanding software structure and behavior is essential for developing maintaining and improving software this is particularly true of agent based systems in which the actions of autonomous agents are affected by numerous factors such as events in dynamic environment local uncertain beliefs and intentions of other agents existing comprehension tools are not suited to such large concurrent software and do not leverage concepts of the agent oriented paradigm to aid the user in understanding the software's behavior to address the software comprehension of agent based systems this research proposes method and accompanying tool that automates some of the manual tasks performed by the human user during software comprehension such as explanation generation and knowledge verification
this paper focuses on the realizability problem of framework for modeling and specifying the global behavior of reactive electronic services services in this framework web accessible programs peers communicate by asynchronous message passing and virtual global watcher listens silently to the network the global behavior is characterized by conversation which is the infinite sequence of messages observed by the watcher we show that given b√ºchi automaton specifying the desired set of conversations called conversation protocol it is possible to implement it using set of finite state peers if three realizability conditions are satisfied in particular the synthesized peers will conform to the protocol by generating only those conversations specified by the protocol our results enable top down verification strategy where conversation protocol is specified by realizable b√ºchi automaton the properties of the protocol are verified on the b√ºchi automaton specification the peer implementations are synthesized from the protocol via projection
traditional duplicate elimination techniques are not applicable to many data stream applications in general precisely eliminating duplicates in an unbounded data stream is not feasible in many streaming scenarios therefore we target at approximately eliminating duplicates in streaming environments given limited space based on well known bitmap sketch we introduce data structure stable bloom filter and novel and simple algorithm the basic idea is as follows since there is no way to store the whole history of the stream sbf continuously evicts the stale information so that sbf has room for those more recent elements after finding some properties of sbf analytically we show that tight upper bound of false positive rates is guaranteed in our empirical study we compare sbf to alternative methods the results show that our method is superior in terms of both accuracy and time effciency when fixed small space and an acceptable false positive rate are given
modern techniques for distributed information retrieval use set of documents sampled from each server but these samples have been underutilised in server selection we describe new server selection algorithm sushi which unlike earlier algorithms can make full use of the text of each sampled document and which does not need training data sushi can directly optimise for many common cases including high precision retrieval and by including simple stopping condition can do so while reducing network traffic our experiments compare sushi with alternatives and show it achieves the same effectiveness as the best current methods while being substantially more efficient selecting as few as as many servers
design and control of vector fields is critical for many visualization and graphics tasks such as vector field visualization fluid simulation and texture synthesis the fundamental qualitative structures associated with vector fields are fixed points periodic orbits and separatrices in this paper we provide new technique that allows for the systematic creation and cancellation of fixed points and periodic orbits this technique enables vector field design and editing on the plane and surfaces with desired qualitative properties the technique is based on conley theory which provides unified framework that supports the cancellation of fixed points and periodic orbits we also introduce novel periodic orbit extraction and visualization algorithm that detects for the first time periodic orbits on surfaces furthermore we describe the application of our periodic orbit detection and vector field simplification algorithms to engine simulation data demonstrating the utility of the approach we apply our design system to vector field visualization by creating data sets containing periodic orbits this helps us understand the effectiveness of existing visualization techniques finally we propose new streamline based technique that allows vector field topology to be easily identified
sensor networks are very specific type of wireless networks where both security and performance issues need to be solved efficiently in order to avoid manipulations of the sensed data and at the same time minimize the battery energy consumption this paper proposes an efficient way to perform data collection by grouping the sensors in aggregation zones allowing the aggregators to process the sensed data inside the aggregation zone in order to minimize the amount of transmissions to the sink moreover the paper provides security mechanism based on hash chains to secure data transmissions in networks with low capability sensors and without the requirements of an instantaneous source authentication
grid resources are non dedicated and thus grid users are forced to compete with resource owners for idle cpu cycles as result the turnaround times of both the grid jobs and the owners jobs are invariably delayed to resolve this problem the current study proposes progressive multi layer resource reconfiguration framework designated as pmr in which intra and inter site reconfiguration strategies are employed to adapt grid users jobs dynamically to changes in the available cpu resources at each node the experimental results show that pmr enables the idle cpu cycles of resource to be fully exploited by grid users with minimum interference to the resource owner's jobs
there exist emerging applications of data streams that require association rule mining such as network traffic monitoring and web click streams analysis different from data in traditional static databases data streams typically arrive continuously in high speed with huge amount and changing data distribution this raises new issues that need to be considered when developing association rule mining techniques for stream data this paper discusses those issues and how they are addressed in the existing literature
classification is an important problem in data mining given an example and class classifier usually works by estimating the probability of being member of ie membership probability well calibrated classifiers are those able to provide accurate estimates of class membership probabilities that is the estimated probability is close to which is the true empirical probability of being member of given that the probability estimated by the classifier is calibration is not necessary property for producing accurate classifiers and thus most of the research has focused on direct accuracy maximization strategies ie maximum margin rather than on calibration however non calibrated classifiers are problematic in applications where the reliability associated with prediction must be taken into account ie cost sensitive classification cautious classification etc in these applications sensible use of the classifier must be based on the reliability of its predictions and thus the classifier must be well calibrated in this paper we show that lazy associative classifiers lac are accurate and well calibrated using well known sound entropy minimization method we explore important applications where such characteristics ie accuracy and calibration are relevant and we demonstrate empirically that lac drastically outperforms other classifiers such as svms naive bayes and decision trees even after these classifiers are calibrated by specific methods additional highlights of lac include the ability to incorporate reliable predictions for improving training and the ability to refrain from doubtful predictions
in recent years wireless sensor networking has shown great promise in applications ranging from industrial control environmental monitoring and inventory tracking given the resource constrained nature of sensor devices and the dynamic wireless channel used for communication sensor networking protocol needs to be compact energy efficient and highly adaptable in this paper we present sampl simple aggregation and message passing layer aimed at flexible aggregation of sensor information over long period of time and supporting sporadic messages from mobile devices sampl is compact network layer that operates on top of low power csma ca based mac protocol the protocol has been designed with extensibility in mind to support new transducer devices and unforeseen applications without requiring reprogramming of the entire network sampl uses highly adaptive tree based routing scheme to achieve highly robust operation in time varying environment the protocol supports peer to peer data transactions local storage of data similar to what many rfid systems provide as well as secure gateway to infrastructure communication sampl is built on top of the nano rk operating system that runs on the firefly sensor networking platform nano rk's resource management primitives are used to create virtual energy budgets within sampl that enforce application lifetimes as of october sampl has been operating as part of the sensor andrew project at carnegie mellon university with battery powered sensor nodes for over seven months and continues to be actively used as research testbed we describe our deployment tools and network health monitoring strategies necessary for configuring and maintaining long term operation of sensor network our approach has led to sustainable average packet success rate of across the entire network
to execute mpi applications reliably fault tolerance mechanisms are needed message logging is well known solution to provide fault tolerance for mpi applications it as been proved that it can tolerate higher failure rate than coordinated checkpointing however pessimistic and causal message logging can induce high overhead on failure free execution in this paper we present op new optimistic message logging protocol based on active optimistic message logging contrary to existing optimistic message logging protocols that saves dependency information on reliable storage periodically op logs dependency information as soon as possible to reduce the amount of data piggybacked on application messages thus it reduces the overhead of the protocol on failure free execution making it more scalable and simplifying recovery op is implemented as module of the open mpi library experiments show that active message logging is promising to improve scalability and performance of optimistic message logging
we introduce sensordcsp naturally distributed benchmark based on real world application that arises in the context of networked distributed systems in order to study the performance of distributed csp discsp algorithms in truly distributed setting we use discrete event network simulator which allows us to model the impact of different network traffic conditions on the performance of the algorithms we consider two complete discsp algorithms asynchronous backtracking abt and asynchronous weak commitment search awc and perform performance comparison for these algorithms on both satisfiable and unsatisfiable instances of sensordcsp we found that random delays due to network traffic or in some cases actively introduced by the agents combined with dynamic decentralized restart strategy can improve the performance of discsp algorithms in addition we introduce gsensordcsp plain embedded version of sensordcsp that is closely related to various real life dynamic tracking systems we perform both analytical and empirical study of this benchmark domain in particular this benchmark allows us to study the attractiveness of solution repairing for solving sequence of discsps that represent the dynamic tracking of set of moving objects
we analyze theoretically the subspace best approximating images of convex lambertian object taken from the same viewpoint but under different distant illumination conditions since the lighting is an arbitrary function the space of all possible images is formally infinite dimensional however previous empirical work has shown that images of largely diffuse objects actually lie very close to five dimensional subspace in this paper we analytically construct the principal component analysis for images of convex lambertian object explicitly taking attached shadows into account and find the principal eigenmodes and eigenvalues with respect to lighting variability our analysis makes use of an analytic formula for the irradiance in terms of spherical harmonic coefficients of the illumination and shows under appropriate assumptions that the principal components or eigenvectors are identical to the spherical harmonic basis functions evaluated at the surface normal vectors our main contribution is in extending these results to the single viewpoint case showing how the principal eigenmodes and eigenvalues are affected when only limited subset the upper hemisphere of normals is available and the spherical harmonics are no longer orthonormal over the restricted domain our results are very close both qualitatively and quantitatively to previous empirical observations and represent the first essentially complete theoretical explanation of these observations our analysis is also likely to be of interest in other areas of computer vision and image based rendering in particular our results indicate that using complex illumination for photometric problems in computer vision is not significantly more difficult than using directional sources
various programming languages allow the construction of structure shy programs such programs are defined generically for many different datatypes and only specify specific behavior for few relevant subtypes typical examples are xml query languages that allow selection of subdocuments without exhaustively specifying intermediate element tags other examples are languages and libraries for polytypic or strategic functional programming and for adaptive object oriented programming in this paper we present an algebraic approach to transformation of declarative structure shy programs in particular for strategic functions and xml queries we formulate rich set of algebraic laws not just for transformation of structure shy programs but also for their conversion into structure sensitive programs and vice versa we show how subsets of these laws can be used to construct effective rewrite systems for specialization generalization and optimization of structure shy programs we present type safe encoding of these rewrite systems in haskell which itself uses strategic functional programming techniques
programs which manipulate pointers are hard to debug pointer analysis algorithms originally aimed at optimizing compilers may provide some remedy by identifying potential errors such as dereferencing null pointers by statically analyzing the behavior of programs on all their input dataour goal is to identify the core program analysis techniques that can be used when developing realistic tools which detect memory errors at compile time without generating too many false alarms our preliminary experience indicates that the following techniques are necessary finding aliases between pointers ii flow sensitive techniques that account for the program control flow constructs iii partial interpretation of conditional statements iv analysis of the relationships between pointers and sometimes analysis of the underlying data structures manipulated by the programwe show that combination of these techniques can yield better results than those achieved by state of the art tools yet it is not clear to us whether our ideas are applicable to large programs
in this paper we introduce new approach for the embedding of linear elastic deformable models our technique results in significant improvements in the efficient physically based simulation of highly detailed objects first our embedding takes into account topological details that is disconnected parts that fall into the same coarse element are simulated independently second we account for the varying material properties by computing stiffness and interpolation functions for coarse elements which accurately approximate the behaviour of the embedded material finally we also take into account empty space in the coarse embeddings which provides better simulation of the boundary the result is straightforward approach to simulating complex deformable models with the ease and speed associated with coarse regular embedding and with quality of detail that would only be possible at much finer resolution
recently the increasing use of time series data has initiated various research and development attempts in the field of data and knowledge management time series data is characterized as large in data size high dimensionality and update continuously moreover the time series data is always considered as whole instead of individual numerical fields indeed large set of time series data is from stock market stock time series has its own characteristics over other time series moreover dimensionality reduction is an essential step before many time series analysis and mining tasks for these reasons research is prompted to augment existing technologies and build new representation to manage financial time series data in this paper financial time series is represented according to the importance of the data points with the concept of data point importance tree data structure which supports incremental updating is proposed to represent the time series and an access method for retrieving the time series data point from the tree which is according to their order of importance is introduced this technique is capable to present the time series in different levels of detail and facilitate multi resolution dimensionality reduction of the time series data in this paper different data point importance evaluation methods new updating method and two dimensionality reduction approaches are proposed and evaluated by series of experiments finally the application of the proposed representation on mobile environment is demonstrated
through analysis and experiments this paper investigates two phase waiting algorithms to minimize the cost of waiting for synchronization in large scale multiprocessors in two phase algorithm thread first waits by polling synchronization variable if the cost of polling reaches limit lpoll and further waiting is necessary the thread is blocked incurring an additional fixed cost the choice of lpoll is critical determinant of the performance of two phase algorithms we focus on methods for statically determining lpoll because the run time overhead of dynamically determining lpoll can be comparable to the cost of blocking in large scale multiprocessor systems with lightweight threads our experiments show that always block lpoll is good waiting algorithm with performance that is usually close to the best of the algorithms compared we show that even better performance can be achieved with static choice of lpoll based on knowledge of likely wait time distributions motivated by the observation that different synchronization types exhibit different wait time distributions we prove that static choice of lpoll can yield close to optimal on line performance against an adversary that is restricted to choosing wait times from fixed family of probability distributions this result allows us to make an optimal static choice of lpoll based on synchronization type for exponentially distributed wait times we prove that setting lpoll results in waiting cost that is no more than times the cost of an optimal off line algorithm for uniformly distributed wait times we prove that setting lpoll square root of results in waiting cost that is no more than square root of the golden ratio times the cost of an optimal off line algorithm experimental measurements of several parallel applications on the alewife multiprocessor simulator corroborate our theoretical findings
code sandboxing is useful for many purposes but most sandboxing techniques require kernel modifications do not completely isolate guest code or incur substantial performance costs vx is multipurpose user level sandbox that enables any application to load and safely execute one or more guest plug ins confining each guest to system call api controlled by the host application and to restricted memory region within the host's address space vx runs guest code efficiently on several widespread operating systems without kernel extensions or special privileges it protects the host program from both reads and writes by its guests and it allows the host to restrict the instruction set available to guests the key to vx combination of portability flexibility and efficiency is its use of segmentation hardware to sandbox the guest's data accesses along with lightweight instruction translator to sandbox guest instructions we evaluate vx using microbenchmarks and whole system benchmarks and we examine four applications based on vx an archival storage system an extensible public key infrastructure an experimental user level operating system running atop another host os and linux system call jail the first three applications export custom apis independent of the host os to their guests making their plug ins binary portable across host systems compute intensive workloads for the first two applications exhibit between slowdown and speedup on vx relative to native execution speedups result from vx instruction translator improving the cache locality of guest code the experimental user level operating system allows the use of the guest os's applications alongside the host's native applications and runs faster than whole system virtual machine monitors such as vmware and qemu the linux system call jail incurs up to overhead but requires no kernel modifications and is delegation based avoiding concurrency vulnerabilities present in other interposition mechanisms
little is known about the strategies end user programmers use in debugging their programs and even less is known about gender differences that may exist in these strategies without this type of information designers of end user programming systems cannot know the target at which to aim if they are to support male and female end user programmers we present study investigating this issue we asked end user programmers to debug spreadsheets and to describe their debugging strategies using mixed methods we analyzed their strategies and looked for relationships among participants strategy choices gender and debugging success our results indicate that males and females debug in quite different ways that opportunities for improving support for end user debugging strategies for both genders are abundant and that tools currently available to end user debuggers may be especially deficient in supporting debugging strategies used by females
visual complexity is an apparent feature in website design yet its effects on cognitive and emotional processing are not well understood the current study examined website complexity within the framework of aesthetic theory and psychophysiological research on cognition and emotion we hypothesized that increasing the complexity of websites would have detrimental cognitive and emotional impact on users in passive viewing task pvt website screenshots differing in their degree of complexity operationalized by jpeg file size correlation with complexity ratings in preliminary study were presented to participants in randomized order additionally standardized visual search task vst assessing reaction times and one week delayed recognition task on these websites were conducted and participants rated all websites for arousal and valence psychophysiological responses were assessed during the pvt and vst visual complexity was related to increased experienced arousal more negative valence appraisal decreased heart rate and increased facial muscle tension musculus corrugator visual complexity resulted in increased reaction times in the vst and decreased recognition rates reaction times in the vst were related to increases in heart rate and electrodermal activity these findings demonstrate that visual complexity of websites has multiple effects on human cognition and emotion including experienced pleasure and arousal facial expression autonomic nervous system activation task performance and memory it should thus be considered an important factor in website design
the vast expansion of interconnectivity with the internet and the rapid evolution of highly capable but largely insecure mobile devices threatens cellular networks in this paper we characterize the impact of the large scale compromise and coordination of mobile phones in attacks against the core of these networks through combination of measurement simulation and analysis we demonstrate the ability of botnet composed of as few as compromised mobile phones to degrade service to area code sized regions by as such attacks are accomplished through the execution of network service requests and not constant stream of phone calls users are unlikely to be aware of their occurrence we then investigate number of significant network bottlenecks their impact on the density of compromised nodes per base station and how they can be avoided we conclude by discussing number of countermeasures that may help to partially mitigate the threats posed by such attacks
many enterprise applications require the use of object oriented middleware and message oriented middleware in combination middleware mediated transacdons have been proposed as transaction model to address reliability of such applications they extend distributed object transactions to include message oriented transactions in this paper we present three message queuing patterns that we have found useful for implementing middleware mediated transactions we discuss and show how the patterns can be applied to support guaranteed compensation in the engineering of transactional enterprise applications
join is fundamental operator in data stream management system dsms it is more efficient to share execution of multiple windowed joins than separate execution of everyone because the former saves part of cost in common windows therefore shared window join is adopted widely in multi queries dsms when all tasks of queries exceed maximum system capacity the overloaded dsms fails to process all of its input data and keep up with the rates of data arrival especially in time critical environment queries should be completed not just timely but within certain deadlines in this paper we address load shedding approach for shared window join over real time data streams load shedding algorithm ls sjrt cw is proposed to handle queries shared window join in overloaded real time system effectively it would reduce load shedding overhead by adjusting sliding window size experiment results show that our algorithm would decrease average deadline miss ratio over some ranges of workloads
as interactive multimedia communications are developing rapidly on the internet they present stringent challenges on end to end ee performance on the other hand however the internet's architecture ipv remains almost the same as it was originally designed for only data transmission purpose and has experienced big hurdle to actualize qos universally this paper designs cooperatively overlay routing service cors aiming to overcome the performance limit inherent in the internet's ip layer routing service the key idea of cors is to efficiently compose number of eligible application layer paths with suitable relays in the overlay network besides the direct ip path cors can transfer data simultaneously through one or more application layer paths to adaptively satisfy the data's application specific requirements on ee performance simulation results indicate the proposed schemes are scalable and effective practical experiments based on prototype implemented on planetlab show that cors is feasible to enhance the transmission reliability and the quality of multimedia communications
sensors have been increasingly used for many ubiquitous computing applications such as asset location monitoring visual surveillance and human motion tracking in such applications it is important to place sensors such that every point of the target area can be sensed by more than one sensor especially many practical applications require coverage for triangulation hull building and etc also in order to extract meaningful information from the data sensed by multiple sensors those sensors need to be placed not too close to each other minimum separation requirement to address the coverage problem with the minimum separation requirement our recent work kim et al proposes two heuristic methods so called overlaying method and tre based method which complement each other depending on the minimum separation requirement for these two methods we also provide mathematical analysis that can clearly guide us when to use the tre based method and when to use the overlaying method and also how many sensors are required to make it self contained in this paper we first revisit the two heuristic methods then as an extension we present an ilp based optimal solution targeting for grid coverage with this ilp based optimal solution we investigate how much close the two heuristic methods are to the optimal solution finally this paper discusses the impacts of the proposed methods on real deployed systems using two example sensor systems to the best of our knowledge this is the first work that systematically addresses the coverage problem with the minimum separation requirement
mobile location aware applications have become quite popular across range of new areas such as pervasive games and mobile edutainment applications however it is only recently that approaches have been presented which combine gaming and education with mobile augmented reality systems however they typically lack close crossmedia integration of the surroundings and often annotate or extend the environment rather than modifying and altering it in this paper we present mobile outdoor mixed reality game for exploring the history of city in the spatial and the temporal dimension we introduce the design and concept of the game and present universal mechanism to define and setup multi modal user interfaces for the game challenges finally we discuss the results of the user tests
surrogate is an object that stands for document and enables navigation to that document hypermedia is often represented with textual surrogates even though studies have shown that image and text surrogates facilitate the formation of mental models and overall understanding surrogates may be formed by breaking document down into set of smaller elements each of which is surrogate candidate while processing these surrogate candidates from an html document relevant information may appear together with less useful junk material such as navigation bars and advertisements this paper develops pattern recognition based approach for eliminating junk while building the set of surrogate candidates the approach defines features on candidate elements and uses classification algorithms to make selection decisions based on these features for the purpose of defining features in surrogate candidates we introduce the document surrogate model dsm streamlined document object model dom like representation of semantic structure using quadratic classifier we were able to eliminate junk surrogate candidates with an average classification rate of by using this technique semiautonomous agents can be developed to more effectively generate surrogate collections for users we end by describing new approach for hypermedia and the semantic web which uses the dsm to define value added surrogates for document
we consider generic garbled circuit gc based techniques for secure function evaluation sfe in the semi honest modelwe describe efficient gc constructions for addition subtraction multiplication and comparison functions our circuits for subtraction and comparison are approximately two times smaller in terms of garbled tables than previous constructions this implies corresponding computation and communication improvements in sfe of functions using our efficient building blocks the techniques rely on recently proposed free xor gc techniquefurther we present concrete and detailed improved gc protocols for the problem of secure integer comparison and related problems of auctions minimum selection and minimal distance performance improvement comes both from building on our efficient basic blocks and several problem specific gc optimizations we provide precise cost evaluation of our constructions which serves as baseline for future protocols
the restricted correspondence problem is the task of solving the classical stereo correspondence problem when the surface being observed is known to belong to family of surfaces that vary in known way with one or more parameters under this constraint the surface can be extracted far more robustly than by classical stereo applied to an arbitrary surface since the problem is solved semi globally rather than locally for each epipolar line here the restricted correspondence problem is solved for two examples the first being the extraction of the parameters of an ellipsoid from calibrated stereo pair the second example is the estimation of the osculating paraboloid at the frontier points of convex object
recently research on text mining has attracted lots of attention from both industrial and academic fields text mining concerns of discovering unknown patterns or knowledge from large text repository the problem is not easy to tackle due to the semi structured or even unstructured nature of those texts under consideration many approaches have been devised for mining various kinds of knowledge from texts one important aspect of text mining is on automatic text categorization which assigns text document to some predefined category if the document falls into the theme of the category traditionally the categories are arranged in hierarchical manner to achieve effective searching and indexing as well as easy comprehension for human beings the determination of category themes and their hierarchical structures were most done by human experts in this work we developed an approach to automatically generate category themes and reveal the hierarchical structure among them we also used the generated structure to categorize text documents the document collection was trained by self organizing map to form two feature maps these maps were then analyzed to obtain the category themes and their structure although the test corpus contains documents written in chinese the proposed approach can be applied to documents written in any language and such documents can be transformed into list of separated terms
unsupervised clustering can be significantly improved using supervision in the form of pairwise constraints ie pairs of instances labeled as belonging to same or different clusters in recent years number of algorithms have been proposed for enhancing clustering quality by employing such supervision such methods use the constraints to either modify the objective function or to learn the distance measure we propose probabilistic model for semi supervised clustering based on hidden markov random fields hmrfs that provides principled framework for incorporating supervision into prototype based clustering the model generalizes previous approach that combines constraints and euclidean distance learning and allows the use of broad range of clustering distortion measures including bregman divergences eg euclidean distance and divergence and directional similarity measures eg cosine similarity we present an algorithm that performs partitional semi supervised clustering of data by minimizing an objective function derived from the posterior energy of the hmrf model experimental results on several text data sets demonstrate the advantages of the proposed framework
evaluative texts on the web have become valuable source of opinions on products services events individuals etc recently many researchers have studied such opinion sources as product reviews forum posts and blogs however existing research has been focused on classification and summarization of opinions using natural language processing and data mining techniques an important issue that has been neglected so far is opinion spam or trustworthiness of online opinions in this paper we study this issue in the context of product reviews which are opinion rich and are widely used by consumers and product manufacturers in the past two years several startup companies also appeared which aggregate opinions from product reviews it is thus high time to study spam in reviews to the best of our knowledge there is still no published study on this topic although web spam and email spam have been investigated extensively we will see that opinion spam is quite different from web spam and email spam and thus requires different detection techniques based on the analysis of million reviews and million reviewers from amazoncom we show that opinion spam in reviews is widespread this paper analyzes such spam activities and presents some novel techniques to detect them
given string and language the hamming distance of to is the minimum hamming distance of to any string in the edit distance of string to language is analogously definedfirst we prove that there is language in ac such that both hamming and edit distance to this language are hard to approximate they cannot be approximated with factor for any unless np denotes the length of the input string second we show the parameterized intractability of computing the hamming distance we prove that for every there exists language in ac for which computing the hamming distance is hard moreover there is language in for which computing the hamming distance is wp hardthen we show that the problems of computing the hamming distance and of computing the edit distance are in some sense equivalent by presenting approximation ratio preserving reductions from the former to the latter and vice versafinally we define hamp to be the class of languages to which the hamming distance can efficiently ie in polynomial time be computed we show some properties of the class hamp on the other hand we give evidence that characterization in terms of automata or formal languages might be difficult
in this paper new color space called the rgb color ratio space is proposed and defined according to reference color such that an image can be transformed from conventional color space to the rgb color ratio space because color in the rgb color ratio space is represented as three color ratios and intensity the chrominance can be completely reserved three color ratios and the luminance can be de correlated with the chrominance different from traditional distance measurement road color model is determined by an ellipse area in the rgb ratio space enclosed by the estimated boundaries proposed adaptive fuzzy logic in which fuzzy membership functions are defined according to estimated boundaries is introduced to implement clustering rules therefore each pixel will have its own fuzzy membership function corresponding to its intensity basic neural network is trained and used to achieve parameters optimization the low computation cost of the proposed segmentation method shows the feasibility for real time application experimental results for road detection demonstrate the robustness to intensity variation of the proposed approach
we show the decidability of model checking pa processes against several first order logics based upon the reachability predicate the main tool for this result is the recognizability by tree automata of the reachability relation the tree automata approach and the transition logics we use allow smooth and general treatment of parameterized model checking for pa this approach is extended to handle quite general notion of costs of pa steps in particular when costs are parikh images of traces we show decidability of transition logic extended by some form of first order reasoning over costs
hierarchical agent framework is proposed to construct monitoring layer towards self aware parallel systems on chip socs with monitoring services as new design dimension systems are capable of observing and reconfiguring themselves dynamically at all levels of granularity based on application requirements and platform conditions agents with hierarchical priorities work adaptively and cooperatively to maintain and improve system performance in the presence of variations and faults function partitioning of agents and hierarchical monitoring operations on parallel socs are analyzed applying the design approach on the network on chip noc platform demonstrates the design process and benefits using the novel approach
proliferation of portable wireless enabled laptop computers and pdas cost effective deployment of access points and availability of the license exempt bands and appropriate networking standards contribute to the conspicuous success of ieee wlans in the article we provide comprehensive overview of techniques for capacity improvement and qos provisioning in the ieee protocol family these techniques represent the efforts both in the research community and the ieee working groups specifically we summarize the operations of ieee legacy as well as its extension introduce several protocol modeling techniques and categorize the various approaches to improve protocol capacity to provide qos by either devising new mac protocol components or fine tuning protocol parameters in ieee and to judiciously arbitrate radio resources eg transmission rate and power to demonstrate how to adapt qos provisioning in newly emerging areas we use the wireless mesh network as an example discuss the role ieee plays in such network and outline research issues that arise
digital audio video data have become an integral part of multimedia information systems to reduce storage and bandwidth requirements they are commonly stored in compressed format such as mpeg increasing amounts of mpeg encoded audio and video documents are available online and in proprietary collections in order to effectively utilise them we need tools and techniques to automatically analyse segment and classify mpeg video content several techniques have been developed both in the audio and visual domain to analyse videos this paper presents survey of audio and visual analysis techniques on mpeg encoded media that are useful in supporting variety of video applications although audio and visual feature analyses have been carried out extensively they become useful to applications only when they convey semantic meaning of the video content therefore we also present survey of works that provide semantic analysis on mpeg encoded videos
bottom sketch is summary of set of items with nonnegative weights each such summary allows us to compute approximate aggregates over the set of items bottom sketches are obtained by associating with each item in ground set an independent random rank drawn from probability distribution that depends on the weight of the item for each subset of interest the bottom sketch is the set of the minimum ranked items and their ranks bottom sketches have numerous applications we develop and analyze data structures and estimators for bottom sketches to facilitate their deployment we develop novel estimators and algorithms that show that they are superior alternative to other sketching methods in both efficiency of obtaining the sketches and the accuracy of the estimates derived from the sketches
novel touch based interaction method by use of orientation information of touch region is proposed to capture higher dimensional information of touch including position and an orientation as well we develop robust algorithms to detect contact shape and to estimate its orientation angle also we suggest practical guidelines to use our method through experiments considering various conditions and show possible service scenarios of aligning documents and controlling media player
this paper offers theoretical study of constraint simplification fundamental issue for the designer of practical type inference system with subtyping in the simpler case where constraints are equations simple isomorphism between constrained type schemes and finite state automata yields complete constraint simplification method using it as guide for the intuition we move on to the case of subtyping and describe several simplification algorithms although no longer complete they are conceptually simple efficient and very effective in practice overall this paper gives concise theoretical account of the techniques found at the core of our type inference system our study is restricted to the case where constraints are interpreted in non structural lattice of regular terms nevertheless we highlight small number of general ideas which explain our algorithms at high level and may be applicable to variety of other systems copyright academic press
personalization of learning has become prominent issue in the educational field at various levels this article elaborates different view on personalisation than what usually occurs in this area its baseline is that personalisation occurs when learning turns out to become personal in the learner's mind through literature survey we analyze constitutive dimensions of this inner sense of personalisation here we devote special attention to confronting learners with tracked information making their personal interaction footprints visible contrasts with the back office usage of this data by researchers instructors or adaptive systems we contribute prototype designed for the moodle platform according to the conceptual approach presented here
the fluid documents project has developed various research prototypes that show that powerful annotation techniques based on animated typographical changes can help readers utilize annotations more effectively our recently developed fluid open hypermedia prototype supports the authoring and browsing of fluid annotations on third party web pages this prototype is an extension of the arakne environment an open hypermedia application that can augment web pages with externally stored hypermedia structures this paper describes how various web standards including dom css xlink xpointer and rdf can be used and extended to support fluid annotations
wireless sensor networks have created new opportunities for data collection in variety of scenarios such as environmental and industrial where we expect data to be temporally and spatially correlated researchers may want to continuously collect all sensor data from the network for later analysis suppression both temporal and spatial provides opportunities for reducing the energy cost of sensor data collection we demonstrate how both types can be combined for maximal benefit we frame the problem as one of monitoring node and edge constraints monitored node triggers report if its value changes monitored edge triggers report if the difference between its nodes values changes the set of reports collected at the base station is used to derive all node values we fully exploit the potential of this global inference in our algorithm conch short for constraint chaining constraint chaining builds network of constraints that are maintained locally but allow global view of values to be maintained with minimal cost network failure complicates the use of suppression since either causes an absence of reports we add enhancements to conch to build in redundant constraints and provide method to interpret the resulting reports in case of uncertainty using simulation we experimentally evaluate conch's effectiveness against competing schemes in number of interesting scenarios
abstract researchers have recently discovered several interesting self organized regularities from the world wide web ranging from the structure and growth of the web to the access patterns in web surfing what remains to be great challenge in web log mining is how to explain user behavior underlying observed web usage regularities in this paper we will address the issue of how to characterize the strong regularities in web surfing in terms of user navigation strategies and present an information foraging agent based approach to describing user behavior by experimenting with the agent based decision models of web surfing we aim to explain how some web design factors as well as user cognitive factors may affect the overall behavioral patterns in web usage
most video retrieval systems are multimodal commonly relying on textual information low and high level semantic features extracted from query visual examples in this work we study the impact of exploiting different knowledge sources in order to automatically retrieve query visual examples relevant to video retrieval task our hypothesis is that the exploitation of external knowledge sources can help on the identification of query semantics as well as on improving the understanding of video contents we propose set of techniques to automatically obtain additional query visual examples from different external knowledge sources such as dbpedia flickr and google images which have different coverage and structure characteristics the proposed strategies attempt to exploit the semantics underlying the above knowledge sources to reduce the ambiguity of the query and to focus the scope of the image searches in the repositories we assess and compare the quality of the images obtained from the different external knowledge sources when used as input of number of video retrieval tasks we also study how much they complement manually provided sets of examples such as those given by trecvid tasks based on our experimental results we report which external knowledge source is more likely to be suitable for the evaluated retrieval tasks results also demonstrate that the use of external knowledge can be good complement to manually provided examples and when lacking of visual examples provided by user our proposed approaches can retrieve visual examples to improve the user's query
we introduce light weight scalable truthful routing protocol lstop for selfish nodes problem in mobile ad hoc networks where node may use different cost to send packets to different neighbours lstop encourages nodes cooperation by rewarding nodes for their forwarding service according to their cost it incurs low overhead of in the worst case and only on the average we show the truthfulness of lstop and present the result of an extensive simulation study to show that lstop approaches optimal cost routing and achieves significant better network performance compared to ad hoc vcg
dynamic binary translators dbts provide powerful platforms for building dynamic program monitoring and adaptation tools dbts however have high memory demands because they cache translated code and auxiliary code to software code cache and must also maintain data structures to support the code cache the high memory demands make it difficult for memory constrained embedded systems to take advantage of dbt based tools previous research on dbt memory management focused on the translated code and auxiliary code only however we found that data structures are comparable to the code cache in size we show that the translated code size auxiliary code size and the data structure size interact in complex manner depending on the path selection trace selection and link formation strategy therefore holistic memory efficiency comprising translated code auxiliary code and data structures cannot be improved by focusing on the code cache only in this paper we use path selection for improving holistic memory efficiency which in turn impacts performance in memory constrained environments although there has been previous research on path selection such research only considered performance in memory unconstrained environments the challenge for holistic memory efficiency is that the path selection strategy results in complex interactions between the memory demand components also individual aspects of path selection and the holistic memory efficiency may impact performance in complex ways we explore these interactions to motivate path selection targeting holistic memory demand we enumerate all the aspects involved in path selection design and evaluate comprehensive set of approaches for each aspect finally we propose path selection strategy that reduces memory demands by and at the same time improves performance by compared to an industrial strength dbt
effective identification of coexpressed genes and coherent patterns in gene expression data is an important task in bioinformatics research and biomedical applications several clustering methods have recently been proposed to identify coexpressed genes that share similar coherent patterns however there is no objective standard for groups of coexpressed genes the interpretation of co expression heavily depends on domain knowledge furthermore groups of coexpressed genes in gene expression data are often highly connected through large number of intermediate genes there may be no clear boundaries to separate clusters clustering gene expression data also faces the challenges of satisfying biological domain requirements and addressing the high connectivity of the data sets in this paper we propose an interactive framework for exploring coherent patterns in gene expression data novel coherent pattern index is proposed to give users highly confident indications of the existence of coherent patterns to derive coherent pattern index and facilitate clustering we devise an attraction tree structure that summarizes the coherence information among genes in the data set we present efficient and scalable algorithms for constructing attraction trees and coherent pattern indices from gene expression data sets our experimental results show that our approach is effective in mining gene expression data and is scalable for mining large data sets
we address specific enterprise document search scenario where the information need is expressed in an elaborate manner in our scenario information needs are expressed using short query of few keywords together with examples of key reference pages given this setup we investigate how the examples can be utilized to improve the end to end performance on the document retrieval task our approach is based on language modeling framework where the query model is modified to resemble the example pages we compare several methods for sampling expansion terms from the example pages to support query dependent and query independent query expansion the latter is motivated by the wish to increase aspect recall and attempts to uncover aspects of the information need not captured by the query for evaluation purposes we use the csiro data set created for the trec enterprise track the best performance is achieved by query models based on query independent sampling of expansion terms from the example documents
the view update problem is concerned with indirectly modifying those tuples that satisfy view or derived table by an appropriate update against the corresponding base tables the notion of deduction tree is defined and the relationship between such trees and the view update problem for indefinite deductive databases is considered it is shown that traversal of an appropriate deduction tree yields sufficient information to perform view updates at the propositional level to obtain similar result at the first order level it is necessary for theoretical and computational reasons to impose some weak stratification and definiteness constraints on the database
this paper describes new method for contouring signed grid whose edges are tagged by hermite data ie exact intersection points and normals this method avoids the need to explicitly identify and process features as required in previous hermite contouring methods using new numerically stable representation for quadratic error functions we develop an octree based method for simplifying contours produced by this method we next extend our contouring method to these simpli pound ed octrees this new method imposes no constraints on the octree such as being restricted octree and requires no crack patching we conclude with simple test for preserving the topology of the contour during simplification
benchmarking is critical when evaluating performance but is especially difficult for file and storage systems complex interactions between devices caches kernel daemons and other os components result in behavior that is rather difficult to analyze moreover systems have different features and optimizations so no single benchmark is always suitable the large variety of workloads that these systems experience in the real world also adds to this difficulty in this article we survey file system and storage benchmarks from recent papers we found that most popular benchmarks are flawed and many research papers do not provide clear indication of true performance we provide guidelines that we hope will improve future performance evaluations to show how some widely used benchmarks can conceal or overemphasize overheads we conducted set of experiments as specific example slowing down read operations on ext by factor of resulted in only percnt wall clock slowdown in popular compile benchmark finally we discuss future work to improve file system and storage benchmarking
vector and matrix clocks are extensively used in asynchronous distributed systems this paper asks how does the clock abstraction generalize to address this problem the paper motivates and proposes logical clocks of arbitrary dimensions it then identifies and explores the conceptual link between such clocks and knowledge it establishes the necessary and sufficient conditions on the size and dimension of clocks required to attain any specified level of knowledge about the timestamp of the most recent system state for which this is possible without using any messages in the clock protocol the paper then gives algorithms to determine the time stamp of the latest system state about which specified level of knowledge is attainable in given system state and to compute the timestamp of the earliest system state in which specified level of knowledge about given system state is attainable the results are applicable to applications that deal with certain class of properties identified as monotonic properties
check if we can apply woodruff's method to our protocol we show an efficient secure two party protocol based on yao's construction which provides security against malicious adversaries yao's original protocol is only secure in the presence of semi honest adversaries security against malicious adversaries can be obtained by applying the compiler of goldreich micali and wigderson the gmw compiler however this approach does not seem to be very practical as it requires using generic zero knowledge proofsour construction is based on applying cut and choose techniques to the original circuit and inputs security is proved according to the ideal real simulation paradigm and the proof is in the standard model with no random oracle model or common reference string assumptions the resulting protocol is computationally efficient the only usage of asymmetric cryptography is for running oblivious transfers for each input bit or for each bit of statistical security parameter whichever is larger our protocol combines techniques from folklore like cut and choose along with new techniques for efficiently proving consistency of inputs we remark that naive implementation of the cut and choose technique with yao's protocol does not yield secure protocol this is the first paper to show how to properly implement these techniques and to provide full proof of securityour protocol can also be interpreted as constant round black box reduction of secure two party computation to oblivious transfer and perfectly hiding commitments or black box reduction of secure two party computation to oblivious transfer alone with number of rounds which is linear in statistical security parameter these two reductions are comparable to kilian's reduction which uses ot alone but incurs number of rounds which is linear in the depth of the circuit
reuse signature or reuse distance pattern is an accurate model for program memory accessing behaviors it has been studied and shown to be effective in program analysis and optimizations by many recent works however the high overhead associated with reuse distance measurement restricts the scope of its application this paper explores applying sampling in reuse signature collection to reduce the time overhead we compare different sampling strategies and show that an enhanced systematic sampling with uniform coverage of all distance ranges can be used to extrapolate the reuse distance distribution based on that analysis we present novel sampling method with measurement accuracy of more than our average speedup of reuse signature collection is while the best improvement observed is this is the first attempt to utilize sampling in measuring reuse signatures experiments with varied programs and instrumentation tools show that sampling has great potential in promoting the practical uses of reuse signatures and enabling more optimization opportunities
embedded system designers face unique set of challenges in making their systems more secure as these systems often have stringent resource constraints or must operate in harsh or physically insecure environments one of the security issues that have recently drawn attention is software integrity which ensures that the programs in the system have not been changed either by an accident or an attack in this paper we propose an efficient hardware mechanism for runtime verification of software integrity using encrypted instruction block signatures we introduce several variations of the basic mechanism and give details of three techniques that are most suitable for embedded systems performance evaluation using selected mibench mediabench and basicrypt benchmarks indicates that the considered techniques impose relatively small performance overhead the best overall technique has performance overhead in the range when protecting byte instruction blocks with byte signatures with byte instruction blocks the overhead is in the range the average overhead with kb cache is with additional investment in signature cache this overhead can be almost completely eliminated
clustered microarchitectures are an effective organization to deal with the problem of wire delays and complexity by partitioning some of the processor resources the organization of the data cache is key factor in these processors due to its effect on cache miss rate and inter cluster communications this paper investigates alternative designs of the data cache centralized distributed replicated and physically distributed cache architectures are analyzed results show similar average performance but significant performance variations depending on the application features specially cache miss ratio and communications in addition we also propose novel instruction steering scheme in order to reduce communications this scheme conditionally stalls the dispatch of instructions depending on the occupancy of the clusters whenever the current instruction cannot be steered to the cluster holding most of the inputs this new steering outperforms traditional schemes results show an average speedup of and up to for some applications
age specific human computer interaction ashci has vast potential applications in daily life however automatic age estimation technique is still underdeveloped one of the main reasons is that the aging effects on human faces present several unique characteristics which make age estimation challenging task that requires non standard classification approaches according to the speciality of the facial aging effects this paper proposes the ages aging pattern subspace method for automatic age estimation the basic idea is to model the aging pattern which is defined as sequence of personal aging face images by learning representative subspace the proper aging pattern for an unseen face image is then determined by the projection in the subspace that can best reconstruct the face image while the position of the face image in that aging pattern will indicate its age the ages method has shown encouraging performance in the comparative experiments either as an age estimator or as an age range estimator
this paper presents an empirical study that evaluates oo method function points oomfp functional size measurement procedure for object oriented systems that are specified using the oo method approach laboratory experiment with students was conducted to compare oomfp with the ifpug function point analysis fpa procedure on range of variables including efficiency reproducibility accuracy perceived ease of use perceived usefulness and intention to use the results show that oomfp is more time consuming than fpa but the measurement results are more reproducible and accurate the results also indicate that oomfp is perceived to be more useful and more likely to be adopted in practice than fpa in the context of oo method systems development we also report lessons learned and suggest improvements to the experimental procedure employed and replications of this study using samples of industry practitioners
as an approach that applies not only to support user navigation on the web recommender systems have been built to assist and augment the natural social process of asking for recommendations from other people in typical recommender system people provide suggestions as inputs which the system aggregates and directs to appropriate recipients in some cases the primary computation is in the aggregation in others the value of the system lies in its ability to make good matches between the recommenders and those seeking recommendationsin this paper we discuss the architectural and design features of webmemex system that provides recommended information based on the captured history of navigation from list of people well known to the users including the users themselves allows users to have access from any networked machine demands user authentication to access the repository of recommendations and allows users to specify when the capture of their history should be performed
as the world uses more digital video that requires greater storage space grid computing is becoming indispensable for urgent problems in multimedia content analysis parallel horus support tool for applications in multimedia grid computing lets users implement multimedia applications as sequential programs for efficient execution on clusters and grids based on wide area multimedia services
traditionally software pipelining is applied either to theinnermost loop of given loop nest or from the innermostloop to outer loops in this paper we propose three stepapproach called single dimension software pipelining ssp to software pipeline loop nest at an arbitraryloop levelthe first step identifies the most profitable loop level forsoftware pipelining in terms of initiation rate or data reusepotential the second step simplifies the multi dimensionaldata dependence graph ddg into dimensional ddgand constructs dimensional schedule for the selectedloop level the third step derives simple mapping functionwhich specifies the schedule time for the operations of themulti dimensional loop based on the dimensional schedulewe prove that the ssp method is correct and at least asefficient as other modulo scheduling methodswe establish the feasibility and correctness of our approachby implementing it on the ia architecture experimentalresults on small number of loops show significantperformance improvements over existing modulo schedulingmethods that software pipeline loop nest from the innermostloop
the problem of performing tasks on asynchronous or undependable processors is basic problem in distributed computing this paper considers an abstraction of this problem called write all using processors write into all locations of an array of size in this problem writing abstracts the notion of performing simple task despite substantial research there is dearth of efficient deterministic asynchronous algorithms for write all efficiency of algorithms is measured in terms of work that accounts for all local steps performed by the processors in solving the problem thus an optimal algorithm would have work however it is known that optimality cannot be achieved when the quest then is to obtain work optimal solutions for this problem using non trivial compared to number of processors recently it was shown that optimality can be achieved using non trivial number of processors where log the new result in this paper significantly extends the range of processors for which optimality is achieved the result shows that optimality can be achieved using close to processors more precisely using log processors for any additionally the new result uses only the atomic read write memory without resorting to using the test and set primitive that was necessary in the previous solution this paper presents the algorithm and gives its analysis showing that the work complexity of the algorithm is which is optimal when while all prior deterministic algorithms require super linear work when
given sequence of symbols over some alphabet sigma of size sigma we develop new compression methods that are very simple to implement ii provide time random access to any symbol or short substring of the original sequence our simplest solution uses at most bits of space where and is the zeroth order empirical entropy of we discuss number of improvements and trade offs over the basic method for example we can achieve bits of space for log sigma several applications are discussed including text compression compressed full text indexing and string matching
most search engines display some document metadata such as title snippet and url in conjunction with the returned hits to aid users in determining documents however metadata is usually fragmented pieces of information that even when combined does not provide an overview of returned document in this paper we propose mechanism of enriching metadata of the returned results by incorporating automatically extracted document keyphrases with each returned hit we hypothesize that keyphrases of document can better represent the major theme in that document therefore by examining the keyphrases in each returned hit users can better predict the content of documents and the time spent on downloading and examining the irrelevant documents will be reduced substantially
when the mobile environment consists of light weight devices the energy consumption of location based services lbss and the limited bandwidth of the wireless network become important issues motivated by this we propose new spatial query processing algorithms to support mobile continuous nearest neighbor query mcnnq in wireless broadcast environments our solution provides general client server architecture for answering mcnnq on objects with unknown and possibly variable movement types our solution enables the application of spatio temporal access methods specifically designed for particular type to arbitrary movements without any false misses our algorithm does not require any conventional spatial index for mcnnq processing it can be adapted to static or moving objects and does not require additional knowledge eg direction of moving objects beyond the maximum speed and the location of each object extensive experiments demonstrate that our location based data dissemination algorithm significantly outperforms index based solutions
creating maintaining or using digital library requires the manipulation of digital documents information workspaces provide visual representation allowing users to collect organize annotate and author information the visual knowledge builder vkb helps users access collect annotate and combine materials from digital libraries and other sources into personal information workspace vkb has been enhanced to include direct search interfaces for nsdl and google users create visualization of search results while selecting and organizing materials for their current activity additionally metadata applicators have been added to vkb this interface allows the rapid addition of metadata to documents and aids the user in the extraction of existing metadata for application to other documents study was performed to compare the selection and organization of documents in vkb to the commonly used tools ofa web browser and word processor this study shows the value of visual workspaces for such effort but points to the need for subdocument level objects ephemeral visualizations and support for moving from visual representations to metadata
growing attention is being paid to application security at requirements engineering time confidentiality is particular subclass of security concerns that requires sensitive information to never be disclosed to unauthorized agents disclosure refers to undesired knowledge states of such agents in previous work we have extended our requirements specification framework with epistemic constructs for capturing what agents may or may not know about the application roughly an agent knows some property if the latter is found in the agent's memorythis paper makes the semantics of such constructs further precise through formal model of how sensitive information may appear or disappear in an agent's memory based on this extended framework catalog of specification patterns is proposed to codify families of confidentiality requirements proof of concept tool is presented for early checking of requirements models against such confidentiality patterns in case of violation the counterexample scenarios generated by the tool show how an unauthorized agent may acquire confidential knowledge counter measures should then be devised to produce further confidentiality requirements
constructing multipath routes in manets is important for providing reliable delivery load balancing and bandwidth aggregation however popular multipath routing approaches fail to produce spatially disjoint routes in simple and cost effective manner and existing single path approaches cannot be easily modified to produce multiple disjoint routes in this paper we propose electric field based routing efr as reliable framework for routing in manets by applying the concept of electric field lines our location based protocol naturally provides spatially disjoint routes based on the shapes of these lines the computation is highly localized and requires no explicit coordination among routes efr can also be easily extended to offer load balancing bandwidth aggregation and power management through simulation efr shows higher delivery ratio and lower overhead under high mobility high network loads and network failures compared to popular multipath and location based schemes efr also demonstrates high resiliency to dos attacks
simulation is widely used for developing evaluating and analyzing sensor network applications especially when deploying large scale sensor network remains expensive and labor intensive however due to its computation intensive nature existent simulation tools have to make trade offs between fidelity and scalability and thus offer limited capabilities as design and analysis tools in this paper we introduce disens distributed sensor network simulation highly scalable distributed simulation system for sensor networks disens does not only faithfully emulates an extensive set of sensor hardware and supports extensible radio power models so that sensor network applications can be simulated transparently with high fidelity but also employs distributed memory parallel cluster system to attack the complex simulation problem combining an efficient distributed synchronization protocol and sophisticated node partitioning algorithm based on existent research disens achieves greater scalability than even many discrete event simulators on small to medium size cluster nodes disens is able to simulate hundreds of motes in realtime speed and scale to thousands in sub realtime speed to our knowledge disens is the first full system sensor network simulator with such scalability
this paper provides an extensive survey of the different methods of addressing security issues in the grid computing environment and specifically contributes to the research environment by developing comprehensive framework for classification of these research endeavors the framework presented classifies security literature into system solutions behavioral solutions hybrid solutions and related technologies each one of these categories is explained in detail in the paper to provide insight as to their unique methods of accomplishing grid security the types of grid and security situations they apply best to and the pros and cons for each type of solution further several areas of research were identified in the course of the literature survey where more study is warranted these avenues for future research are also discussed in this paper several types of grid systems exist currently and the security needs and solutions to address those needs for each type vary as much as the types of systems themselves this research framework will aid in future research efforts to define analyze and address grid security problems for the many varied types of grid setups as well as the many security situations that each grid may face
in this paper we extend the scope of mining association rules from traditional single dimensional intratransaction associations to multidimensional intertransaction associations intratransaction associations are the associations among items with the same transaction where the notion of the transaction could be the items bought by the same customer the events happened on the same day and so on however an intertransaction association describes the association relationships among different transactions such as ldquo if company a's stock goes up on day b's stock will go down on day but go up on day rdquo in this case whether we treat company or day as the unit of transaction the associated items belong to different transactions moreover such an intertransaction association can be extended to associate multiple contextual properties in the same rule so that multidimensional intertransaction associations can be defined and discovered two dimensional intertransaction association rule example is ldquo after mcdonald and burger king open branches kfc will open branch two months later and one mile away rdquo which involves two dimensions time and space mining intertransaction associations poses more challenges on efficient processing than mining intratransaction associations interestingly intratransaction association can be treated as special case of intertransaction association from both conceptual and algorithmic point of view in this study we introduce the notion of multidimensional intertransaction association rules study their measurements mdash support and confidence mdash and develop algorithms for mining intertransaction associations by extension of apriori we overview our experience using the algorithms on both real life and synthetic data sets further extensions of multidimensional intertransaction association rules and potential applications are also discussed
modern data mining settings involve combination of attribute valued descriptors over entities as well as specified relationships between these entities we present an approach to cluster such non homogeneous datasets by using the relationships to impose either dependent clustering or disparate clustering constraints unlike prior work that views constraints as boolean criteria we present formulation that allows constraints to be satisfied or violated in smooth manner this enables us to achieve dependent clustering and disparate clustering using the same optimization framework by merely maximizing versus minimizing the objective function we present results on both synthetic data as well as several real world datasets
one of the main challenges faced by content based publish subscribe systems is handling large amount of dynamic subscriptions and publications in multidimensional content space to reduce subscription forwarding load and speed up content matching subscription covering subsumption and merging techniques have been proposed in this paper we propose mics multidimensional indexing for content space that provides an efficient representation and processing model for large number of subscriptions and publications mics creates one dimensional representation for publications and subscriptions using hilbert space filling curve based on this representation we propose novel content matching and subscription management covering subsumption and merging algorithms our experimental evaluation indicates that the proposed approach significantly speeds up subscription management operations compared to the naive linear approach
an overall sensornet architecture would help tame the increasingly complex structure of wireless sensornet software and help foster greater interoperability between different codebases previous step in this direction is the sensornet protocol sp unifying link abstraction layer this paper takes the natural next step by proposing modular network layer for sensornets that sits atop sp this modularity eases implementation of new protocols by increasing code reuse and enables co existing protocols to share and reduce code and resources consumed at run time we demonstrate how current protocols can be decomposed into this modular structure and show that the costs in performance and code footprint are minimal relative to their monolithic counterparts
die stacking is an exciting new technology that increases transistor density by vertically integrating two or more die with dense high speed interface the result of die stacking is significant reduction of interconnect both within die and across dies in system for instance blocks within microprocessor can be placed vertically on multiple die to reduce block to block wire distance latency and power disparate si technologies can also be combined in die stack such as dram stacked on cpu resulting in lower power higher bw and lower latency interfaces without concern for technology integration into single process flow has the potential to change processor design constraints by providing substantial power and performance benefits despite the promising advantages of there is significant concern for thermal impact in this research we study the performance advantages and thermal challenges of two forms of die stacking stacking large dram or sram cache on microprocessor and dividing traditional microarchitecture between two die in stack results it is shown that mb stacked dram cache can reduce the cycles per memory access of twothreaded rms benchmark on average by and as much as while increasing the peak temperature by negligible ¬∫c off die bw and power are also reduced by on average it is also shown that floorplan of high performance microprocessor can simultaneously reduce power and increase performance with small ¬∫c increase in peak temperature voltage scaling can reach neutral thermals with simultaneous power reduction and performance improvement
as gml is becoming the de facto standard for geographic data storage transmission and exchange more and more geographic data exists in gml format in applications gml documents are usually very large in size because they contain large number of verbose markup tags and large amount of spatial coordinate data in order to speedup data transmission and reduce network cost it is essential to develop effective and efficient gml compression tools although gml is special case of xml current xml compressors are not effective if directly applied to gml because these compressors have been designed for general xml data in this paper we propose gpress compressor for effectively compressing gml documents to the best of our knowledge gpress is the first compressor specifically for gml documents compression gpress exploits the unique characteristics of gml documents to achieve good performance extensive experiments over real world gml documents show that gpress evidently outperforms xmill one of the best existing xml compressors in compression ratio while its compression efficiency is comparable to the existing xml compressors
aggregation and cube are important operations for online analytical processing olap many efficient algorithms to compute aggregation and cube for relational olap have been developed some work has been done on efficiently computing cube for multidimensional data warehouses that store data sets in multidimensional arrays rather than in tables however to our knowledge there is nothing to date in the literature describing aggregation algorithms on compressed data warehouses for multidimensional olap this paper presents set of aggregation algorithms on compressed data warehouses for multidimensional olap these algorithms operate directly on compressed data sets which are compressed by the mapping complete compression methods without the need to first decompress them the algorithms have different performance behaviors as function of the data set parameters sizes of outputs and main memory availability the algorithms are described and the and cpu cost functions are presented in this paper decision procedure to select the most efficient algorithm for given aggregation request is also proposed the analysis and experimental results show that the algorithms have better performance on sparse data than the previous aggregation algorithms
emerging grid computing infrastructures such as cloud computing can only become viable alternatives for the enterprise if they can provide stable service levels for business processes and sla based costing in this paper we describe and apply three step approach to map sla and qos requirements of business processes to such infrastructures we start with formalization of service capabilities and business process requirements we compare them and if we detect performance or reliability gap we dynamically improve performance of individual services deployed in grid and cloud computing environments here we employ translucent replication of services an experimental evaluation in amazon ec verified our approach
in this paper we present new end to end protocol namely scalable streaming video protocol ssvp which operates on top of udp and is optimized for unicast video streaming applications ssvp employs additive increase multiplicative decrease aimd based congestion control and adapts the sending rate by properly adjusting the inter packet gap ipg the smoothness oriented modulation of aimd parameters and ipg adjustments reduce the magnitude of aimd oscillation and allow for smooth transmission patterns while tcp friendliness is maintained our experimental results demonstrate that ssvp eventually adapts to the vagaries of the network and achieves remarkable performance on real time video delivery in the event where awkward network conditions impair the perceptual video quality we investigate the potential improvement via layered adaptation mechanism that utilizes receiver buffering and adapts video quality along with long term variations in the available bandwidth the adaptation mechanism sends new layer based on explicit criteria that consider both the available bandwidth and the amount of buffering at the receiver preventing wasteful layer changes that have an adverse effect on user perceived quality quantifying the interactions of ssvp with the specific adaptation scheme we identify notable gains in terms of video delivery especially in the presence of limited bandwidth
this work examines how awareness systems class of technologies that support sustained and effortless communication between individuals and groups can support family communication going beyond the evaluation of specific design concepts this paper reports on three studies that aimed to answer the following research questions do families want to be aware of each other through the day or would they perhaps rather not know more about each other's activities and whereabouts than they already do if they do wish to have some awareness what should they be aware of the research involved in depth interviews with participants field trial of an awareness system connecting five busy parents with their children and survey of participants conducted over the web triangulation of the results of the three studies leads to the following conclusions some busy parents want to automatically exchange awareness information during the day while others do not availability of partner for coordinating family activities daily activities in new family situations activity and location information of dependent children are salient awareness information needs for this group awareness information needs to vary with contexts suggesting the need for flexible mechanisms to manage the sharing of such information
we present novel framework based on continuous fluid simulator for general simulation of realistic bubbles with which we can handle as many significant dynamic bubble effects as possible to capture very thin liquid film of bubbles we have developed regional level set method allowing multi manifold interface tracking based on the definitions of regional distance and its five operators the implementation of the regional level set method is very easy an implicit surface of liquid film with arbitrary thickness can be reconstructed from the regional level set function to overcome the numerical instability problem we exploit new semi implicit surface tension model which is unconditionally stable and makes the simulation of surface tension dominated phenomena much more efficient an approximated film thickness evolution model is proposed to control the bubble's lifecycle all these new techniques combine into general framework that can produce various realistic dynamic effects of bubbles
shortest path queries spq are essential in many graph analysis and mining tasks however answering shortest path queries on the fly on large graphs is costly to online answer shortest path queries we may materialize and index shortest paths however straightforward index of all shortest paths in graph of vertices takes space in this paper we tackle the problem of indexing shortest paths and online answering shortest path queries as many large real graphs are shown richly symmetric the central idea of our approach is to use graph symmetry to reduce the index size while retaining the correctness and the efficiency of shortest path query answering technically we develop framework to index large graph at the orbit level instead of the vertex level so that the number of breadth first search trees materialized is reduced from to delta where delta le is the number of orbits in the graph we explore orbit adjacency and local symmetry to obtain compact breadth first search trees compact bfs trees an extensive empirical study using both synthetic data and real data shows that compact bfs trees can be built efficiently and the space cost can be reduced substantially moreover online shortest path query answering can be achieved using compact bfs trees
many large organizations have multiple large databases as they transact from multiple branches most of the previous pieces of work are based on single database thus it is necessary to study data mining on multiple databases in this paper we propose two measures of similarity between pair of databases also we propose an algorithm for clustering set of databases efficiency of the clustering process has been improved using the following strategies reducing execution time of clustering algorithm using more appropriate similarity measure and storing frequent itemsets space efficiently
knowing that two numerical variables always hold different values at some point of program can be very useful especially for analyzing aliases if then and are not aliased and this knowledge is of great help for many other program analyses surprisingly disequalities are seldom considered in abstract interpretation most of the proposed numerical domains being restricted to convex sets in this paper we propose to combine simple ordering properties with disequalities difference bound matrices or dbms is domain proposed by david dill for expressing relations of the form or we define ddbms disequalities dbms as conjunctions of dbms with simple disequalities of the form or we give algorithms on ddbms for deciding the emptiness computing normal form and performing the usual operations of an abstract domain these algorithms have the same complexity where is the number of variables than those for classical dbms if the variables are considered to be valued in dense set or in the arithmetic case the emptiness decision is np complete and other operations run in
as organizations implement information strategies that call for sharing access to resources in the networked environment mechanisms must be provided to protect the resources from adversaries the proposed delegation framework addresses the issue of how to advocate selective information sharing in role based systems while minimizing the risks of unauthorized access we introduce systematic approach to specify delegation and revocation policies using set of rules we demonstrate the feasibility of our framework through policy specification enforcement and proof of concept implementation on specific domains eg the healthcare environment we believe that our work can be applied to organizations that rely heavily on collaborative tasks
as new communications media foster international collaborations we would be remiss in overlooking cultural differences when assessing them in this study pairs in three cultural groupings american american aa chinese chinese cc and american chinese ac worked on two decision making tasks one face to face and the other via im drawing upon prior research we predicted differences in conversational efficiency conversational content interaction quality persuasion and performance the quantitative results combined with conversation analysis suggest that the groups viewed the task differently aa pairs as an exercise in situation specific compromise cc as consensus reaching cultural differences were reduced but not eliminated in the im condition
automatic annotation of medical images is an increasingly important tool for physicians in their daily activity hospitals nowadays produce an increasing amount of data manual annotation is very costly and prone to human mistakes this paper proposes multi cue approach to automatic medical image annotation we represent images using global and local features these cues are then combined using three alternative approaches all based on the support vector machine algorithm we tested our methods on the irma database and with two of the three approaches proposed here we participated in the imageclefmed benchmark evaluation in the medical image annotation track these algorithms ranked first and fifth respectively among all submission experiments using the third approach also confirm the power of cue integration for this task
we evaluate various heuristics for hierarchical spectral clustering in large telephone call and web graphs spectral clustering without additional heuristics often produces very uneven cluster sizes or low quality clusters that may consist of several disconnected components fact that appears to be common for several data sources but to our knowledge no general solution provided so far divide and merge recently described postfiltering procedure may be used to eliminate bad quality branches in binary tree hierarchy we propose an alternate solution that enables way cuts in each step by immediately filtering unbalanced or low quality clusters before splitting them furtherour experiments are performed on graphs with various weight and normalization built based on call detail records and web crawls we measure clustering quality both by modularity as well as by the geographic and topical homogeneity of the clusters compared to divide and merge we give more homogeneous clusters with more desirable distribution of the cluster sizes
enforcing strong replica consistency among set of replicas of service deployed across an asynchronous distributed system in the presence of crash failures is real practical challenge if each replica runs the consistency protocol bundled with the actual service implementation this target cannot be achieved as replicas need to be located over partially synchronous distributed system to solve the distributed agreement problems underlying strong replica consistencya three tier architecture for software replication enables the separation of the replication logic ie protocols and mechanisms necessary for managing software replication from both clients and server replicas the replication logic is embedded in middle tier that confines the need of partial synchrony and thus frees replica deploymentin this paper we first introduce the basic concepts underlying three tier replication then we present the interoperable replication logic irl architecture fault tolerant corba compliant infrastructure irl exploits three tier approach to replicate stateful deterministic corba objects and allows object replicas to run on object request brokers from different vendors description of an irl prototype developed in our department is proposed along with an extensive performance analysis
this paper presents cognitive vision system capable of autonomously learning protocols from perceptual observations of dynamic scenes the work is motivated by the aim of creating synthetic agent that can observe scene containing interactions between unknown objects and agents and learn models of these sufficient to act in accordance with the implicit protocols present in the scene discrete concepts utterances and object properties and temporal protocols involving these concepts are learned in an unsupervised manner from continuous sensor input alone crucial to this learning process are methods for spatio temporal attention applied to the audio and visual sensor data these identify subsets of the sensor data relating to discrete concepts clustering within continuous feature spaces is used to learn object property and utterance models from processed sensor data forming symbolic description the progol inductive logic programming system is subsequently used to learn symbolic models of the temporal protocols presented in the presence of noise and over representation in the symbolic data input to it the models learned are used to drive synthetic agent that can interact with the world in semi natural way the system has been evaluated in the domain of table top game playing and has been shown to be successful at learning protocol behaviours in such real world audio visual environments
the modeling analysis and design of systems is generally based on many formalisms to describe discrete and or continuous behaviors and to map these descriptions into specific platform in this context the article proposes the concept of functional metamodeling to capture then to integrate modeling languages the concept offers an alternative to standard model driven engineering mde and is well adapted to mathematical descriptions such as the ones found in system modeling as an application set of functional metamodels is proposed for dataflows usable to model continuous behaviors state transition systems usable to model discrete behaviors and metamodel for actions to model interactions with target platform and concurrent execution model of control architecture for legged robot is proposed as an application of these modeling languages
we propose simple obstacle model to be used while simulating wireless sensor networks to the best of our knowledge this is the first time such an integrated and systematic obstacle model for these networks has been proposed we define several types of obstacles that can be found inside the deployment area of wireless sensor network and provide categorization of these obstacles based on their nature physical and communication obstacles ie obstacles that are formed out of node distribution patterns or have physical presence respectively their shape and their change of nature over time we make an extension to custom made sensor network simulator simdust and conduct number of simulations in order to study the effect of obstacles on the performance of some representative in terms of their logic data propagation protocols for wireless sensor networks our findings confirm that obstacle presence has significant impact on protocol performance and also that different obstacle shapes and sizes may affect each protocol in different ways this provides an insight into how routing protocol will perform in the presence of obstacles and highlights possible protocol shortcomings moreover our results show that the effect of obstacles is not directly related to the density of sensor network and cannot be emulated only by changing the network density
technological achievements have made it possible to fabricate cmos circuits with over billion transistors implement boolean operations using quantum devices and or the spin of an electron implement transformations using bio and molecular based cells problems with many of these technologies are due to such factors as process variations defects and impurities in materials and solutions and noise consequently many systems built from these technologies operate imperfectly luckily there are many complex and large market systems applications that tolerate acceptable though not always correct results in addition there is emerging body of mathematical analysis related to imperfect computation in this paper we first introduce the concepts of acceptable error tolerance and acceptable performance degradation and demonstrate how important attributes of these concepts can be quantified we interlace this discussion with several examples of systems that can effectively employ these two concepts next we mention several immerging technologies that motivate the need to study these concepts as well as related mathematical paradigms finally we will list few cad issues that are needed to support this new form of technological revolution
in mainstream oo languages inheritance can be used to add new methods or to override existing methods virtual classes and feature oriented programming are techniques which extend the mechanism of inheritance so that it is possible to refine nested classes as well these techniques are attractive for programming in the large because inheritance becomes tool for manipulating whole class hierarchies rather than individual classes nevertheless it has proved difficult to design static type systems for virtual classes because virtual classes introduce dependent types the compile time type of an expression may depend on the run time values of objects in that expressionwe present formal object calculus which implements virtual classes in type safe manner our type system uses novel technique based on prototypes which blur the distinction between compile time and run time at run time prototypes act as objects and they can be used in ordinary computations at compile time they act as types prototypes are similar in power to dependent types and subtyping is shown to be form of partial evaluation we prove that prototypes are type safe but undecidable and briefly outline decidable semi algorithm for dealing with them
distributed sparing is method to improve the performance of raid disk arrays with respect to dedicated sparing system with disks including the spare disk since it utilizes the bandwidth of all disks we analyze the performance of raid with distributed sparing in normal mode degraded mode and rebuild mode in an oltp environment which implies small reads and writes the analysis in normal mode uses an queuing model which takes into account the components of disk service time in degraded mode low cost approximate method is developed to estimate the mean response time of fork join requests resulting from accesses to recreate lost data on the failed disk rebuild mode performance is analyzed by considering an vacationing server model with multiple vacations of different types to take into account differences in processing requirements for reading the first and subsequent tracks an iterative solution method is used to estimate the mean response time of disk requests as well as the time to read each disk which is shown to be quite accurate through validation against simulation results we next compare raid performance in system without cache with cache and with nonvolatile storage nvs cache the last configuration in addition to improved read response time due to cache hits provides fast write capability such that dirty blocks can be destaged asynchronously and at lower priority than read requests resulting in an improvement in read response time the small write penalty is also reduced due to the possibility of repeated writes to dirty blocks in the cache and by taking advantage of disk geometry to efficiently destage multiple blocks at time
this work investigates the problem of privacy preserving mining of frequent itemsets we propose procedure to protect the privacy of data by adding noisy items to each transaction then an algorithm is proposed to reconstruct frequent itemsets from these noise added transactions the experimental results indicate that this method can achieve rather high level of accuracy our method utilizes existing algorithms for frequent itemset mining and thereby takes full advantage of their progress to mine frequent itemset efficiently
in mobile computing environments as result of the reduced capacity of local storage it is commonly not feasible to replicate entire datasets on each mobile unit in addition reliable secure and economical access to central servers is not always possible moreover since mobile computers are designed to be portable they are also physically small and thus often unable to hold or process the large amounts of data held in centralised databases as many systems are only as useful as the data they can process the support provided by database and system management middleware for applications in mobile environments is an important driver for the uptake of this technology by application providers and thus also for the wider use of the technologyone of the approaches to maximize the available storage is through the use of database summarisation to date most strategies for reducing data volumes have used compression techniques that ignore the semantics of the data those that do not use data compression techniques adopt structural ie data and use independent methods in this paper we outline the special constraints imposed on storing information in mobile databases and provide flexible data summarisation policy the method works by assigning level of priority to each data item through the setting of number of parameters the paper discusses some policies for setting these parameters and some implementation strategies
with concurrent and garbage collected languages like java and becoming popular the need for suitable non intrusive efficient and concurrent multiprocessor garbage collector has become acute we propose novel mark and sweep on the fly algorithm based on the sliding views mechanism of levanoni and petrank we have implemented our collector on the jikes java virtual machine running on netfinity multiprocessor and compared it to the concurrent algorithm and to the stop the world collector supplied with jikes jvm the maximum pause time that we measured with our benchmarks over all runs was ms in all runs the pause times were smaller than those of the stop the world collector by two orders of magnitude and they were also always shorter than the pauses of the jikes concurrent collector throughput measurements of the new garbage collector show that it outperforms the jikes concurrent collector by up to as expected the stop the world does better than the on the fly collectors with results showing about differenceon top of being an effective mark and sweep on the fly collector standing on its own our collector may also be used as backup collector collecting cyclic data structures for the levanoni petrank reference counting collector these two algorithms perfectly fit sharing the same allocator similar data structure and similar jvm interface
dynamic energy performance scaling deps framework is proposed to save energy in fixed priority hard real time embedded systems in this generalized framework two existing technologies ie dynamic hardware resource configuration dhrc and dynamic voltage frequency scaling dvfs can be combined for energy performance tradeoff the problem of selecting the optimal hardware configuration and voltage frequency parameters is formulated to achieve maximal energy savings and meet the deadline constraint simultaneously through case study the effectiveness of deps has been validated
phrase based statistical machine translation approach mdash the alignment template approach mdash is described this translation approach allows for general many to many relations between words thereby the context of words is taken into account in the translation model and local changes in word order from source to target language can be learned explicitly the model is described using log linear modeling approach which is generalization of the often used source ndash channel approach thereby the model is easier to extend than classical statistical machine translation systems we describe in detail the process for learning phrasal translations the feature functions used and the search algorithm the evaluation of this approach is performed on three different tasks for the german ndash english speech verbmobil task we analyze the effect of various system components on the french ndash english canadian hansards task the alignment template system obtains significantly better results than single word based translation model in the chinese ndash english national institute of standards and technology nist machine translation evaluation it yields statistically significantly better nist scores than all competing research and commercial translation systems
we introduce the concept of administrative scope in role hierarchy and demonstrate that it can be used as basis for role based administration we then develop family of models for role hierarchy administration rha employing administrative scope as the central concept we then extend rha the most complex model in the family to complete decentralized model for role based administration we show that sarbac the resulting role based administrative model has significant practical and theoretical advantages over arbac we also discuss how administrative scope might be applied to the administration of general hierarchical structures how our model can be used to reduce inheritance in the role hierarchy and how it can be configured to support discretionary access control features
broadcast data dissemination is well suited for mobile wireless environments where bandwidth is scarce and mutual interference must be minimised however broadcasting monopolises the medium precluding clients from performing any other communication we address this problem in two ways firstly we segment the server broadcast with intervening periods of silence during which the wireless devices may communicate secondly we reduce the average access delay for clients using novel cooperative caching scheme our scheme is fully decentralised and uses information available locally at the client our results show that our model prevents the server from monopolising the medium and that our caching strategy reduces client access delays significantly
recently planning based on answer set programming has been proposed as an approach towards realizing declarative planning systems in this paper we present the language Œ∫c which extends the declarative planning language by action costs Œ∫c provides the notion of admissible and optimal plans which are plans whose overall action costs are within given limit resp minimum over all plans ie cheapest plans as we demonstrate this novel language allows for expressing some nontrivial planning tasks in declarative way furthermore it can be utilized for representing planning problems under other optimality criteria such as computing shortest plans with the least number of steps and refinement combinations of cheapest and fastest plans we study complexity aspects of the language Œ∫c and provide transformation to logic programs such that planning problems are solved via answer set programming furthermore we report experimental results on selected problems our experience is encouraging that answer set planning may be valuable approach to expressive planning systems in which intricate planning problems can be naturally specified and solved
in this contribution we present new paradigm and methodology for the network on chip noc based design of complex hardware software systems while classical industrial design platforms represent dedicated fixed architectures for specific applications flexible noc architectures open new degrees of system reconfigurability after giving an overview on required demands for noc hyper platforms we describe the realisation of these prerequisites within the hinoc platform we introduce new dynamic hardware software co design methodology for pre and post manufacturing design finally we will summarize the concept combined with an outlook on further investigations
mobile services operate on hosts with diverse capabilities in heterogeneous networks where the usage of resources such as processor memory and network is constantly changing in order to maintain efficiency in terms of performance and resource utilization such services should be able to adapt to changes in their environmentthis paper proposes and empirically evaluates an application transparent adaptation strategy for service oriented systems the strategy is based upon the solution of an optimization model derived from an existing suite of metrics for services which maps system services to network nodesthe strategy is evaluated empirically using number of distinct scenarios involving runtime changes in processor memory and network utilization in order to maintain execution efficiency in response to these changing operating conditions the strategy rearranges the service topology of the system dynamically by moving services between network nodes the results show that the negative impact of environmental changes on runtime efficiency can be reduced after adaptation from to depending on the selected parameters
we present an optimization framework for exploring gradient domain solutions for image and video processing the proposed framework unifies many of the key ideas in the gradient domain literature under single optimization formulation our hope is that this generalized framework will allow the reader to quickly gain general understanding of the field and contribute new ideas of their own we propose novel metric for measuring local gradient saliency that identifies salient gradients that give rise to long coherent edges even when the individual gradients are faint we present general weighting scheme for gradient constraints that improves the visual appearance of results we also provide solution for applying gradient domain filters to videos and video streams in coherent manner finally we demonstrate the utility of our formulation in creating effective yet simple to implement solutions for various image processing tasks to exercise our formulation we have created new saliency based sharpen filter and pseudo image relighting application we also revisit and improve upon previously defined filters such as nonphotorealistic rendering image deblocking and sparse data interpolation over images eg colorization using optimization
upcoming multi media compression applications will require high memory bandwidth in this paper we estimate that software reference implementation of an mpeg video decoder typically requires mtransfers to memory to decode cif times video object plane vop at frames this imposes high penalty in terms of power but also performancehowever we also show that we can heavily improve on the memory transfers without sacrificing speed even gaining about on cache misses and cycles for dec alpha by aggressive code transformations for this purpose we have manually applied an extended version of our data transfer and storage exploration dtse methodology which was originally developed for custom hardware implementations
although hierarchical pp systems have been found to outperform flat systems in many respects current pp research does not focus on strategies to build and maintain such systems available solutions assume either no or little coordination between peers that could lead the system toward satisfying globally defined goal eg minimizing traffic in this paper we focus on hierarchical dhts and provide full set of algorithms to build and maintain such systems that mitigate this problem in particular given the goal state of minimizing the total traffic without overloading any peer our algorithms dynamically adjust the system state as to keep the goal met at any time the algorithms are fully decentralized and probabilistic all decisions taken by the peers are based on their partial view on set of system wide parameters thus they demonstrate the main principle of self organization the system behavior emerges from local interactions our simulations run in range of realistic settings confirm good performance of the algorithms
programmers have traditionally used locks to synchronize concurrent access to shared data lock based synchronization however has well known pitfalls using locks for fine grain synchronization and composing code that already uses locks are both difficult and prone to deadlock transactional memory provides an alternate concurrency control mechanism that avoids these pitfalls and significantly eases concurrent programming transactional memory language constructs have recently been proposed as extensions to existing languages or included in new concurrent language specifications opening the door for new compiler optimizations that target the overheads of transactional memorythis paper presents compiler and runtime optimizations for transactional memory language constructs we present high performance software transactional memory system stm integrated into managed runtime environment our system efficiently implements nested transactions that support both composition of transactions and partial roll back our jit compiler is the first to optimize the overheads of stm and we show novel techniques for enabling jit optimizations on stm operations we measure the performance of our optimizations on way smp running multi threaded transactional workloads our results show that these techniques enable transactional memory's performance to compete with that of well tuned synchronization
this paper proposes neural network based approach for solving the resource discovery problem in peer to peer pp networks and an adaptive global local memetic algorithm aglma for performing the training of the neural network this training is very challenging due to the large number of weights and noise caused by the dynamic neural network testing the aglma is memetic algorithm consisting of an evolutionary framework which adaptively employs two local searchers having different exploration logic and pivot rules furthermore the aglma makes an adaptive noise compensation by means of explicit averaging on the fitness values and dynamic population sizing which aims to follow the necessity of the optimization process the numerical results demonstrate that the proposed computational intelligence approach leads to an efficient resource discovery strategy and that the aglma outperforms two classical resource discovery strategies as well as popular neural network training algorithm
this paper introduces the prophet critic hybrid conditionalbranch predictor which has two component predictorsthat play the role of either prophet or critictheprophet is conventional predictor that uses branch historyto predict the direction of the current branchfurther accessesof the prophet yield predictions for the branches followingthe current onepredictions for the current branchand the ones that follow are collectively known as thebranch's futurethey are actually prophecy or predictedbranch futurethe critic uses both the branch's history andfuture to give critique of the prophet's prediction fo thecurrent branchthe critique either agree or disagree isused to generate the final prediction for the branchour results show an byte prophet critic hybridhas fewer mispredicts than byte bc gskewpredictor predictor similar to that of the proposed compaq alpha ev processor across wide range of applicationsthe distance between pipeline flushes due to mispredictsincreases from one flush per micro operations uops to one per uopsfor gcc the percentage of mispredictedbranches drops from to on machinebased on the intel pentium processor this improvesupc uops per cycle by for gcc andreduces the number of uops fetched along both correct andincorrect paths by
in this paper we propose new dynamic and efficient bounding volume hierarchy for breakable objects undergoing structured and or unstructured motion our object space method is based on different ways to incrementally update the hierarchy during simulation by exploiting temporal coherence and lazy evaluation techniques this leads to significant advantages in terms of execution speed furthermore we also show how our method lends itself naturally for an adaptive low memory cost implementation which may be of critical importance in some applications finally we propose two different techniques for detecting self intersections one using our hierarchical data structure and the other is an improved sorting based method
in this article we present an experimental study of the properties of webgraphs we study large crawl from of pages and about billion edges made available by the webbase project at stanford as well as several synthetic ones generated according to various models proposed recently we investigate several topological properties of such graphs including the number of bipartite cores and strongly connected components the distribution of degrees and pagerank values and some correlations we present comparison study of the models against these measuresour findings are that the webbase sample differs slightly from the older samples studied in the literature and ii despite the fact that these models do not catch all of its properties they do exhibit some peculiar behaviors not found for example in the models from classical random graph theorymoreover we developed software library able to generate and measure massive graphs in secondary memory this library is publicy available under the gpl licence we discuss its implementation and some computational issues related to secondary memory graph algorithms
process algebraic techniques for distributed systems are increasingly being targeted at identifying abstractions that are adequate for both high level programming and specification and security analysis and verification drawing on our earlier work in bugliesi and focardi we investigate the expressive power of core set of security and network abstractions that provide high level primitives for specifying the honest principals in network while at the same time enabling an analysis of the network level adversarial attacks that may be mounted by an intruder we analyse various bisimulation equivalences for security that arise from endowing the intruder with label label different adversarial capabilities and label ii label increasingly powerful control over the interaction among the distributed principals of network by comparing the relative strength of the bisimulation equivalences we obtain direct measure of the intruder's discriminating power and hence of the expressiveness of the corresponding intruder model
we present aspect oriented programming in jiazzi jiazzi enhances java with separately compiled externally linked code modules called units units can act as effective aspect constructs with the ability to separate crosscutting concern code in non invasive and safe way unit linking provides convenient way for programmers to explicitly control the inclusion and configuration of code that implements concern while separate compilation of units enhances the independent development and deployment of the concern the expressiveness of concern separation is enhanced by units in two ways first classes can be made open to the addition of new behavior fields and methods after they are initially defined which enables the direct modularization of concerns whose code crosscut object boundaries second the signatures of methods and classes can also be made open to refinement which permits more aggressive modularization by isolating the naming and calling requirements of concern implementation
end user programming has become ubiquitous so much so that there are more end user programmers today than there are professional programmers end user programming empowers but to do what make really bad decisions based on really bad programs enter software engineering's focus on quality considering software quality is necessary because there is ample evidence that the programs end users create are filled with expensive errors in this paper consider what happens when we add to end user programming environments considerations of software quality going beyond the create program aspect of end user programming describe philosophy to software engineering for end users and then survey several projects in this area basic premise is that end user software engineering can only succeed to the extent that it respects the fact that the user probably has little expertise or even interest in software engineering
motivated by the optimality of shortest remaining processing time srpt for mean response time in recent years many computer systems have used the heuristic of favoring small jobs in order to dramatically reduce user response times however rarely do computer systems have knowledge of exact remaining sizes in this paper we introduce the class of smart policies which formalizes the heuristic of favoring small jobs in way that includes wide range of policies that schedule using inexact job size information examples of smart policies include policies that use exact size information eg srpt and psjf ii policies that use job size estimates and iii policies that use finite number of size based priority levels for many smart policies eg srpt with inexact job size information there are no analytic results available in the literature in this work we prove four main results we derive upper and lower bounds on the mean response time the mean slowdown the response time tail and the conditional response time of smart policies in each case the results explicitly characterize the tradeoff between the accuracy of the job size information used to prioritize and the performance of the resulting policy thus the results provide designers insight into how accurate job size information must be in order to achieve desired performance guarantees
in this paper we show how to augment object oriented application interfaces with enhanced specifications that include sequencing constraints called protocols protocols make explicit the relationship between messages methods supported by the application these relationships are usually only given implicitly either in the code or in textual comments we define notions of interface compatibility based upon protocols and show how compatibility can be checked discovering class of errors that cannot be discovered via the type system alone we then define software adaptors that can be used to bridge the difference between object oriented applications that have functionally compatible but type incompatible interfaces we discuss what it means for an adaptor to be well formed leveraging the information provided by protocols we show how adaptors can be automatically generated from high level description called an interface mapping
location based and personalized services are the key factors for promoting user satisfaction however most service providers did not consider the needs of mobile user in terms of their location and event participation consequently the service provider may lose the chance for better service and profit in this paper we present multi stage collaborative filtering mscf process to provide event recommendation based on mobile user's location to achieve this purpose the collaborative filtering cf technique is employed and the adaptive resonance theory art network is applied to cluster mobile users according to their personal profile sequential pattern mining is then used to discover the correlations between events for recommendation the mscf is designed not only to recommend for the old registered mobile user ormu but also to handle the cold start problem for new registered mobile user nrmu this research is designed to achieve the followings to present personalized event recommendation system for mobile users to discover mobile users moving patterns to provide recommendations based on mobile users preferences to overcome the cold start problem for new registered mobile user the experimental results of this research show that the mscf is able to accomplish the above purposes and shows better outcome for cold start problem when comparing with user based cf and item based cf
in this research we aim to identify factors that significantly affect the clickthrough of web searchers our underlying goal is determine more efficient methods to optimize the clickthrough rate we devise clickthrough metric for measuring customer satisfaction of search engine results using the number of links visited number of queries user submits and rank of clicked links we use neural network to detect the significant influence of searching characteristics on future user clickthrough our results show that high occurrences of query reformulation lengthy searching duration longer query length and the higher ranking of prior clicked links correlate positively with future clickthrough we provide recommendations for leveraging these findings for improving the performance of search engine retrieval and result ranking along with implications for search engine marketing copy wiley periodicals inc
various code certification systems allow the certification and static verification of important safety properties such as memory and control flow safety these systems are valuable tools for verifying that untrusted and potentially malicious code is safe before execution however one important safety property that is not usually included is that programs adhere to specific bounds on resource consumption such as running time we present decidable type system capable of specifying and certifying bounds on resource consumption our system makes two advances over previous resource bound certification systems both of which are necessary for practical system we allow the execution time of programs and their subroutines to vary depending on their arguments and we provide fully automatic compiler generating certified executables from source level programs the principal device in our approach is strategy for simulating dependent types using sum and inductive kinds
assessing mobility in thorough fashion is crucial step toward more efficient mobile network design recent research on mobility has focused on two main points analyzing models and studying their impact on data transport these works investigate the consequences of mobility in this paper instead we focus on the causes of mobility starting from established research in sociology we propose simps mobility model of human crowds with pedestrian motion this model defines process called sociostation rendered by two complimentary behaviors namely socialize and isolate that regulate an individual with regard to her his own sociability level simps leads to results that agree with scaling laws observed both in small scale and large scale human motion although our model defines only two simple individual behaviors we observe many emerging collective behaviors group formation splitting path formation and evolution
data domain description techniques aim at deriving concise descriptions of objects belonging to category of interest for instance the support vector domain description svdd learns hypersphere enclosing the bulk of provided unlabeled data such that points lying outside of the ball are considered anomalous however relevant information such as expert and background knowledge remain unused in the unsupervised setting in this paper we rephrase data domain description as semi supervised learning task that is we propose semi supervised generalization of data domain description sssvdd to process unlabeled and labeled examples the corresponding optimization problem is non convex we translate it into an unconstraint continuous problem that can be optimized accurately by gradient based techniques furthermore we devise an effective active learning strategy to query low confidence observations our empirical evaluation on network intrusion detection and object recognition tasks shows that our sssvdds consistently outperform baseline methods in relevant learning settings
hair simulation remains one of the most challenging aspects of creating virtual characters most research focuses on handling the massive geometric complexity of hundreds of thousands of interacting hairs this is accomplished either by using brute force simulation or by reducing degrees of freedom with guide hairs this paper presents hybrid eulerian lagrangian approach to handling both self and body collisions with hair efficiently while still maintaining detail bulk interactions and hair volume preservation is handled efficiently and effectively with flip based fluid solver while intricate hair hair interaction is handled with lagrangian self collisions thus the method has the efficiency of continuum guide based hair models with the high detail of lagrangian self collision approaches
we present an adaptive work stealing thread scheduler steal for fork join multithreaded jobs like those written using the cilk multithreaded language or the hood work stealing library the steal algorithm is appropriate for large parallel servers where many jobs share common multiprocessor resource and in which the number of processors available to particular job may vary during the job's execution steal provides continual parallelism feedback to job scheduler in the form of processor requests and the job must adaptits execution to the processors allotted to it assuming that the job scheduler never allots any job more processors than requested by thejob's thread scheduler steal guarantees that the job completes in near optimal time while utilizing at least constant fraction of the allotted processors our analysis models the job scheduler as the thread scheduler's adversary challenging the thread scheduler to be robust to the system environment and the job scheduler's administrative policies we analyze the performance of steal using trim analysis which allows us to prove that our thread scheduler performs poorly on at most small number of time steps while exhibiting near optimal behavior on the vast majority to be precise suppose that job has work and span critical path length on machine with processors steal completes the job in expected lg time steps where is the length of scheduling quantum and denotes the lg trimmed availability this quantity is the average of the processor availability over all but the lg time steps having the highest processor availability when the job's parallelism dominates the trimmed availability that is the job achieves nearly perfect linear speedup conversely when the trimmed mean dominates the parallelism the asymptotic running time of the job is nearly its span
mobile storage devices such as usb flash drives offer flexible solution for the transport and exchange of data nevertheless in order to prevent unauthorized access to sensitive data many enterprises require strict security policies for the use of such devices with the effect of rendering their advantages rather unfruitful trusted virtual domains tvds provide secure it infrastructure offering homogeneous and transparent enforcement of access control policies on data and network resources however the current model does not specifically deal with mobile storage devices in this paper we present an extension of the tvd architecture to incorporate the usage of mobile storage devices our proposal addresses three major issues coherent extension of tvd policy enforcement by introducing architectural components that feature identification and management of transitory devices transparent mandatory encryption of sensitive data stored on mobile devices and highly dynamic centralized key management service in particular we address offline scenarios allowing users to access and modify data while being temporarily disconnected from the domain we also present prototype implementation based on the turaya security kernel
we consider the problem of clustering web image search results generally the image search results returned by an image search engine contain multiple topics organizing the results into different semantic clusters facilitates users browsing in this paper we propose hierarchical clustering method using visual textual and link analysis by using vision based page segmentation algorithm web page is partitioned into blocks and the textual and link information of an image can be accurately extracted from the block containing that image by using block level link analysis techniques an image graph can be constructed we then apply spectral techniques to find euclidean embedding of the images which respects the graph structure thus for each image we have three kinds of representations ie visual feature based representation textual feature based representation and graph based representation using spectral clustering techniques we can cluster the search results into different semantic clusters an image search example illustrates the potential of these techniques
several supervised learning algorithms are suited to classify instances into multiclass value space multinomial logit mnl is recognized as robust classifier and is commonly applied within the crm customer relationship management domain unfortunately to date it is unable to handle huge feature spaces typical of crm applications hence the analyst is forced to immerse himself into feature selection surprisingly in sharp contrast with binary logit current software packages lack any feature selection algorithm for multinomial logit conversely random forests another algorithm learning multiclass problems is just like mnl robust but unlike mnl it easily handles high dimensional feature spaces this paper investigates the potential of applying the random forests principles to the mnl framework we propose the random multinomial logit rmnl ie random forest of mnls and compare its predictive performance to that of mnl with expert feature selection random forests of classification trees we illustrate the random multinomial logit on cross sell crm problem within the home appliances industry the results indicate substantial increase in model accuracy of the rmnl model to that of the mnl model with expert feature selection
dynamic information flow tracking dift is an important tool for detecting common security attacks and memory bugs dift tool tracks the flow of information through monitored program's registers and memory locations as the program executes detecting and containing fixing problems on the fly unfortunately sequential dift tools are quite slow and dift is quite challenging to parallelize in this paper we present new approach to parallelizing dift like functionality extending our recent work on accelerating sequential dift we consider variant of dift that tracks the information flow only through unary operations relaxed dift and yet makes sense for detecting security attacks and memory bugs we present parallel algorithm for relaxed dift based on symbolic inheritance tracking which achieves linear speed up asymptotically moreover we describe techniques for reducing the constant factors so that speed ups can be obtained even with just few processors we implemented the algorithm in the context of log based architectures lba system which provides hardware support for logging program trace and delivering it to other monitoring processors our simulation results on spec benchmarks and video player show that our parallel relaxed dift reduces the overhead to as low as using monitoring cores on core chip multiprocessor
this article presents resolution matched shadow maps rmsm modified adaptive shadow map asm algorithm that is practical for interactive rendering of dynamic scenes adaptive shadow maps which build quadtree of shadow samples to match the projected resolution of each shadow texel in eye space offer robust solution to projective and perspective aliasing in shadow maps however their use for interactive dynamic scenes is plagued by an expensive iterative edge finding algorithm that takes highly variable amount of time per frame and is not guaranteed to converge to correct solution this article introduces simplified algorithm that is up to ten times faster than asms has more predictable performance and delivers more accurate shadows our main contribution is the observation that it is more efficient to forgo the iterative refinement analysis in favor of generating all shadow texels requested by the pixels in the eye space image the practicality of this approach is based on the insight that for surfaces continuously visible from the eye adjacent eye space pixels map to adjacent shadow texels in quadtree shadow space this means that the number of contiguous regions of shadow texels which can be efficiently generated with rasterizer is proportional to the number of continuously visible surfaces in the scene moreover these regions can be coalesced to further reduce the number of render passes required to shadow an image the secondary contribution of this paper is demonstrating the design and use of data parallel algorithms inseparably mixed with traditional graphics programming to implement novel interactive rendering algorithm for the scenes described in this paper we achieve frames per second on static scenes and frames per second on dynamic scenes for and images with maximum effective shadow resolution of texels
escape analysis is static analysis that determines whether the lifetime of data may exceed its static scopethis paper first presents the design and correctness proof of an escape analysis for javatm this analysis is interprocedural context sensitive and as flow sensitive as the static single assignment form so assignments to object fields are analyzed in flow insensitive manner since java is an imperative language the effect of assignments must be precisely determined this goal is achieved thanks to our technique using two interdependent analyses one forward one backward we introduce new method to prove the correctness of this analysis using aliases as an intermediate step we use integers to represent the escaping parts of values which leads to fast and precise analysisour implementation blanchet which applies to the whole java language is then presented escape analysis is applied to stack allocation and synchronization elimination in our benchmarks we stack allocate percnt to percnt of data eliminate more than percnt of synchronizations on most programs percnt and percnt on two examples and get up to percnt runtime decrease percnt on average our detailed experimental study on large programs shows that the improvement comes more from the decrease of the garbage collection and allocation times than from improvements on data locality contrary to what happened for ml this comes from the difference in the garbage collectors
regular path queries are way of declaratively expressing queries on graphs as regular expression like patterns that are matched against paths in the graph there are two kinds of queries existential queries which specify properties about individual paths and universal queries which specify properties about all paths they provide simple and convenient framework for expressing program analyses as queries on graph representations of programs for expressing verification model checking problems as queries on transition systems for querying semi structured data etc parametric regular path queries extend the patterns with variables called parameters which significantly increase the expressiveness by allowing additional information along single or multiple paths to be captured and relatethis paper shows how variety of program analysis and model checking problems can be expressed easily and succinctly using parametric regular path queries the paper describes the specification design analysis and implementation of algorithms and data structures for efficiently solving existential and universal parametric regular path queries major contributions include the first complete algorithms and data structures for directly and efficiently solving existential and universal parametric regular path queries detailed complexity analysis of the algorithms detailed analytical and experimental performance comparison of variations of the algorithms and data structures and investigation of efficiency tradeoffs between different formulations of queries
mobile agent technology has emerged as promising programming paradigm for developing highly dynamic and large scale service oriented computing middlewares due to its desirable features for this purpose first of all scalable location transparent agent communication issue should be addressed in mobile agent systems despite agent mobility although there were proposed several directory service and message delivery mechanisms their disadvantages force them not to be appropriate to both low overhead location management and fast delivery of messages to agents migrating frequently to mitigate their limitations this paper presents scalable distributed directory service and message delivery mechanism the proposed mechanism enables each mobile agent to autonomously leave tails of forwarding pointers on some few of its visiting nodes depending on its preferences this feature results in low message forwarding overhead and low storage and maintenance cost of increasing chains of pointers per host also keeping mobile agent location information in the effective binding cache of each sending agent the sending agent can communicate with mobile agents much faster compared with the existing ones
idle resources can be exploited not only to run important local tasks such as data replication and virus checking but also to make contributions to society by participating in open computing projects like seti home when executing background processes to utilize such valuable idle resources we need to explicitly control them so that the user will not be discouraged from exploiting idle resources by foreground performance degradation unfortunately common priority based schedulers lack such explicit execution control in addition to encourage active use of idle resources mechanism for controlling background processes should not require modifications to the underlying operating system or user applications if such modifications are required the user may be reluctant to employ the mechanism in this paper we argue that we can reasonably detect resource contention between foreground and background processes and properly control background process execution at the user level we infer the existence of resource contention from the approximated resource shares of background processes our approach takes advantage of dynamically instrumented probes which are becoming increasingly popular in estimating the resource shares also it considers different resource types in combination and can handle varied workloads including multiple background processes we show that our system effectively avoids the performance degradation of foreground activities by suspending background processes in an appropriate fashion our system keeps the increase in foreground execution time due to background processes below or much lower in most of our experiments also we extend our approach to address undesirable resource allocations to cpu intensive processes that can occur in multiprocessor environments
we practices lack an impact on industry partly due to we field that is not quality aware in fact it is difficult to find we methodologies that pay explicit attention to quality aspects however the use of systematic process that includes quality concerns from the earliest stages of development can contribute to easing the building up of quality guaranteed web applications without drastically increasing development costs and time to market in this kind of process quality issues should be taken into account while developing each outgoing artifact from the requirements model to the final application also quality models should be defined to evaluate the quality of intermediate we artifacts and how it contributes to improving the quality of the deployed application in order to tackle its construction while avoiding some of the most common problems that existing quality models suffer from in this paper we propose number of we quality models to address the idiosyncrasies of the different stakeholders and we software artifacts involved additionally we propose that these we quality models are supported by an ontology based we measurement meta model that provides set of concepts with clear semantics and relationships this we quality metamodel is one of the main contributions of this paper furthermore we provide an example that illustrates how such metamodel may drive the definition of particular we quality model
in this work we present rendering method with guaranteed interactive frame rates in complex scenes the algorithm is based on an new data structure determined in preprocessing to avoid frozen displays in large simulative visualizations like industrial plants typically described as cad models within preprocessing polygons are grouped by size and within these groups core clusters are calculated based on similarity and locality the clusters and polygons are building up hierarchy including weights ascertained within repetitive stages of re grouping and re clustering this additional information allows to choose subset over all primitives to reduce scene complexity depending on the viewer's position sight and the determined weights within the hierarchy to guarantee specific frame rate the number of rendered primitives is limited by constant and typically constrained by hardware this reduction is controlled by the pre calculated weights and the viewer's position and is not done arbitrarily at least the rendered section is suitable scene approximation that includes the viewer's interests combining all this constant frame rate including million polygons at fps is obtainable practical results indicate that our approach leads to good scene approximations and realtime rendering of very large environments at the same time
because of the high volume and unpredictable arrival rate stream processing systems may not always be able to keep up with the input data streams resulting in buffer overflow and uncontrolled loss of data load shedding the prevalent strategy for solving this overflow problem has so far only been considered for relational stream processing but not for xml shedding applied to xml stream processing brings new opportunities and challenges due to complex nested nature of xml structures in this paper we tackle this unsolved xml shedding problem using three pronged approach first we develop an xquery preference model that enables users to specify the relative importance of preserving different subpatterns in the xml result structure this transforms shedding into the problem of rewriting the user query into shed queries that return approximate query answers with utility as measured by the given user preference model second we develop cost model to compare the performance of alternate shed queries third we develop two shedding algorithms optshed and fastshed optshed guarantees to find an optimal solution however at the cost of exponential complexity fastshed as confirmed by our experiments achieves close to optimal result in wide range of test cases finally we describe the in automaton shedding mechanism for xquery stream engines the experiments show that our proposed utility driven shedding solutions consistently achieve higher utility results compared to the existing relational shedding techniques
in the recent years the web has been rapidly deepened with the prevalence of databases online on this deep web many sources are structured by providing structured query interfaces and results organizing such structured sources into domain hierarchy is one of the critical steps toward the integration of heterogeneous web sources we observe that for structured web sources query schemas ie attributes in query interfaces are discriminative representatives of the sources and thus can be exploited for source characterization in particular by viewing query schemas as type of categorical data we abstract the problem of source organization into the clustering of categorical data our approach hypothesizes that homogeneous sources are characterized by the same hidden generative models for their schemas to find clusters governed by such statistical distributions we propose new objective function model differentiation which employs principled hypothesis testing to maximize statistical heterogeneity among clusters our evaluation over hundreds of real sources indicates that the schema based clustering accurately organizes sources by object domains eg books movies and on clustering web query schemas the model differentiation function outperforms existing ones such as likelihood entropy and context linkages with the hierarchical agglomerative clustering algorithm
nearest neighbor search is an important and widely used technique in number of important application domains in many of these domains the dimensionality of the data representation is often very high recent theoretical results have shown that the concept of proximity or nearest neighbors may not be very meaningful for the high dimensional case therefore it is often complex problem to find good quality nearest neighbors in such data sets furthermore it is also difficult to judge the value and relevance of the returned results in fact it is hard for any fully automated system to satisfy user about the quality of the nearest neighbors found unless he is directly involved in the process this is especially the case for high dimensional data in which the meaningfulness of the nearest neighbors found is questionable in this paper we address the complex problem of high dimensional nearest neighbor search from the user perspective by designing system which uses effective cooperation between the human and the computer the system provides the user with visual representations of carefully chosen subspaces of the data in order to repeatedly elicit his preferences about the data patterns which are most closely related to the query point these preferences are used in order to determine and quantify the meaningfulness of the nearest neighbors our system is not only able to find and quantify the meaningfulness of the nearest neighbors but is also able to diagnose situations in which the nearest neighbors found are truly not meaningful
we propose finite element simulation method that addresses the full range of material behavior from purely elastic to highly plastic for physical domains that are substantially reshaped by plastic flow fracture or large elastic deformations to mitigate artificial plasticity we maintain simulation mesh in both the current state and the rest shape and store plastic offsets only to represent the non embeddable portion of the plastic deformation to maintain high element quality in tetrahedral mesh undergoing gross changes we use dynamic meshing algorithm that attempts to replace as few tetrahedra as possible and thereby limits the visual artifacts and artificial diffusion that would otherwise be introduced by repeatedly remeshing the domain from scratch our dynamic mesher also locally refines and coarsens mesh and even creates anisotropic tetrahedra wherever simulation requests it we illustrate these features with animations of elastic and plastic behavior extreme deformations and fracture
shopping lists play central role in grocery shopping among other things shopping lists serve as memory aids and as tool for budgeting more interestingly shopping lists serve as an expression and indication of customer needs and interests accordingly shopping lists can be used as an input for recommendation techniques in this paper we describe methodology for making recommendations about additional products to purchase using items on the user's shopping list as shopping list entries seldom correspond to products we first use information retrieval techniques to map the shopping list entries into candidate products association rules are used to generate recommendations based on the candidate products we evaluate the usefulness and interestingness of the recommendations in user study
the emergence of wireless and mobile networks has made possible the introduction of new research area commerce or mobile commerce mobile payment is natural successor to web centric payments which has emerged as one of the sub domains of mobile commerce applications study reveals that there are wide ranges of mobile payment solutions and models which are available with the aid of various services such as short message service sms but there is no specific mobile payment system for educational institutions to collect the fees as well as for student community to pay the fees without huge investment this paper proposes secured framework for mobile payment consortia system mpcs to carry out the transactions from the bank to the academic institutions for the payment of fees by students through mobile phone mobile payment consortia system provides an end to end security using public key infrastructure pki through mobile information device profile midp enabled mobile device this framework provides an efficient reliable and secured system to perform mobile payment transactions and reduces transactional cost for both students and educational institutions mobile payment consortia system is designed with strong authentication and non repudiation by employing digital signatures confidentiality and message integrity are also provided by encrypting the messages at application level and by using public key certificates and digital signature envelops
the advance of multi core architectures provides significant benefits for parallel and throughput oriented computing but the performance of individual computation threads does not improve and may even suffer penalty because of the increased contention for shared resources this paper explores the idea of using available general purpose cores in cmp as helper engines for individual threads running on the active cores we propose lightweight architectural framework for efficient event driven software emulation of complex hardware accelerators and describe how this framework can be applied to implement variety of prefetching techniques we demonstrate the viability and effectiveness of our framework on wide range of applications from the spec cpu and olden benchmark suites on average our mechanism provides performance benefits within of pure hardware implementations furthermore we demonstrate that running event driven prefetching threads on top of baseline with hardware stride prefetcher yields significant speedups for many programs finally we show that our approach provides competitive performance improvements over other hardware approaches for multi core execution while executing fewer instructions and requiring considerably less hardware support
texture is an essential component of computer generated models for texture mapping procedure to be effective it has to generate continuous textures and cause only small mapping distortion the angle based flattening abf parameterization method is guaranteed to provide continuous no foldovers mapping it also minimizes the angular distortion of the parameterization including locating the optimal planar domain boundary however since it concentrates on minimizing the angular distortion of the mapping it can introduce relatively large linear distortionin this paper we introduce new procedure for reducing length distortion of an existing parameterization and apply it to abf results the linear distortion reduction is added as second step in texture mapping computation the new method is based on computing mapping from the plane to itself which has length distortion very similar to that of the abf parameterization by applying the inverse mapping to the result of the initial parameterization we obtain new parameterization with low length distortion we notice that the procedure for computing the inverse mapping can be applied to any other convenient mapping from the three dimensional surface to the plane in order to improve itthe mapping in the plane is computed by applying weighted laplacian smoothing to cartesian grid covering the planar domain of the initial mapping both the mapping and its inverse are provably continuous since angle preserving conformal mappings such as abf locally preserve distances as well the planar mapping has small local deformation as result the inverse mapping does not significantly increase the angular distortionthe combined texture mapping procedure provides mapping with low distance and angular distortion which is guaranteed to be continuous
retrieval speed and precision ultimately determine the success of any database system this article outlines the challenges posed by distributed and heterogeneous database systems including those that store unstructured data and surveys recent work much work remains to help users retrieve information with ease and efficiency from heterogeneous environment in which relational object oriented textual and pictorial databases coexist the article outlines the progress that has been made in query processing in distributed relational database systems and heterogeneous and multidatabase systems
the ultimate challenges of system modeling concern designing accurate yet highly transparent and user centric models we have witnessed plethora of neurofuzzy architectures which are aimed at addressing these two highly conflicting requirements this study is concerned with the design and the development of transparent logic networks realized with the aid of fuzzy neurons and fuzzy unineurons the construction of networks of this form requires formation of efficient interfaces that constitute conceptually appealing bridge between the model and the real world experimental environment in which the model is to be used in general the interfaces are constructed by invoking some form of granulation of information and binary boolean discretization in particular we introduce new discretization environment that is realized by means of particle swarm optimization pso and data clustering implemented by the means algorithm the underlying structure of the network is optimized by invoking combination of the pso and the mechanisms of conventional gradient based learning we discuss various optimization strategies by considering boolean as well as fuzzy data coming as the result of discretization of original experimental data and then involving several learning strategies we elaborate on the interpretation aspects of the network and show how those could be strengthened through efficient pruning we also show how the interpreted network leads to simpler and more accurate logic description of the experimental data number of experimental studies are included
this paper shows that even very small data caches when split to serve data streams exhibiting temporal and spatial localities can improve performance of embedded applications without consuming excessive silicon real estate or power it also shows that large block sizes or higher set associativities are unnecessary with split cache organizations we use benchmark programs from mibench to show that our cache organization outperforms other organizations in terms of miss rates access times energy consumption and silicon area
in this paper we address the task of crosslingual semantic relatedness we introduce method that relies on the information extracted from wikipedia by exploiting the interlanguage links available between wikipedia versions in multiple languages through experiments performed on several language pairs we show that the method performs well with performance comparable to monolingual measures of relatedness
the fair evaluation and comparison of side channel attacks and countermeasures has been long standing open question limiting further developments in the field motivated by this challenge this work makes step in this direction and proposes framework for the analysis of cryptographic implementations that includes theoretical model and an application methodology the model is based on commonly accepted hypotheses about side channels that computations give rise to it allows quantifying the effect of practically relevant leakage functions with combination of information theoretic and security metrics measuring the quality of an implementation and the strength of an adversary respectively from theoretical point of view we demonstrate formal connections between these metrics and discuss their intuitive meaning from practical point of view the model implies unified methodology for the analysis of side channel key recovery attacks the proposed solution allows getting rid of most of the subjective parameters that were limiting previous specialized and often ad hoc approaches in the evaluation of physically observable devices it typically determines the extent to which basic but practically essential questions such as how to compare two implementations or how to compare two side channel adversaries can be answered in sound fashion
with the significant advances in mobile computing technology there is an increasing demand for various mobile applications to process transactions in real time fashion when remote data access is considered in mobile environment data access delay becomes one of the most serious problems in meeting transaction deadlines in this paper we propose multi version data model and adopt the relative consistency as the correctness criterion for processing of real time transactions in mobile environment the purpose is to reduce the impacts of unpredictable and unreliable mobile network on processing of the real time transactions under the proposed model the overheads for concurrency control can be significantly reduced and the data availability is much enhanced even under network failures real time transaction may access stale data provided that they are relatively consistent with the data accessed by the transaction and the staleness of the data is within the requirements an image transaction model which pre fetches multiple data versions at fixed hosts is proposed to reduce the data access delay and to simplify the management of the real time transactions in mobile environment the image transaction model also helps in reducing the transaction restart overheads and minimizing the impacts of the unpredictable performance of mobile network on transaction executions
bisimulation semantics are very pleasant way to define the semantics of systems mainly because the simplicity of their definitions and their nice coalgebraic properties however they also have some disadvantages they are based on sequential operational semantics defined by means of an ordinary transition system and in order to be bisimilar two systems have to be too similar in this work we will present several natural proposals to define weaker bisimulation semantics that we think properly capture the desired behaviour of distributed systems the main virtue of all these semantics is that they are real bisimulation semantics thus inheriting most of the good properties of bisimulation semantics this is so because they can be defined as particular instances of jacobs and hughes categorical definition of simulation which they have already proved to satisfy all those properties
new ldquo range space rdquo approach is described for synergistic resolution of both stereovision and reflectance visual modeling problems simultaneously this synergistic approach can be applied to arbitrary camera arrangements with different intrinsic and extrinsic parameters image types image resolutions and image number these images are analyzed in step wise manner to extract range measurements and also to render customized perspective view the entire process is fully automatic an extensive and detailed experimental validation phase supports the basic feasibility and generality of the range space approach
category level object recognition segmentation and tracking in videos becomes highly challenging when applied to sequences from hand held camera that features extensive motion and zooming an additional challenge is then to develop fully automatic video analysis system that works without manual initialization of tracker or other human intervention both during training and during recognition despite background clutter and other distracting objects moreover our working hypothesis states that category level recognition is possible based only on an erratic flickering pattern of interest point locations without extracting additional features compositions of these points are then tracked individually by estimating parametric motion model groups of compositions segment video frame into the various objects that are present and into background clutter objects can then be recognized and tracked based on the motion of their compositions and on the shape they form finally the combination of this flow based representation with an appearance based one is investigated besides evaluating the approach on challenging video categorization database with significant camera motion and clutter we also demonstrate that it generalizes to action recognition in natural way
development of new embedded systems requires tuning of the software applications to specific hardware blocks and platforms as well as to the relevant input data instances the behaviour of these applications heavily relies on the nature of the input data samples thus making them strongly data dependent for this reason it is necessary to extensively profile them with representative samples of the actual input data an important aspect of this profiling is done at the dynamic data type level which actually steers the designers choice of implementation of these data types the behaviour of the applications is then characterized through an analysis phase as collection of software metadata that can be used to optimize the system as whole in this paper we propose to represent the behaviour of data dependent applications to enable optimizations rather than to analyze their structure or to define the engineering process behind them moreover we specifically limit ourselves to the scope of applications dominated by dynamically allocated data types running on embedded systems we characterize the software metadata that these optimizations require and we present methodology as well as appropriate techniques to obtain this information from the original application the optimizations performed on complete case study utilizing the extracted software metadata achieve overall improvements of up to in the number of cycles spent accessing memory when compared to code optimized only with the static techniques applied by gnu
this work addresses the issue of answering spatio temporal range queries when there is uncertainty associated with the model of the moving objects uncertainty is inherent in moving objects database mod applications and capturing it in the data model has twofold impact the number of updates when the actual trajectory deviates from its mod representation the linguistic constructs and the processing algorithms for querying the mod the paper presents both spatial and temporal uncertainty aspects which are combined into one model of uncertain trajectories given the model the methodology is presented which enables processing of queries such as what is the probability that given moving object was will be inside given region sometimes always during given time interval where the regions are bounded by arbitrary polygons
in recent years grid systems and peer to peer networks are the most commonly used solutions to achieve the same goal the sharing of resources and services in heterogeneous dynamic distributed environments many studies have proposed hybrid approaches that try to conjugate the advantages of the two models this paper proposes an architecture that integrates the pp interaction model in grid environments so as to build an open cooperative model wherein grid entities are composed in decentralized way in particular this paper focuses on qos aware discovery algorithm for pp grid systems analyzing protocol and explaining techniques used to improve its performance
technological success has ushered in massive amounts of data for scientific analysis to enable effective utilization of these data sets for all classes of users supporting intuitive data access and manipulation interfaces is crucial this paper describes an autonomous scientific workflow system that enables high level natural language based queries over low level data sets our technique involves combination of natural language processing metadata indexing and semantically aware workflow composition engine which dynamically constructs workflows for answering queries based on service and data availability specific contribution of this work is metadata registration scheme that allows for unified index of heterogeneous metadata formats and service annotations our approach thus avoids standardized format for storing all data sets or the implementation of federated mediator based querying framework we have evaluated our system using case study from the geospatial domain to show functional results our evaluation supports the potential benefits which our approach can offer to scientific workflow systems and other domain specific data intensive applications
this paper evaluates pointer tainting an incarnation of dynamic information flow tracking dift which has recently become an important technique in system security pointer tainting has been used for two main purposes detection of privacy breaching malware eg trojan keyloggers obtaining the characters typed by user and detection of memory corruption attacks against non control data eg buffer overflow that modifies user's privilege level in both of these cases the attacker does not modify control data such as stored branch targets so the control flow of the target program does not change phrased differently in terms of instructions executed the program behaves normally as result these attacks are exceedingly difficult to detect pointer tainting is considered one of the onlymethods for detecting them in unmodified binaries unfortunately almost all of the incarnations of pointer tainting are flawed in particular we demonstrate that the application of pointer tainting to the detection of keyloggers and other privacybreaching malware is problematic we also discuss whether pointer tainting is able to reliably detect memory corruption attacks against non control data pointer tainting generates itself the conditions for false positives we analyse the problems in detail and investigate various ways to improve the technique most have serious drawbacks in that they are either impractical and incur many false positives still and or cripple the technique's ability to detect attacks in conclusion we argue that depending on architecture and operating system pointer tainting may have some value in detecting memory orruption attacks albeit with false negatives and not on the popular architecture but it is fundamentally not suitable for automated detecting of privacy breaching malware such as keyloggers
common belief in the scientific community is that traffic classifiers based on deep packet inspection dpi are far more expensive in terms of computational complexity compared to statistical classifiers in this paper we counter this notion by defining accurate models for deep packet inspection classifier and statistical one based on support vector machines and by evaluating their actual processing costs through experimental analysis the results suggest that contrary to the common belief dpi classifier and an svm based one can have comparable computational costs although much work is left to prove that our results apply in more general cases this preliminary analysis is first indication of how dpi classifiers might not be as computationally complex compared to other approaches as we previously thought
in recent years both performance and power have become key factors in efficient memory design in this paper we propose systematic approach to reduce the energy consumption of the entire memory hierarchy we first evaluate an existing power aware memory system where memory modules can exist in different power modes and then propose on chip memory module buffers called energy saver buffers esb which reside in between the cache and main memory esbs reduce the additional overhead incurred due to frequent resynchronization of the memory modules in low power state an additional improvement is attained by using model that dynamically resizes the active cache based on the varying needs of program our experimental results demonstrate that an integrated approach can reduce the energy delay product by as much as when compared to traditional non power aware memory hierarchy
we show how properties of an interesting class of imperative programs can be calculated by means of relational modeling and symbolic computation the ideas of are implemented using symbolic computations based on maple
focus on organizing and implementing workflows in government from standpoint of data awareness or even data centricity might provide opportunities to address several of the challenges facing governments efforts to improve government effectiveness and efficiency as well as interoperation between government entities the notion of data aware as opposed to data unaware or just process centric workflows is based on taking into account and using the particular enactments and instance specific data in the workflow itself and beyond its single instance in other words on the one hand workflows process data however through their enactments and instances on the other hand workflows are data ready as inputs for other workflows including cross process enactment mining and analyses to be useful as strategy in government we need to explore how data centric approaches can tackle the specific challenges of government such as the ill structured or semi structured workflows found in emergency and disaster response management and we need to better understand the specific constraints under which intra and cross agency data centric workflows can be designed and implemented this paper lays out research agenda that will inform questions about the issues with and the potential of the data centric approach in the context of government
many electronic cash systems have been proposed with the proliferation of the internet and the activation of electronic commerce cash enables the exchange of digital coins with value assured by the bank's signature and with concealed user identity in an electronic cash system user can withdraw coins from the bank and then spends each coin anonymously and unlinkably in this paper we design an efficient anonymous mobile payment system based on bilinear pairings in which the anonymity of coins is revocable by trustee in case of dispute the message transfer from the customer to the merchant occurs only once during the payment protocol also the amount of communication between customer and merchant is about bits therefore our mobile payment system can be used in the wireless networks with the limited bandwidth the security of the new system is under the computational diffie hellman problem in the random oracle model
in this paper we present formal model named pobsam policy based self adaptive model for developing and modeling self adaptive systems in this model policies are used as mechanism to direct and adapt the behavior of self adaptive systems pobsam model consists of set of self managed modules smm an smm is collection of autonomous managers and managed actors managed actors are dedicated to functional behavior while autonomous managers govern the behavior of managed actors by enforcing suitable policies to adapt smm behavior in response to changes policies governing an smm are adjusted ie dynamic policies are used to govern and adapt system behavior we employ the combination of an algebraic formalism and an actor based model to specify this model formally managers are modeled as meta actors whose policies are described using an algebra managed actors are expressed by an actor model furthermore we provide an operational semantics for pobsam described using labeled transition systems
embedded systems designers are free to choose the most suitable configuration of cache in modern processor based socs choosing the appropriate cache configuration necessitates the simulation of long memory access traces to accurately obtain hit miss rates the long execution time taken to simulate these traces particularly separate simulation for each configuration is major drawback researchers have proposed techniques to speed up the simulation of caches with lru replacement policy these techniques are of little use in the majority of embedded processors as these processors utilize round robin policy based caches in this paper we propose fast cache simulation approach called scud sorted collection of unique data for caches with the round robin policy scud is single pass cache simulator that can simulate multiple cache configurations with varying set sizes and associativities by reading the application trace once utilizing fast binary searches in novel data structure scud simulates an application trace significantly faster than widely used single configuration cache simulator dinero iv we show scud can simulate set of cache configurations up to times faster than dinero iv scud shows an average speed up of times over dinero iv for mediabench applications and an average speed up of over times for spec cpu applications
the frequent items problem is to process stream of items and find all items occurring more than given fraction of the time it is one of the most heavily studied problems in data stream mining dating back to the many applications rely directly or indirectly on finding the frequent items and implementations are in use in large scale industrial systems however there has not been much comparison of the different methods under uniform experimental conditions it is common to find papers touching on this topic in which important related work is mischaracterized overlooked or reinvented in this paper we aim to present the most important algorithms for this problem in common framework we have created baseline implementations of the algorithms and used these to perform thorough experimental study of their properties we give empirical evidence that there is considerable variation in the performance of frequent items algorithms the best methods can be implemented to find frequent items with high accuracy using only tens of kilobytes of memory at rates of millions of items per second on cheap modern hardware
constructing correct concurrent garbage collection algorithms is notoriously hard numerous such algorithms have been proposed implemented and deployed and yet the relationship among them in terms of speed and precision is poorly understood and the validation of one algorithm does not carry over to othersas programs with low latency requirements written in garbagecollected languages become part of society's mission critical infrastructure it is imperative that we raise the level of confidence in the correctness of the underlying system and that we understand the trade offs inherent in our algorithmic choicein this paper we present correctness preserving transformations that can be applied to an initial abstract concurrent garbage collection algorithm which is simpler more precise and easier to prove correct than algorithms used in practice but also more expensive and with less concurrency we then show how both pre existing and new algorithms can be synthesized from the abstract algorithm by series of our transformations we relate the algorithms formally using new definition of precision and informally with respect to overhead and concurrencythis provides many insights about the nature of concurrent collection allows the direct synthesis of new and useful algorithms reduces the burden of proof to single simple algorithm and lays the groundwork for the automated synthesis of correct concurrent collectors
moving and resizing desktop windows are frequently performed but largely unexplored interaction tasks the standard title bar and border dragging techniques used for window manipulation have not changed much over the years we studied three new methods to move and resize windows the new methods are based on proxy and goal crossing techniques to eliminate the need of long cursor movements and acquiring narrow window borders instead moving and resizing actions are performed by manipulating proxy objects close to the cursor and by sweeping cursor motions across window borders we compared these techniques with the standard techniques the results indicate that further investigations and redesigns of window manipulation techniques are worthwhile all new techniques were faster than the standard techniques with task completion times improving more than in some cases also the new resizing techniques were found to be less error prone than the traditional click and drag method
permissive nominal logic pnl is an extension of first order logic where term formers can bind names in their arguments this allows for direct axiomatisations with binders such as the quantifier of first order logic itself and the binder of the lambda calculus this also allows us to finitely axiomatise arithmetic like first and higher order logic and unlike other nominal logics equality reasoning is not necessary to alpha rename all this gives pnl much of the expressive power of higher order logic but terms derivations and models of pnl are first order in character and the logic seems to strike good balance between expressivity and simplicity
computing environments on cellphones especially smartphones are becoming more open and general purpose thus they also become attractive targets of malware cellphone malware not only causes privacy leakage extra charges and depletion of battery power but also generates malicious traffic and drains down mobile network and service capacity in this work we devise novel behavior based malware detection system named pbmds which adopts probabilistic approach through correlating user inputs with system calls to detect anomalous activities in cellphones pbmds observes unique behaviors of the mobile phone applications and the operating users on input and output constrained devices and leverages hidden markov model hmm to learn application and user behaviors from two major aspects process state transitions and user operational patterns built on these pbdms identifies behavioral differences between malware and human users through extensive experiments on major smartphone platforms we show that pbmds can be easily deployed to existing smartphone hardware and it achieves high detection accuracy and low false positive rates in protecting major applications in smartphones
context the technology acceptance model tam was proposed in as means of predicting technology usage however it is usually validated by using measure of behavioural intention to use bi rather than actual usage objective this review examines the evidence that the tam predicts actual usage using both subjective and objective measures of actual usage method we performed systematic literature review based on search of six digital libraries along with vote counting meta analysis to analyse the overall results results the search identified relevant empirical studies in articles the results show that bi is likely to be correlated with actual usage however the tam variables perceived ease of use peu and perceived usefulness pu are less likely to be correlated with actual usage conclusion care should be taken using the tam outside the context in which it has been validated
recent years have transformed the web from web of content to web of applications and social content thus it has become crucial to be able to tap on this social aspect of the web whenever possible in addition to its content particularly for focused crawling in this paper we present novel profile based focused crawling system for dealing with the increasingly popular social media sharing web sites without assuming any privileged access to the internal private databases of such websites nor any requirement for the existence of apis for the extraction of social data our experiments prove the robustness of our profile based focused crawler as well as significant improvement in harvest ratio compared to breadth first and opic crawlers when crawling the flickr web site for two different topics
the combination of sgml and database technology allows to refine both declarative and navigational access mechanisms for structured document collection with regard to declarative access the user can formulate complex information needs without knowing query language the respective document type definition dtd or the underlying modelling navigational access is eased by hyperlink rendition mechanisms going beyond plain link integrity checking with our approach the database internal representation of documents is configurable it allows for an efficient implementation of operations because dtd knowledge is not needed for document structure recognition we show how the number of method invocations and the cost of parsing can be significantly reduced
information shown on tabletop display can appear distorted when viewed by seated user even worse the impact of this distortion is different depending on the location of the information on the display in this paper we examine how this distortion affects the perception of the basic graphical elements of information visualization shown on displays at various angles we first examine perception of these elements on single display and then compare this to perception across displays in order to evaluate the effectiveness of various elements for use in tabletop and multi display environment we found that the perception of some graphical elements is more robust to distortion than others we then develop recommendations for building data visualizations for these environments
the paper proposes new knowledge representation language called dlp which extends disjunctive logic programming with strong negation by inheritance the addition of inheritance enhances the knowledge modeling features of the language providing natural representation of default reasoning with exceptions declarative model theoretic semantics of dlp is provided which is shown to generalize the answer set semantics of disjunctive logic programs the knowledge modeling features of the language are illustrated by encoding classical nonmonotonic problems in dlp the complexity of dlp is analyzed proving that inheritance does not cause any computational overhead as reasoning in dlp has exactly the same complexity as reasoning in disjunctive logic programming this is confirmed by the existence of an efficient translation from dlp to plain disjunctive logic programming using this translation an advanced kr system supporting the dlp language has been implemented on top of the dlv system and has subsequently been integrated into dlv
the event service of the common object request broker architecture corba is useful in supporting decoupled and asynchronous communication between distributed object components however the specification of the event service standard does not require implementation to provide facilities to guarantee efficient event data delivery consequently applications in which large number of objects need to communicate via an event service channel may suffer from poor performance in this paper generic corba based framework is proposed to tackle this scalability problem two techniques are applied namely event channel federation and load balancing the solution is transparent in the sense that it exports the same idl interface as the original event service we explore three critical dimensions underlying the design of the load balancing algorithm and conduct experiments to evaluate their impact on the overall performance of the framework the results provide some useful insights into the improvement of the scalability of the event service
the development of efficient techniques for transforming massive volumes of remotely sensed hyperspectral data into scientific understanding is critical for space based earth science and planetary exploration although most available parallel processing strategies for information extraction and mining from hyperspectral imagery assume homogeneity in the underlying computing platform heterogeneous networks of computers hnocs have become promising cost effective solution expected to play major role in many on going and planned remote sensing missions in this paper we develop new morphological parallel algorithm for hyperspectral image classification using heterompi an extension of mpi for programming high performance computations on hnocs the main idea of heterompi is to automate and optimize the selection of group of processes that executes heterogeneous algorithm faster than any other possible group in heterogeneous environment in order to analyze the impact of many to one gather communication operations introduced by our proposed algorithm we resort to recently proposed collective communication model the parallel algorithm is validated using two heterogeneous clusters at university college dublin and massively parallel beowulf cluster at nasa's goddard space flight center
this paper presents many core heterogeneous computational platform that employs gals compatible circuit switched on chip network the platform targets streaming dsp and embedded applications that have high degree of task level parallelism among computational kernels the test chip was fabricated in nm cmos consisting of simple small programmable cores three dedicated purpose accelerators and three shared memory modules all processors are clocked by their own local oscillators and communication is achieved through simple yet effective source synchronous communication technique that allows each interconnection link between any two processors to sustain peak throughput of one data word per cycle complete wlan baseband receiver was implemented on this platform it has real time throughput of mbps with all processors running at mhz and and consumes an average mw with mw or dissipated by its interconnection links we can fully utilize the benefit of the gals architecture and by adjusting each processor's oscillator to run at workload based optimal clock frequency with the chip's dual supply voltages set at and the receiver consumes only mw in power reduction measured results of its power consumption on the real chip come within the difference of only compared with the estimated results showing our design to be highly reliable and efficient
despite the fact that global software development gsd is steadily becoming the standard engineering mode in the software industry commercial projects still struggle with how to effectively manage it recent research and our own experiences from numerous gsd projects at capgemini sd indicate that staging the development process with handover checkpoints is promising practice in order to tackle many of the encountered problems in practice in this paper we discuss typical management problems in gsd we describe how handover checkpoints are used at capgemini sd to control and safely manage large gsd projects we show how these handover checkpoints and the use of cohesive and self contained work packages effectively mitigate the discussed management problems we are continuously refining and improving our handover checkpoint approach by applying it within large scale commercial gsd projects we thus believe that the presented results can serve the practitioner as fundament for implementing and customizing handover checkpoints within his own organisation
play on demand is usually regarded as feasible access mode for web content including streaming video web pages and so on web services and some software as service saas applications but not for common desktop applications this paper presents such solution for windows desktop applications based on lightweight virtualization and network transportation technologies which allows user to run her personalized software on any compatible computer across the internet even though they do not exist on local disks of the host in our approach the user's data and their configurations are stored on portable usb device at run time the desktop applications are downloaded from the internet and run in lightweight virtualization environment in which some resource accessing apis such as registry files directories environment variables and the like are intercepted and redirected to the portable device or network as needed because applications are played without installation like streaming media they can be called streaming software moreover to protect software vendors rights access control technologies are used to block any illegal access in the current implementation pp transportation is used as the transport method however our design actually does not rely on pp and another data delivery mechanism like dedicated file server could be employed instead to make the system more predictable this paper describes the design and technical details for this system presents demo application and evaluates it performance the proposed solution is shown to be more efficient in performance and storage capacity than some of the existing solutions based on vm techniques
in an application where sparse matching of feature points is used towards fast scene reconstruction the choice of the type of features to be matched has an important impact on the quality of the resulting model in this work method is presented for quickly and reliably selecting and matching points from three views of scene the selected points are based on epipolar gradients and consist of stable image features relevant to reconstruction then the selected points are matched using edge transfer measure of geometric consistency for point triplets and the edges on which they lie this matching scheme is tolerant to image deformations due to changes in viewpoint models drawn from matches obtained by the proposed technique are shown to demonstrate its usefulness
several approaches to collaborative filtering have been studied but seldom have studies been reported for large several millionusers and items and dynamic the underlying item set is continually changing settings in this paper we describe our approach to collaborative filtering for generating personalized recommendations for users of google news we generate recommendations using three approaches collaborative filtering using minhash clustering probabilistic latent semantic indexing plsi and covisitation counts we combine recommendations from different algorithms using linear model our approach is content agnostic and consequently domain independent making it easily adaptable for other applications and languages with minimal effort this paper will describe our algorithms and system setup in detail and report results of running the recommendations engine on google news
this article presents an approach to identify abstract data types adt and abstract state encapsulations ase also called abstract objects in source code this approach named similarity clustering groups together functions types and variables into adt and ase candidates according to the proportion of features they share the set of features considered includes the context of these elements the relationships to their environment and informal information prototype tool has been implemented to support this approach it has been applied to three systems each between ndash kloc the adts and ases identified by the approach are compared to those identified by software engineers who did not know the proposed approach or other automatic approaches within this case study this approach has been shown to have higher detection quality and to identify in most of the cases more adts and ases than the other techniques in all other cases its detection quality is second best nb this article reports on work in progress on this approach which has evolved since it was presented in the original ase conference paper
competitive native solvers for answer set programming asp perform backtracking search by assuming the truth of literals the choice of literals the heuristic is fundamental for the performance of these systems most of the efficient asp systems employ heuristic based on look ahead that is literal is tentatively assumed and its heuristic value is based on its deterministic consequences however looking ahead is costly operation and indeed look ahead often accounts for the majority of time taken by asp solvers for satisfiability sat radically different approach called look back heuristic proved to be quite successful instead of looking ahead one uses information gathered during the computation performed so far thus looking back in this approach atoms which have been frequently involved in inconsistencies are preferred in this paper we carry over this approach to the framework of disjunctive asp we design number of look back heuristics exploiting peculiarities of asp and implement them in the asp system dlv we compare their performance on collection of hard asp programs both structured and randomly generated these experiments indicate that very basic approach works well outperforming all of the prominent disjunctive asp systems dlv with its traditional heuristic gnt and cmodels on many of the instances considered
the widespread presence of simd devices in today's microprocessors has made compiler techniques for these devices tremendously important one of the most important and difficult issues that must be addressed by these techniques is the generation of the data permutation instructions needed for non contiguous and misaligned memory references these instructions are expensive and therefore it is of crucial importance to minimize their number to improve performance and in many cases enable speedups over scalar codealthough it is often difficult to optimize an isolated data reorganization operation collection of related data permutations can often be manipulated to reduce the number of operations this paper presents strategy to optimize all forms of data permutations the strategy is organized into three steps first all data permutations in the source program are converted into generic representation these permutations can originate from vector accesses to non contiguous and misaligned memory locations or result from compiler transformations second an optimization algorithm is applied to reduce the number of data permutations in basic block by propagating permutations across statements and merging consecutive permutations whenever possible the algorithm can significantly reduce the number of data permutations finally code generation algorithm translates generic permutation operations into native permutation instructions for the target platform experiments were conducted on various kinds of applications the results show that up to of the permutation instructions are eliminated and as result the average performance improvement is on vmx and on sse for several applications near perfect speedups have been achieved on both platforms
this paper presents an approach that uses special purpose rbac constraints to base certain access control decisions on context information in our approach context constraint is defined as dynamic rbac constraint that checks the actual values of one or more contextual attributes for predefined conditions if these conditions are satisfied the corresponding access request can be permitted accordingly conditional permission is an rbac permission which is constrained by one or more context constraints we present an engineering process for context constraints that is based on goal oriented requirements engineering techniques and describe how we extended the design and implementation of an existing rbac service to enable the enforcement of context constraints with our approach we aim to preserve the advantages of rbac and offer an additional means for the definition and enforcement of fine grained context dependent access control policies
publish subscribe systems are used increasingly often as communication mechanism in loosely coupled distributed applications with their gradual adoption in mission critical areas it is essential that systems are subjected to rigorous performance analysis before they are put into production however existing approaches to performance modeling and analysis of publish subscribe systems suffer from many limitations that seriously constrain their practical applicability in this paper we present set of generalized and comprehensive analytical models of publish subscribe systems employing different peer to peer and hierarchical routing schemes the proposed analytical models address the major limitations underlying existing work in this area and are the first to consider all major performance relevant system metrics including the expected broker and link utilization the expected notification delay the expected time required for new subscriptions to become fully active as well as the expected routing table sizes and message rates to illustrate our approach and demonstrate its effectiveness and practicality we present case study showing how our models can be exploited for capacity planning and performance prediction in realistic scenario
speed up techniques that exploit given node coordinates have proven useful for shortest path computations in transportation networks and geographic information systems to facilitate the use of such techniques when coordinates are missing from some or even all of the nodes in network we generate artificial coordinates using methods from graph drawing experiments on large set of german train timetables indicate that the speed up achieved with coordinates from our drawings is close to that achieved with the true coordinates and in some special cases even better
this paper proposes novel method for phrase based statistical machine translation based on the use of pivot language to translate between languages and with limited bilingual resources we bring in third language called the pivot language for the language pairs and there exist large bilingual corpora using only and bilingual corpora we can build translation model for the advantage of this method lies in the fact that we can perform translation between and even if there is no bilingual corpus available for this language pair using bleu as metric our pivot language approach significantly outperforms the standard model trained on small bilingual corpus moreover with small bilingual corpus available our method can further improve translation quality by using the additional and bilingual corpora
media spaces and videoconference systems are beneficial for connecting separated co workers and providing rich contextual information however image sharing communication tools may also touch on sensitive spots of the human psyche related to personal perceived image issues eg appearance self image self presentation and vanity we conducted two user studies to examine the impact of self image concerns on the use of media spaces and videoconference systems our results suggest that personal perceived image concerns have considerable impact on the comfort level of users and may hinder effective communication we also found that image filtering techniques can help users feel more comfortable our results revealed that distortion filters which are frequently cited to help preserve privacy do not tend to be the ones preferred by users instead users seemed to favor filters that make subtle changes to their appearance or in some instances they preferred to use surrogate instead
xml documents have recently become ubiquitous because of their varied applicability in number of applications classification is an important problem in the data mining domain but current classification methods for xml documents use ir based methods in which each document is treated as bag of words such techniques ignore significant amount of information hidden inside the documents in this paper we discuss the problem of rule based classification of xml data by using frequent discriminatory substructures within xml documents such technique is more capable of finding the classification characteristics of documents in addition the technique can also be extended to cost sensitive classification we show the effectiveness of the method with respect to other classifiers we note that the methodology discussed in this paper is applicable to any kind of semi structured data
in this paper we present new algorithm named turbosyn for fpga synthesis with retiming and pipelining to minimizethe clock period for sequential circuitsfor target clockperiod since pipelining can eliminate all critical paths but not critical loops we concentrate on fpga synthesis toeliminate the critical loopswe combine the combinationalfunctional decomposition technique with retiming to performthe sequential functional decomposition and incorporate itin the label computation of turbomap to eliminate allcritical loopsthe results show significant improvementover the state of the art fpga mapping and resynthesis algorithms times reduction on the clock period moreover we develop novel approach for positive loop detectionwhich leads to over times speedup of the algorithmas result turbosyn can optimize sequential circuits ofover gates and flipflops in reasonable time
real time database management systems rt dbms have the necessary characteristics for providing efficient support to develop applications in which both data and transactions have temporal constraints however in the last decade new applications were identified and are characterized by large geographic distribution high heterogeneity lack of global control partial failures and lack of safety besides they need to manage large data volumes with real time constraints scheduling algorithms should consider transactions with soft deadlines and the concurrency control protocols should allow conflicting transactions to execute in parallel the last ones should be based in their requirements which are specified through both quality of services functions and performance metrics in this work method to model and develop applications that execute in open and unpredictable environments is proposed based on this model it is possible to perform analysis and simulations of systems to guide the decision making process and to identify solutions for improving it for validating the model case study considering the application domain of sensors network is discussed
in this paper we describe new via configurable routing architecture which shows much better throughput and performance than the previous structures we demonstrate how to construct single via mask fabric to reduce the mask cost further and we analyze the penalties which it incurs to solve the routability problem commonly existing in fabric based designs an efficient white space allocation and an incremental cell movement scheme are suggested which help to provide fast design convergence and early prediction of circuit's mappability to given fabric
array redistribution is usually needed for more efficiently executing data parallel program on distributed memory multicomputers to minimize the redistribution data transfer cost processor mapping techniques were proposed to reduce the amount of redistributed data elements theses techniques demand that the beginning data elements on processor not be redistributed in the redistribution on the other hand for satisfying practical computation needs programmer may require other data elements to be un redistributed localized in the redistribution in this paper we propose flexible processor mapping technique for the block cyclic redistribution to allow the programmer to localize the required data elements in the redistribution we also present an efficient redistribution method for the redistribution employing our proposed technique the data transfer cost reduction and system performance improvement for the redistributions with data localization are analyzed and presented in our experimental results
consider data warehouses as large data repositories queried for analysis and data mining in variety of application contexts query over such data may take large amount of time to be processed in regular pc consider partitioning the data into set of pcs nodes with either parallel database server or any database server at each node and an engine independent middleware nodes and network may even not be fully dedicated to the data warehouse in such scenario care must be taken for handling processing heterogeneity and availability so we study and propose efficient solutions for this we concentrate on three main contributions performance wise index measuring relative performance replication degree flexible chunk wise organization with on demand processing these contributions extend the previous work on de clustering and replication and are generic in the sense that they can be applied in very different contexts and with different data partitioning approaches we evaluate their merits with prototype implementation of the system
some of the most difficult questions to answer when designing distributed application are related to mobility what information to transfer between sites and when and how to transfer it network transparent distribution the property that program's behavior is independent of how it is partitioned among sites does not directly address these questions therefore we propose to extend all language entities with network behavior that enables efficient distributed programming by giving the programmer simple and predictable control over network communication patterns in particular we show how to give objects an arbitrary mobility behavior that is independent of the objects definition in this way the syntax and semantics of objects are the same regardless of whether they are used as stationary servers mobile agents or simply as caches these ideas have been implemented in distributed oz concurrent object oriented language that is state aware and has dataflow synchronization we prove that the implementation of objects in distributed oz is network transparent to satisfy the predictability condition the implementation avoids forwarding chains through intermediate sites the implementation is an extension to the publicly available dfki oz system
many spatiotemporal applications store moving object data in the form of trajectories various recent works have addressed interesting queries on trajectorial data mainly focusing on range queries and nearest neighbor queries here we examine another interesting query the time relaxed spatiotemporal trajectory join trstj which effectively finds groups of moving objects that have followed similar movements in different times we first attempt to address the trstj problem using symbolic representation algorithm which we have recently proposed for trajectory joins however we show experimentally that this solution produces false positives that grow rapidly with the increase of the problem size as result it is inefficient for trstj queries as it leads to large query time overhead in order to improve query performance we propose two important heuristics that turn the symbolic represenation approach effective for trstj queries our first improvement allows the use of multiple origins when processing strings representing trajectories the experimental evaluation shows that the multiple origin approach drastically reduces query performance we then present divide and conquer approach to further reduce false positives through symbolic class separation the proposed solutions can be combined together which leads to even better query performance we present an experimental study revealing the advantages of using these approaches for solving time relaxed spatiotemporal trajectory join queries
divide and conquer programs are easily parallelized by letting the programmer annotate potential parallelism in the form of spawn and sync constructs to achieve efficient program execution the generated work load has to be balanced evenly among the available cpus for single cluster systems random stealing rs is known to achieve optimal load balancing however rs is inefficient when applied to hierarchical wide area systems where multiple clusters are connected via wide area networks wans with high latency and low bandwidthin this paper we experimentally compare rs with existing load balancing strategies that are believed to be efficient for multi cluster systems random pushing and two variants of hierarchical stealing we demonstrate that in practice they obtain less than optimal results we introduce novel load balancing algorithm cluster aware random stealing crs which is highly efficient and easy to implement crs adapts itself to network conditions and job granularities and does not require manually tuned parameters although crs sends more data across the wans it is faster than its competitors for out of test applications with various wan configurations it has at most overhead in run time compared to rs on single large cluster even with high wide area latencies and low wide area bandwidths these strong results suggest that divide and conquer parallelism is useful model for writing distributed supercomputing applications on hierarchical wide area systems
counterexample guided abstraction refinement cegar is key technique for the verification of computer programs grumberg et al developed cegar based algorithm for the modal calculus there every abstract state is split in refinement step in this paper the work of grumberg et al is generalized by presenting new cegar based algorithm for the calculus it is based on more expressive abstract model and applies refinement only locally at single abstract state ie the lazy abstraction technique for safety properties is adapted to the calculus furthermore it separates refinement determination from the valued based model checking three different heuristics for refinement determination are presented and illustrated
trust management systems are frameworks for authorization in modern distributed systems allowing remotely accessible resources to be protected by providers by allowing providers to specify policy and access requesters to possess certain access rights trust management automates the process of determining whether access should be allowed on the basis of policy rights and an authorization semantics in this paper we survey modern state of the art in trust management authorization focusing on features of policy and rights languages that provide the necessary expressiveness for modern practice we characterize systems in light of generic structure that takes into account components of practical implementations we emphasize systems that have formal foundation since security properties of them can be rigorously guaranteed underlying formalisms are reviewed to provide necessary background
we show how range of role based access control rbac models may be usefully represented as constraint logic programs executable logical specifications the rbac models that we define extend the standard rbac models that are described by sandhu et al and enable security administrators to define range of access policies that may include features like denials of access and temporal authorizations that are often useful in practice but which are not widely supported in existing access control models representing access policies as constraint logic programs makes it possible to support certain policy options constraint checks and administrator queries that cannot be represented by using related methods like logic programs representing an access control policy as constraint logic program also enables access requests and constraint checks to be efficiently evaluated
this paper contributes to growing body of design patterns in interaction design for cooperative work while also describing how to go from field studies to design patterns it focuses on sociable face to face situations the patterns are based on field studies and design work in three sociable settings where desirable use qualities were identified and translated into forces in three design patterns for controlling information visibility on the basis of the patterns the design of multiple device multimedia platform is described it is shown that desirable qualities of systems in use can be utilised as forces in patterns which means that traditional qualitative research is highly valuable when documenting design knowledge in patterns three classes of interaction design patterns are identified environments for interactions means for interaction and interfaces for interaction these classes describe types of patterns within hierarchical model of interaction design
the wireless network community has become increasingly aware of the benefits of data driven link estimation and routing as compared with beacon based approaches but the issue of biased link sampling bls has not been well studied even though it affects routing convergence in the presence of network and environment dynamics focusing on traffic induced dynamics we examine the open unexplored question of how serious the bls issue is and how to effectively address it when the routing metric etx is used for wide range of traffic patterns and network topologies and using both node oriented and network wide analysis and experimentation we discover that the optimal routing structure remains quite stable even though the properties of individual links and routes vary significantly as traffic pattern changes in cases where the optimal routing structure does change data driven link estimation and routing is either guaranteed to converge to the optimal structure or empirically shown to converge to close to optimal structure these findings provide the foundation for addressing the bls issue in the presence of traffic induced dynamics and suggest approaches other than existing ones these findings also demonstrate that it is possible to maintain an optimal stable routing structure despite the fact that the properties of individual links and paths vary in response to network dynamics
mobile computation in which executing computations can move from one physical computing device to another is recurring theme from os process migration to language level mobility to virtual machine migration this article reports on the design implementation and verification of overlay networks to support reliable communication between migrating computations in the nomadic pict project we define two levels of abstraction as calculi with precise semantics low level nomadic pi calculus with migration and location dependent communication and high level calculus that adds location independent communication implementations of location independent communication as overlay networks that track migrations and forward messages can be expressed as translations of the high level calculus into the low we discuss the design space of such overlay network algorithms and define three precisely as such translations based on the calculi we design and implement the nomadic pict distributed programming language to let such algorithms and simple applications above them to be quickly prototyped we go on to develop the semantic theory of the nomadic pi calculi proving correctness of one example overlay network this requires novel equivalences and congruence results that take migration into account and reasoning principles for agents that are temporarily immobile eg waiting on lock elsewhere in the system the whole stands as demonstration of the use of principled semantics to address challenging system design problems
choosing good variable order is crucial for making symbolic state space generation algorithms truly efficient one such algorithm is the mdd based saturation algorithm for petri nets implemented in smart whose efficiency relies on exploiting event locality this paper presents novel static ordering heuristic that considers place invariants of petri nets in contrast to related work we use the functional dependencies encoded by invariants to merge decision diagram variables rather than to eliminate them we prove that merging variables always yields smaller mdds and improves event locality while eliminating variables may increase mdd sizes and break locality combining this idea of merging with heuristics for maximizing event locality we obtain an algorithm for static variable order which outperforms competing approaches regarding both time efficiency and memory efficiency as we demonstrate by extensive benchmarking
in this paper we consider the problem of processor allocation on mesh based multiprocessor systems we employ the idea of using migration to minimize fragmentation and the overall processing time of the tasks in our schemes we consider the use of task migration whenever required to improve the problem of fragmentation to this end we propose three efficient schemes to improve the performance of first fit allocation strategies commonly used in practice the first scheme called the first fit mesh bifurcation ffmb scheme attempts to start the search for free submesh from either the bottom left corner or the top left corner of the mesh so as to reduce the amount of fragmentation in the mesh the next two schemes called the online dynamic compaction single corner odc sc and online dynamic compaction four corners odc fc schemes use task migration to improve the performance of existing submesh allocation strategies we perform rigorous simulation experiments based on practical workloads as reported in the literature to quantify all our proposed schemes and compare them against standard schemes existing in the literature based on the results we make clear recommendations on the choice of the strategies
data exchange and virtual data integration have been the subject of several investigations in the recent literature at the same time the notion of peer data management has emerged as powerful abstraction of many forms of flexible and dynamic data centere ddistributed systems although research on the above issues has progressed considerably in the last years clear understanding on how to combine data exchange and data integration in peer data management is still missing this is the subject of the present paper we start our investigation by first proposing novel framework for peer data exchange showing that it is generalization of the classical data exchange setting we also present algorithms for all the relevant data exchange tasks and show that they can all be done in polynomial time with respect to data complexity based on the motivation that typical mappings and integrity constraints found in data integration are not captured by peer data exchange we extend the framework to incorporate these features one of the main difficulties is that the constraints of this new class are not amenable to materialization we address this issue by resorting to suitable combination of virtual and materialized data exchange showing that the resulting framework is generalization of both classical data exchange and classical data integration and that the new setting incorporates the most expressive types of mapping and constraints considered in the two contexts finally we present algorithms for all the relevant data management tasks also in the new setting and show that again their data complexity is polynomial
radiance transfer represents how generic source lighting is shadowed and scattered by an object to produce view dependent appearance we generalize by rendering transfer at two scales macro scale is coarsely sampled over an object's surface providing global effects like shadows cast from an arm onto body meso scale is finely sampled over small patch to provide local texture low order spherical harmonics represent low frequency lighting dependence for both scales to render coefficient vector representing distant source lighting is first transformed at the macro scale by matrix at each vertex of coarse mesh the resulting vectors represent spatially varying hemisphere of lighting incident to the meso scale function called radiance transfer texture rtt then specifies the surface's meso scale response to each lighting basis component as function of spatial index and view direction finally dot product of the macro scale result vector with the vector looked up from the rtt performs the correct shading integral we use an id map to place rtt samples from small patch over the entire object only two scalars are specified at high spatial resolution results show that bi scale decomposition makes preprocessing practical and efficiently renders self shadowing and interreflection effects from dynamic low frequency light sources at both scales
we present multiple pass streaming algorithms for basic clustering problem for massive data sets if our algorithm is allotted passes it will produce an approximation with error at most epsilon using otilde epsilon bits of memory the most critical resource for streaming computation we demonstrate that this tradeoff between passes and memory allotted is intrinsic to the problem and model of computation by proving lower bounds on the memory requirements of any pass randomized algorithm that are nearly matched by our upper bounds to the best of our knowledge this is the first time nearly matching bounds have been proved for such an exponential tradeoff for randomized computationin this problem we are given set of points drawn randomly according to mixture of uniform distributions and wish to approximate the density function of the mixture the points are placed in datastream possibly in adversarial order which may only be read sequentially by the algorithm we argue that this models among others the datastream produced by national census of the incomes of all citizens
we present unified feature representation of pointclouds and apply it to face recognition the representation integrates local and global geometrical cues in single compact representation which makes matching probe to large database computationally efficient the global cues provide geometrical coherence for the local cues resulting in better descriptiveness of the unified representation multiple rank tensors scalar features are computed at each point from its local neighborhood and from the global structure of the pointcloud forming multiple rank tensor fields the pointcloud is then represented by the multiple rank tensor fields which are invariant to rigid transformations each local tensor field is integrated with every global field in histogram which is indexed by local field in one dimension and global field in the other dimension finally pca coefficients of the histograms are concatenated into single feature vector the representation was tested on frgc data set and achieved identification and verification rate at far
we present method for learning model of human body shape variation from corpus of range scans our model is the first to capture both identity dependent and pose dependent shape variation in correlated fashion enabling creation of variety of virtual human characters with realistic and non linear body deformations that are customized to the individual our learning method is robust to irregular sampling in pose space and identity space and also to missing surface data in the examples our synthesized character models are based on standard skinning techniques and can be rendered in real time
unconstrained consumer photos pose great challenge for content based image retrieval unlike professional images or domain specific images consumer photos vary significantly more often than not the objects in the photos are ill posed occluded and cluttered with poor lighting focus and exposure in this paper we propose cascading framework for combining intra image and inter class similarities in image retrieval motivated from probabilistic bayesian principles support vector machines are employed to learn local view based semantics based on just in time fusion of color and texture features new detection driven block based segmentation algorithm is designed to extract semantic features from images the detection based indexes also serve as input for support vector learning of image classifiers to generate class relative indexes during image retrieval both intra image and inter class similarities are combined to rank images experiments using query by example on genuine heterogeneous consumer photos with semantic queries show that the combined matching approach is better than matching with single index it also outperformed the method of combining color and texture features by in average precision
compiler based auto parallelization is much studied area yet has still not found wide spread application this is largely due to the poor exploitation of application parallelism subsequently resulting in performance levels far below those which skilled expert programmer could achieve we have identified two weaknesses in traditional parallelizing compilers and propose novel integrated approach resulting in significant performance improvements of the generated parallel code using profile driven parallelism detection we overcome the limitations of static analysis enabling us to identify more application parallelism and only rely on the user for final approval in addition we replace the traditional target specific and inflexible mapping heuristics with machine learning based prediction mechanism resulting in better mapping decisions while providing more scope for adaptation to different target architectures we have evaluated our parallelization strategy against the nas and spec omp benchmarks and two different multi core platforms dual quad core intel xeon smp and dual socket qs cell blade we demonstrate that our approach not only yields significant improvements when compared with state of the art parallelizing compilers but comes close to and sometimes exceeds the performance of manually parallelized codes on average our methodology achieves of the performance of the hand tuned openmp nas and spec parallel benchmarks on the intel xeon platform and gains significant speedup for the ibm cell platform demonstrating the potential of profile guided and machine learning based parallelization for complex multi core platforms
we present new variational method for multi view stereovision and non rigid three dimensional motion estimation from multiple video sequences our method minimizes the prediction error of the shape and motion estimates both problems then translate into generic image registration task the latter is entrusted to global measure of image similarity chosen depending on imaging conditions and scene properties rather than integrating matching measure computed independently at each surface point our approach computes global image based matching score between the input images and the predicted images the matching process fully handles projective distortion and partial occlusions neighborhood as well as global intensity information can be exploited to improve the robustness to appearance changes due to non lambertian materials and illumination changes without any approximation of shape motion or visibility moreover our approach results in simpler more flexible and more efficient implementation than in existing methods the computation time on large datasets does not exceed thirty minutes on standard workstation finally our method is compliant with hardware implementation with graphics processor units our stereovision algorithm yields very good results on variety of datasets including specularities and translucency we have successfully tested our motion estimation algorithm on very challenging multi view video sequence of non rigid scene
interfaces based on recognition technologies are used extensively in both the commercial and research worlds but recognizers are still error prone and this results in human performance problems brittle dialogues and other barriers to acceptance and utility of recognition systems interface techniques specialized to recognition systems can help reduce the burden of recognition errors but building these interfaces depends on knowledge about the ambiguity inherent in recognition we have extended user interface toolkit in order to model and to provide structured support for ambiguity at the input event level this makes it possible to build re usable interface components for resolving ambiguity and dealing with recognition errors these interfaces can help to reduce the negative effects of recognition errors by providing these components at toolkit level we make it easier for application writers to provide good support for error handling further with this robust support we are able to explore new types of interfaces for resolving more varied range of ambiguity
we present an efficient approach for end to end out of core construction and interactive inspection of very large arbitrary surface models the method tightly integrates visibility culling and out of core data management with level of detail framework at preprocessing time we generate coarse volume hierarchy by binary space partitioning the input triangle soup leaf nodes partition the original data into chunks of fixed maximum number of triangles while inner nodes are discretized into fixed number of cubical voxels each voxel contains compact direction dependent approximation of the appearance of the associated volumetric sub part of the model when viewed from distance the approximation is constructed by visibility aware algorithm that fits parametric shaders to samples obtained by casting rays against the full resolution dataset at rendering time the volumetric structure maintained off core is refined and rendered in front to back order exploiting vertex programs for gpu evaluation of view dependent voxel representations hardware occlusion queries for culling occluded subtrees and asynchronous for detecting and avoiding data access latencies since the granularity of the multiresolution structure is coarse data management traversal and occlusion culling cost is amortized over many graphics primitives the efficiency and generality of the approach is demonstrated with the interactive rendering of extremely complex heterogeneous surface models on current commodity graphics platforms
paraphrasing van rijsbergen the time is ripe for another attempt at using natural language processing nlp for information retrieval ir this paper introduces my dissertation study which will explore methods for integrating modern nlp with state of the art ir techniques in addition to text will also apply retrieval to conversational speech data which poses unique set of considerations in comparison to text greater use of nlp has potential to improve both text and speech retrieval
on line analytical processing olap is technology basically created to provide users with tools in order to explore and navigate into data cubes unfortunately in huge and sparse data exploration becomes tedious task and the simple user's intuition or experience does not lead to efficient results in this paper we propose to exploit the results of the multiple correspondence analysis mca in order to enhance data cube representations and make them more suitable for visualization and thus easier to analyze our approach addresses the issues of organizing data in an interesting way and detects relevant facts our purpose is to help the interpretation of multidimensional data by efficient and simple visual effects to validate our approach we compute its efficiency by measuring the quality of resulting multidimensional data representations in order to do so we propose an homogeneity criterion to measure the visual relevance of data representations this criterion is based on the concept of geometric neighborhood and similarity between cells experimental results on real data have shown the interest of using our approach on sparse data cubes
there is growing demand for provisioning of different levels of quality of service qos on scalable web servers to meet changing resource availability and to satisfy different client requirements in this paper we investigate the problem of providing proportional qos differentiation with respect to response time on web servers we first present processing rate allocation scheme based on the foundations of queueing theory it provides different processing rates to requests of different client classes so as to achieve the differentiation objective at application level process is used as the resource allocation principal for achieving processing rates on apache web servers we design and implement an adaptive process allocation approach guided by the queueing theoretical rate allocation scheme on an apache server this application level implementation however shows weak qos predictability because it does not have fine grained control over the consumption of resources that the kernel consumes and hence the processing rate is not strictly proportional to the number of processes allocated we then design feedback controller and integrate it with the queueing theoretical approach it adjusts process allocations according to the difference between the target response time and the achieved response time using proportional integral derivative controller experimental results demonstrate that this integrated approach can enable web servers to provide robust proportional response time differentiation
we present hybridpointing technique that lets users easily switch between absolute and relative pointing with direct input device such as pen our design includes new graphical element the trailing widget which remains close at hand but does not interfere with normal cursor operation the use of visual feedback to aid the user's understanding of input state is discussed and several novel visual aids are presented an experiment conducted on large wall sized display validates the benefits of hybridpointing under certain conditions we also discuss other situations in which hybridpointing may be useful finally we present an extension to our technique that allows for switching between absolute and relative input in the middle of single drag operation
many adaptive routing algorithms have been proposed for wormhole routed interconnection networks comparatively little work however has been done on determining how the selection function routing policy affects the performance of an adaptive routing algorithm in this paper we present detailed simulation study of various selection functions for fully adaptive wormhole routing on two dimensional meshes the simulation results show that the choice of selection function has significant effect on the average message latency in addition it is possible to find single selection function that exhibits excellent performance across wide range of traffic patterns network sizes and number of virtual channels thus well chosen selection function for an adaptive routing algorithm can lead to consistently better performance than an arbitrary selection function one of the selection functions considered is theoretically optimal selection function ieee trans comput october we show that although theoretically optimal the actual performance of the optimal selection function is not always best an explanation and interpretation of the results is provided
imperative and object oriented programs make ubiquitous use of shared mutable objects updating shared object can and often does transgress boundary that was supposed to be established using static constructs such as class with private fields this paper shows how auxiliary fields can be used to express two state dependent encapsulation disciplines ownership kind of separation and friendship kind of sharing methodology is given for specification and modular verification of encapsulated object invariants and shown sound for class based language as an example the methodology is used to specify iterators which are problematic for previous ownership systems
energy consumption is of significant concern in battery operated embedded systems in the processors of such systems the instruction cache consumes significant fraction of the total energy one of the most popular methods to reduce the energy consumption is to shut down idle cache banks however we observe that operating idle cache banks at reduced voltage frequency level along with the active banks in pipelined manner can potentially achieve even better energy savings in this paper we propose novel dvs based pipelined reconfigurable instruction memory hierarchy called prim canonical example of our proposed prim consists of four cache banks two of these cache banks can be configured at runtime to operate at lower voltage and frequency levels than that of the normal cache instruction fetch throughput is maintained by pipelining the accesses to the low voltage banks we developed profile driven compilation framework that analyzes applications and inserts the appropriate cache reconfiguration points our experimental results show that prim can significant reduce the energy consumption for popular embedded benchmarks with minimal performance overhead we obtained and energy savings for aggressive and conservative vdd settings respectively at the expense of performance overhead
software defect prediction is important for reducing test times by allocating testing resources effectively in terms of predicting the defects in software naive bayes outperforms wide range of other methods however naive bayes assumes the independence and equal importance of attributes in this work we analyze these assumptions of naive bayes using public software defect data from nasa our analysis shows that independence assumption is not harmful for software defect data with pca pre processing our results also indicate that assigning weights to static code attributes may increase the prediction performance significantly while removing the need for feature subset selection
this paper presents fundamentally new approach to integrating local decisions from various nodes and efficiently routing data in sensor networks by classifying the nodes in the sensor field as hot or cold in accordance with whether or not they sense the target we are able to concentrate on smaller set of nodes and gear the routing of data to and from the sink to fraction of the nodes that exist in the network the introduction of this intermediary step is fundamentally new and allows for efficient and meaningful fusion and routing this is made possible through the use of novel markov random field mrf approach which to the best of our knowledge has never been applied to sensor networks in combination with maximum posteriori probability map stochastic relaxation tools to flag out the hot nodes in the network and to optimally combine their data and decisions towards an integrated and collaborative global decision fusion this global decision supersedes all local decisions and provides the basis for efficient use of the sensed data because of the mrf local nature nodes need not see or interact with other nodes in the sensor network beyond their immediate neighborhood which can either be defined in terms of distance between nodes or communication connectivity hence adding to the flexibility of dealing with irregular and varying sensor topologies and also minimizing node power usage and providing for easy scalability the routing of the hot nodes data is confined to cone of nodes and power constraints are taken into account we also use the found location of the centroid of the hot nodes over time to track the movement of the target this is achieved by using the segmentation at time as an initial state in the stochastic map relaxation at time dt
pen gesture interfaces have difficulty supporting arbitrary multiple stroke selections because lifting the pen introduces ambiguity as to whether the next stroke should add to the existing selection or begin new one we explore and evaluate techniques that use non preferred hand button or touchpad to phrase together one or more independent pen strokes into unitary multi stroke gesture we then illustrate how such phrasing techniques can support multiple stroke selection gestures with tapping crossing lassoing disjoint selection circles of exclusion selection decorations and implicit grouping operations these capabilities extend the expressiveness of pen gesture interfaces and suggest new directions for multiple stroke pen input techniques
in this paper we propose an image completion algorithm which takes advantage of the countless number of images available on internet photo sharing sites to replace occlusions in an input image the algorithm automatically selects the most suitable images from database of downloaded images and seamlessly completes the input image using the selected images with minimal user intervention experimental results on input images captured at various locations and scene conditions demonstrate the effectiveness of the proposed technique in seamlessly reconstructing user defined occlusions
we extend distributed database query optimization techniques to support database programming language language much richer than relational query languages with the richness comes difficulties eg how to recognize joins and how to handle aliases in this paper we describe our techniques dataflow analysis abstract evaluation partial evaluation and rewriting also we overview the algorithm that uses these techniques
this paper considers new security protocol paradigm whereby principals negotiate and on the fly generate security protocols according to their needs when principals wish to interact then rather than offering each other fixed menu of known protocols they negotiate and possibly with the collaboration of other principles synthesise new protocol that is tailored specifically to their current security environment and requirements this approach provides basis for autonomic security protocols such protocols are self configuring since only principal assumptions and protocol goals need to be priori configured the approach has the potential to survive security compromises that can be modelled as changes in the beliefs of the principals compromise of key or change in the trust relationships between principals can result in principal self healing and synthesising new protocol to survive the event
bilingual documentation has become common phenomenon in official institutions and private companies in this scenario the categorization of bilingual text is useful tool in this paper different approaches will be proposed to tackle this bilingual classification task on the one hand three finite state transducer algorithms from the grammatical inference framework will be presented on the other hand naive combination of smoothed gram models will be introduced to evaluate the performance of bilingual classifiers two categorized bilingual corpora of different complexity were considered experiments in limited domain task show that all the models obtain similar results however results on more open domain task denote the supremacy of the naive approach
code generation for embedded processors opens up the possibility for several performance optimization techniques that have been ignored by traditional compilers due to compilation time constraints we present techniques that take into account the parameters of the data caches for organizing scalar and array variables declared in embedded code into memory with the objective of improving data cache performance we present techniques for clustering variables to minimize compulsory cache misses and for solving the memory assignment problem to minimize conflict cache misses our experiments with benchmark code kernels from dsp and other domains on the cw embedded processor from lsi logic indicate significant improvements in data cache performance by the application of our memory organization technique
one purpose of software metrics is to measure the quality of programs the results can be for example used to predict maintenance costs or improve code quality an emerging view is that if software metrics are going to be used to improve quality they must help in finding code that should be refactored often refactoring or applying design pattern is related to the role of the class to be refactored in client based metrics project gives the class context these metrics measure how class is used by other classes in the context we present new client based metric lcic lack of coherence in clients which analyses if the class being measured has coherent set of roles in the program interfaces represent the roles of classes if class does not have coherent set of roles it should be refactored or new interface should be defined for the class we have implemented tool for measuring the metric lcic for java projects in the eclipse environment we calculated lcic values for classes of several open source projects we compare these results with results of other related metrics and inspect the measured classes to find out what kind of refactorings are needed we also analyse the relation of different design patterns and refactorings to our metric our experiments reveal the usefulness of client based metrics to improve the quality of code
ranked queries return the top objects of database according to preference function we present and evaluate experimentally and theoretically core algorithm that answers ranked queries in an efficient pipelined manner using materialized ranked views we use and extend the core algorithm in the described prefer and merge systems prefer precomputes set of materialized views that provide guaranteed query performance we present an algorithm that selects near optimal set of views under space constraints we also describe multiple optimizations and implementation aspects of the downloadable version of prefer then we discuss merge which operates at metabroker and answers ranked queries by retrieving minimal number of objects from sources that offer ranked queries speculative version of the pipelining algorithm is described
the geographic routing is an ideal approach to realize pointto point routing in wireless sensor networks because packets can be delivered by only maintaining small set of neighbors physical positions the geographic routing assumes that packet can be moved closer to the destination in the network topology if it is moved geographically closer to the destination in the physical space this assumption however only holds in an ideal model where uniformly distributed nodes communicate with neighbors through wireless channels with perfect reception because this model oversimplifies the spatial complexity of wireless sensor network the geographic routing may often lead packet to the local minimum or low quality route unlike the geographic forwarding the etx embedding proposed in this paper can accurately encode both network's topological structure and channel quality to small size nodes virtual coordinates which makes it possible for greedy forwarding to guide packet along an optimal routing path our performance evaluation based on both the mica sensor platform and tossim simulator shows that the greedy forwarding based on etx embedding outperforms previous geographic routing approaches
soft real time systems can tolerate some occasional deadline misses this feature provides unique opportunity to reduce system's energy consumption in this paper we study the system with firm deadline popular model for soft real time systems it basically requires at least successful completions in any consecutive executions our goal is to design such system with dual supply voltages for energy efficiency to reach this goal we first propose an on line greedy deterministic scheduler that provides the firm guarantee with the provably minimum energy consumption we then develop novel exact method to compute the scheduler's average energy consumption per iteration this leads us to the numerical solution to the voltage set up problem which seeks for the values of the two supply voltages to achieve the most energy efficiency with firm guarantee simulation shows that dual voltage system can reduce significant amount of energy over single voltage system our numerical method finds the best voltage set ups in seconds while it takes hours to obtain almost identical solutions by simulation
in this paper we describe how the memory management mechanisms of the intel iapx are used to implement the visibility rules of ada at any point in the execution of an ada reg program on the the program has protected address space that corresponds exactly to the program's accessibility at the corresponding point in the program's source this close match of architecture and language did not occur because the was designed to execute ada mdash it was not rather both ada and the are the result of very similar design goals to illustrate this point we compare in their support for ada the memory management mechanisms of the to those of traditional computers the most notable differences occur in heap space management and multitasking with respect to the former we describe degree of hardware software cooperation that is not typical of other systems in the latter area we show how ada's view of sharing is the same as the but differs totally from the sharing permitted by traditional systems description of these differences provide some insight into the problems of implementing an ada compiler for traditional architecture
the diffusion of mobile devices in the working landscape is promoting collaboration across time and space following through this development we investigate opportunities for improving awareness in mobile environments with view to enable collaboration under power constraints and transitory network disconnections we elaborate in particular on synchronous cscw and expose with it significant details of group awareness while we contribute protocol for awareness support over large areas that strikes balance between energy consumption and notification time to avoid user disruption this protocol notifies awareness information in multicast fashion while the bandwidth is allocated dynamically among notifications and data requests thus minimizing the time needed by each one of them and ensuring the isochronous delivery of information to all clients the efficiency and scalability of our protocol are evaluated with simulation experiments whereby we compare various notification schemes and finally choose one that changes dynamically over time
the advances in wireless and mobile computing allow mobile user to perform wide range of aplications once limited to non mobile hard wired computing environments as the geographical position of mobile user is becoming more trackable users need to pull data which are related to their location perhaps seeking information about unfamiliar places or local lifestyle data in these requests location attribute has to be identified in order to provide more efficient access to location dependent data whose value is determined by the location to which it is related local yellow pages local events and weather information are some of the examples of these data in this paper we give formalization of location relatedness in queries we differentiate location dependence and location awareness and provide thorough examples to support our approach
power consumption is major factor that limits the performance of computers we survey the ldquo state of the art rdquo in techniques that reduce the total power consumed by microprocessor system over time these techniques are applied at various levels ranging from circuits to architectures architectures to system software and system software to applications they also include holistic approaches that will become more important over the next decade we conclude that power management is multifaceted discipline that is continually expanding with new techniques being developed at every level these techniques may eventually allow computers to break through the ldquo power wall rdquo and achieve unprecedented levels of performance versatility and reliability yet it remains too early to tell which techniques will ultimately solve the power problem
automatic localisation of correspondences for the construction of statistical shape models from examples has been the focus of intense research during the last decade several algorithms are available and benchmarking is needed to rank the different algorithms prior work has argued that the quality of the models produced by the algorithms can be evaluated by measuring compactness generality and specificity in this paper severe problems with these standard measures are analysed both theoretically and experimentally both on natural and synthetic datasets we also propose that ground truth correspondence measure gcm is used for benchmarking and in this paper benchmarking is performed on several state of the art algorithms using seven real and one synthetic dataset
we present core calculus with two of key constructs for parallelism namely async and finish our calculus forms convenient basis for type systems and static analyses for languages with async finish parallelism and for tractable proofs of correctness for example we give short proof of the deadlock freedom theorem of saraswat and jagadeesan our main contribution is type system that solves the open problem of context sensitive may happen in parallel analysis for languages with async finish parallelism we prove the correctness of our type system and we report experimental results of performing type inference on lines of code our analysis runs in polynomial time takes total of seconds on our benchmarks and produces low number of false positives which suggests that our analysis is good basis for other analyses such as race detectors
we present efficient fixed parameter algorithms for the np complete edge modification problems cluster editing and cluster deletion here the goal is to make the fewest changes to the edge set of an input graph such that the new graph is vertex disjoint union of cliques allowing up to edge additions and deletions cluster editing we solve this problem in time allowing only up to edge deletions cluster deletion we solve this problem in time the key ingredients of our algorithms are two easy to implement bounded search tree algorithms and reduction to problem kernel of size this improves and complements previous work
data integration is the problem of combining data residing at different autonomous heterogeneous sources and providing the client with unified reconciled global view of the data we discuss data integration systems taking the abstract viewpoint that the global view is an ontology expressed in class based formalism we resort to an expressive description logic alcqi that fully captures class based representation formalisms and we show that query answering in data integration as well as all other relevant reasoning tasks is decidable however when we have to deal with large amounts of data the high computational complexity in the size of the data makes the use of full fledged expressive description logic infeasible in practice this leads us to consider dl lite specifically tailored restriction of alcqi that ensures tractability of query answering in data integration while keeping enough expressive power to capture the most relevant features of class based formalisms
novel technique is proposed for the management of reconfigurable device in order to get true hardware multitasking we use vertex list set to keep track of the free area boundary this structure contains the best candidate locations for the task and several heuristics are proposed to select one of them based in fragmentation and adjacency look ahead heuristic that anticipates the next known event is also proposed metric is used to estimate the fragmentation status of the fpga based on the number of holes and their shape defragmentation measures are taken when needed
labeling text data is quite time consuming but essential for automatic text classification especially manually creating multiple labels for each document may become impractical when very large amount of data is needed for training multi label text classifiers to minimize the human labeling efforts we propose novel multi label active learning approach which can reduce the required labeled data without sacrificing the classification accuracy traditional active learning algorithms can only handle single label problems that is each data is restricted to have one label our approach takes into account the multi label information and select the unlabeled data which can lead to the largest reduction of the expected model loss specifically the model loss is approximated by the size of version space and the reduction rate of the size of version space is optimized with support vector machines svm an effective label prediction method is designed to predict possible labels for each unlabeled data point and the expected loss for multi label data is approximated by summing up losses on all labels according to the most confident result of label prediction experiments on several real world data sets all are publicly available demonstrate that our approach can obtain promising classification result with much fewer labeled data than state of the art methods
this paper introduces novel technique for joint surface reconstruction and registration given set of roughly aligned noisy point clouds it outputs noise free and watertight solid model the basic idea of the new technique is to reconstruct prototype surface at increasing resolution levels according to the registration accuracy obtained so far and to register all parts with this surface we derive non linear optimization problem from bayesian formulation of the joint estimation problem the prototype surface is represented as partition of unity implicit surface which is constructed from piecewise quadratic functions defined on octree cells and blended together using spline basis functions allowing the representation of objects with arbitrary topology with high accuracy we apply the new technique to set of standard data sets as well as especially challenging real world cases in practice the novel prototype surface based joint reconstruction registration algorithm avoids typical convergence problems in registering noisy range scans and substantially improves the accuracy of the final output
we study traffic measurement issue for active queue management and dynamic bandwidth allocation at single network node under the constraint of cell loss probability clp or buffer overflow probability using the concept of measurement based virtual queue vq and frequency domain traffic filtering we propose an online algorithm to estimate the real time bandwidth demand under both short and long term cell loss constraint the algorithm is adaptive and robust to the piece wise stationary traffic dynamics the vq runs in parallel to the real queueing system and monitors the latter in non intrusive way it captures proper traffic sampling interval tc simulation and analysis show its critical role in achieving higher utilization of bandwidth and buffer resource without violating qos requirement given an appropriate tc we argue that network controls such as the call admission control cac and dynamic bandwidth allocation are facilitated with accurate information about traffic loading and qos status at the smallest timescale
two methods have been used extensively to model resting contact for rigid body simulation the first approach the penalty method applies virtual springs to surfaces in contact to minimize interpenetration this method as typically implemented results in oscillatory behavior and considerable penetration the second approach based on formulating resting contact as linear complementarity problem determines the resting contact forces analytically to prevent interpenetration the analytical method exhibits expected case polynomial complexity in the number of contact points and may fail to find solution in polynomial time when friction is modeled we present fast penalty method that minimizes oscillatory behavior and leads to little penetration during resting contact our method compares favorably to the analytical method with regard to these two measures while exhibiting much faster performance both asymptotically and empirically
this paper proposes cluster based peer to peer system called peercluster for sharing data over the internet in peercluster all participant computers are grouped into various interest clusters each of which contains computers that have the same interests the intuition behind the system design is that by logically grouping users interested in similar topics together we can improve query efficiency to efficiently route and broadcast messages across within interest clusters hypercube topology is employed in addition to ensure that the structure of the interest clusters is not altered by arbitrary node insertions deletions we have devised corresponding join and leave protocols the complexities of these protocols are analyzed moreover we augment peercluster with system recovery mechanism to make it robust against unpredictable computer network failures using an event driven simulation we evaluate the performance of our approach by varying several system parameters the experimental results show that peercluster outperforms previous approaches in terms of query efficiency while still providing the desired functionality of keyword based search
grid resource management has been traditionally limited to just two levels of hierarchy namely local resource managers and metaschedulers this results in non manageable and thus not scalable architecture where each metascheduler has to be able to access thousands of resources which also implies having detailed knowledge about their interfaces and configuration this paper presents recursive architecture allowing an arbitrary number of levels in the hierarchy this way resources can be arranged in different ways for example following organizational boundaries or aggregating them by similarity while hiding the access details an implementation of this architecture is shown as well as its benefits in terms of autonomy scalability deployment and security the proposed implementation is based on existing interfaces allowing for standardization
tail calls are expected not to consume stack space in most functional languages however there is no support for tail calls in some environments even in such environments proper tail calls can be implemented with technique called trampoline to reduce the overhead of trampolining while preserving stack space asymptotically we propose selective tail call elimination based on an effect system the effect system infers the number of successive tail calls generated by the execution of an expression and trampolines are introduced only when they are necessary
automatic processing of medical dictations poses significant challenge we approach the problem by introducing statistical framework capable of identifying types and boundaries of sections lists and other structures occurring in dictation thereby gaining explicit knowledge about the function of such elements training data is created semi automatically by aligning parallel corpus of corrected medical reports and corresponding transcripts generated via automatic speech recognition we highlight the properties of our statistical framework which is based on conditional random fields crfs and implemented as an efficient publicly available toolkit finally we show that our approach is effective both under ideal conditions and for real life dictation involving speech recognition errors and speech related phenomena such as hesitation and repetitions
this paper concerns construction of additive stretched spanners with few edges for vertex graphs having tree decomposition into bags of diameter at most ie the tree length graphs for such graphs we construct additive spanners with dn nlogn edges and additive spanners with dn edges this provides new upper bounds for chordal graphs for which we also show lower bound and prove that there are graphs of tree length for which every multiplicative spanner and thus every additive spanner requires edges
sequential sat solver satori was recently proposed as an alternative to combinational sat in verification applications this paper describes the design of seq sat an efficient sequential sat solver with improved search strategies over satori the major improvements include new and better heuristic for minimizing the set of assignments to state variables new priority based search strategy and flexible sequential search framework which integrates different search strategies and decision variable selection heuristic more suitable for solving the sequential problems we present experimental results to demonstrate that our sequential sat solver can achieve orders of magnitude speedup over satori we plan to release the source code of seq sat along with this paper
finding frequent patterns in continuous stream of transactions is critical for many applications such as retail market data analysis network monitoring web usage mining and stock market prediction even though numerous frequent pattern mining algorithms have been developed over the past decade new solutions for handling stream data are still required due to the continuous unbounded and ordered sequence of data elements generated at rapid rate in data stream therefore extracting frequent patterns from more recent data can enhance the analysis of stream data in this paper we propose an efficient technique to discover the complete set of recent frequent patterns from high speed data stream over sliding window we develop compact pattern stream tree cps tree to capture the recent stream data content and efficiently remove the obsolete old stream data content we also introduce the concept of dynamic tree restructuring in our cps tree to produce highly compact frequency descending tree structure at runtime the complete set of recent frequent patterns is obtained from the cps tree of the current window using an fp growth mining technique extensive experimental analyses show that our cps tree is highly efficient in terms of memory and time complexity when finding recent frequent patterns from high speed data stream
initial algebra semantics is cornerstone of the theory of modern functional programming languages for each inductive data type it provides fold combinator encapsulating structured recursion over data of that type church encoding build combinator which constructs data of that type and fold build rule which optimises modular programs by eliminating intermediate data of that type it has long been thought that initial algebra semantics is not expressive enough to provide similar foundation for programming with nested types specifically the folds have been considered too weak to capture commonly occurring patterns of recursion and no church encodings build combinators or fold build rules have been given for nested types this paper overturns this conventional wisdom by solving all of these problems
existing test suite reduction techniques employed for testing web applications have either used traditional program coverage based requirements or usage based requirements in this paper we explore three different strategies to integrate the use of program coverage based requirements and usage based requirements in relation to test suite reduction for web applications we investigate the use of usage based test requirements for comparison of test suites that have been reduced based on program coverage based test requirements we examine the effectiveness of test suite reduction process based on combination of both usage based and program coverage based requirements finally we modify popular test suite reduction algorithm to replace part of its test selection process with selection based on usage based test requirements our case study suggests that integrating program coverage based and usage based test requirements has positive impact on the effectiveness of the resulting test suites
this paper demonstrates the use of model based evaluation approach for instrumentation systems iss the overall objective of this study is to provide early feedback to tool developers regarding is overhead and performance such feedback helps developers make appropriate design decisions about alternative system configurations and task scheduling policies we consider three types of system architectures network of workstations now symmetric multiprocessors smp and massively parallel processing mpp systems we develop resource occupancy rocc model for an on line is for an existing tool and parameterize it for an ibm sp platform this model is simulated to answer several what if questions regarding two policies to schedule instrumentation data forwarding collect and forward cf and batch and forward bf in addition this study investigates two alternatives for forwarding the instrumentation data direct and binary tree forwarding for an mpp system simulation results indicate that the bf policy can significantly reduce the overhead and that the tree forwarding configuration exhibits desirable scalability characteristics for mpp systems initial measurement based testing results indicate more than percent reduction in the direct is overhead when the bf policy was added to paradyn parallel performance measurement tool
database optimizers require statistical information about data distributions in order to evaluate result sizes and access plan costs for processing user queries in this context we consider the problem of estimating the size of the projections of database relation when measures on attribute domain cardinalities are maintained in the system our main theoretical contribution is new formal model ad valid under the hypotheses of attribute independence and uniform distribution of attribute values derived considering the difference between time invariant domain the set of values that an attribute can assume and time dependent active domain the set of values that are actually assumed at certain time early models developed under the same assumptions are shown to be formally incorrect since the ad model is computationally high demanding we also introduce an approximate easy to compute model ad that unlike previous approximations yields low errors on all the parameter space of the active domain cardinalities finally we extend the ad model to the case of nonuniform distributions and present experimental results confirming the good behavior of the model
in the past years research on inductive inference has developed along different lines eg in the formalizations used and in the classes of target concepts considered one common root of many of these formalizations is gold's model of identification in the limit this model has been studied for learning recursive functions recursively enumerable languages and recursive languages reflecting different aspects of machine learning artificial intelligence complexity theory and recursion theory one line of research focuses on indexed families of recursive languages classes of recursive languages described in representation scheme for which the question of membership for any string in any of the given languages is effectively decidable with uniform procedure such language classes are of interest because of their naturalness the survey at hand picks out important studies on learning indexed families including basic as well as recent research summarizes and illustrates the corresponding results and points out links to related fields such as grammatical inference machine learning and artificial intelligence in general
two complications frequently arise in real world applications motion and the contamination of data by outliers we consider fundamental clustering problem the center problem within the context of these two issues we are given finite point set of size and an integer in the standard center problem the objective is to compute set of center points to minimize the maximum distance from any point of to its closest center or equivalently the smallest radius such that can be covered by disks of this radius in the discrete center problem the disk centers are drawn from the points of and in the absolute center problem the disk centers are unrestricted we generalize this problem in two ways first we assume that points are in continuous motion and the objective is to maintain solution over time second we assume that some given robustness parameter
existing works on processing of extensible markup language xml documents have been concentrated on query optimisation storage problems documents transformation compressing methods and normalisation there are only few papers on concurrency control in accessing and modifying xml documents which are stored in xml database systems the aim of this paper is to analyse and compare the quantity of concurrency control methods for xml database systems based on dom api
we compare the performance of three usual allocations namely max min fairness proportional fairness and balanced fairness in communication network whose resources are shared by random number of data flows the model consists of network of processor sharing queues the vector of service rates which is constrained by some compact convex capacity set representing the network resources is function of the number of customers in each queue this function determines the way network resources are allocated we show that this model is representative of rich class of wired and wireless networks we give in this general framework the stability condition of max min fairness proportional fairness and balanced fairness and compare their performance on number of toy networks
context uncertainty is an unavoidable issue in software engineering and an important area of investigation this paper studies the impact of uncertainty on total duration ie make span for implementing all features in operational release planning objective the uncertainty factors under investigation are the number of new features arriving during release construction the estimated effort needed to implement features the availability of developers and the productivity of developers method an integrated method is presented combining monte carlo simulation to model uncertainty in the operational release planning orp process with process simulation to model the orp process steps and their dependencies as well as an associated optimization heuristic representing an organization specific staffing policy for make span minimization the method allows for evaluating the impact of uncertainty on make span the impact of uncertainty factors both in isolation and in combination are studied in three different pessimism levels through comparison with baseline plan initial evaluation of the method is done by an explorative case study at chartwell technology inc to demonstrate its applicability and its usefulness results the impact of uncertainty on release make span increases both in terms of magnitude and variance with an increase of pessimism level as well as with an increase of the number of uncertainty factors among the four uncertainty factors we found that the strongest impact stems from the number of new features arriving during release construction we have also demonstrated that for any combination of uncertainty factors their combined ie simultaneous impact is bigger than the addition of their individual impacts conclusion the added value of the presented method is that managers are able to study the impact of uncertainty on existing ie baseline operational release plans pro actively
in this paper an approach for the implementation of quality based web search engine is proposed quality retrieval is introduced and an overview on previous efforts to implement such service is given machine learning approaches are identified as the most promising methods to determine the quality of web pages features for the most appropriate characterization of web pages are determined quality model is developed based on human judgments this model is integrated into meta search engine which assesses the quality of all results at run time the evaluation results show that quality based ranking does lead to better results concerning the perceived quality of web pages presented in the result set the quality models are exploited to identify potentially important features and characteristics for the quality of web pages
in this work we present new in network techniques for communication efficient approximate query processing in wireless sensornets we use model based approach that constructs and maintains spanning tree within the network rooted at the basestation the tree maintains compressed summary information for each link that is used to stub out traversal during query processing our work is based on formal model of the in network tree construction task framed as an optimization problemwe demonstrate hardness results for that problem and develop efficient approximation algorithms for subtasks that are too expensive to compute exactly we also propose efficient heuristics to accommodate wider set of workloads and empirically evaluate their performance and sensitivity to model changes
in practice any database management system sometimes needs reorganization that is change in some aspect of the logical and or physical arrangement of database in traditional practice many types of reorganization have required denying access to database taking the database offline during reorganization taking database offline can be unacceptable for highly available hour database for example database serving electronic commerce or armed forces or for very large database solution is to reorganize online concurrently with usage of the database incrementally during users activities or interpretively this article is tutorial and survey on requirements issues and strategies for online reorganization it analyzes the issues and then presents the strategies which use the issues the issues most of which involve design trade offs include use of partitions the locus of control for the process that reorganizes background process or users activities reorganization by copying to newly allocated storage as opposed to reorganizing in place use of differential files references to data that has moved performance and activation of reorganization the article surveys online strategies in three categories of reorganization the first category maintenance involves restoring the physical arrangement of data instances without changing the database definition this category includes restoration of clustering reorganization of an index rebalancing of parallel or distributed data garbage collection for persistent storage and cleaning reclamation of space in log structured file system the second category involves changing the physical database definition topics include construction of indexes conversion between trees and linear hash files and redefinition eg splitting of partitions the third category involves changing the logical database definition some examples are changing column's data type changing the inheritance hierarchy of object classes and changing relationship from one to many to many to many the survey encompasses both research and commercial implementations and this article points out several open research topics as highly available or very large databases continue to become more common and more important in the world economy the importance of online reorganization is likely to continue growing
we have devised an algorithm for minimal placement of bank selections in partitioned memory architectures this algorithm is parameterizable for chosen metric such as speed space or energy bank switching is technique that increases the code and data memory in microcontrollers without extending the address buses given program in which variables have been assigned to data banks we present novel optimization technique that minimizes the overhead of bank switching through cost effective placement of bank selection instructions the placement is controlled by number of different objectives such as runtime low power small code size or combination of these parameters we have formulated the minimal placement of bank selection instructions as discrete optimization problem that is mapped to partitioned boolean quadratic programming pbqp problem we implemented the optimization as part of pic microchip backend and evaluated the approach for several optimization objectives our benchmark suite comprises programs from mibench and dspstone plus microcontroller real time kernel and drivers for microcontroller hardware devices our optimization achieved reduction in program memory space of between and percent and an overall improvement with respect to instruction cycles between and percent our optimization achieved the minimal solution for all benchmark programs we investigated the scalability of our approach toward the requirements of future generations of microcontrollers this study was conducted as worst case analysis on the entire mibench suite our results show that our optimization scales well to larger numbers of memory banks scales well to the larger problem sizes that will become feasible with future microcontrollers and achieves minimal placement for more than percent of all functions from mibench
we present new metric for routing in multi radio multi hop wireless networks we focus on wireless networks with stationary nodes such as community wireless networksthe goal of the metric is to choose high throughput path between source and destination our metric assigns weights to individual links based on the expected transmission time ett of packet over the link the ett is function of the loss rate and the bandwidth of the link the individual link weights are combined into path metric called weighted cumulative ett wcett that explicitly accounts for the interference among links that use the same channel the wcett metric is incorporated into routing protocol that we call multi radio link quality source routingwe studied the performance of our metric by implementing it in wireless testbed consisting of nodes each equipped with two wireless cards we find that in multi radio environment our metric significantly outperforms previously proposed routing metrics by making judicious use of the second radio
semantic annotations of web services can support the effective and efficient discovery of services and guide their composition into workflows at present however the practical utility of such annotations is limited by the small number of service annotations available for general use manual annotation of services is time consuming and thus expensive task so some means are required by which services can be automatically or semi automatically annotated in this paper we show how information can be inferred about the semantics of operation parameters based on their connections to other annotated operation parameters within tried and tested workflows because the data links in the workflows do not necessarily contain every possible connection of compatible parameters we can infer only constraints on the semantics of parameters we show that despite their imprecise nature these so called loose annotations are still of value in supporting the manual annotation task inspecting workflows and discovering services we also show that derived annotations for already annotated parameters are useful by comparing existing and newly derived annotations of operation parameters we can support the detection of errors in existing annotations the ontology used for annotation and in workflows the derivation mechanism has been implemented and its practical applicability for inferring new annotations has been established through an experimental evaluation the usefulness of the derived annotations is also demonstrated
we describe an ethnographic study that explores how low tech and new tech surfaces support participation and collaboration during workshop breakout session the low tech surfaces were post it notes and large sheets of paper the new tech surfaces were writeable walls and multi touch tabletop four groups used the different surfaces during three phases brief presentation of position papers and discussion of themes ii the creation of group presentation and iii report back session participation and collaboration varied depending on the physical technological and social factors at play when using the different surfaces we discuss why this is the case noting how new shareable surfaces may need to be constrained to invite participation in ways that are simply taken for granted because of their familiarity when using low tech materials
this paper addresses the pragmatics of web information systems wis by analysing their usage starting from classification of intentions we first present life cases which capture observations of user behaviour in reality we discuss the facets of life cases and present semi formal way for their documentation life cases can be used in pragmatic way to specify story space which is an important component of storyboard in second step we complement life cases by user models that are specified by various facets of actor profiles that are needed for them we analyse actor profiles and present semi formal way for their documentation we outline how these profiles can be used to specify actors which are an important component of storyboard finally we analyse contexts and the way they impact on life cases user models and the storyboard
we present method to align words in bitext that combines elements of traditional statistical approach with linguistic knowledge we demonstrate this approach for arabic english using an alignment lexicon produced by statistical word aligner as well as linguistic resources ranging from an english parser to heuristic alignment rules for function words these linguistic heuristics have been generalized from development corpus of parallel sentences our aligner ualign outperforms both the commonly used giza aligner and the state of the art leaf aligner on measure and produces superior scores in end to end statistical machine translation bleu points over giza and over leaf
this paper investigates efficient evaluation of database updates and presents procedural semantics for stratified update programs that extend stratified logic programs with bulk updates and hypothetical reasoning bulk rules with universal quantification in the body allow an arbitrary update to be applied simultaneously for every answer of an arbitrary query hypothetical reasoning is supported by testing the success or failure of an update the procedural semantics offers efficient goal dash oriented tabled evaluation of database updates it guarantees termination for function dash free stratified update programs and avoids repeated computation of identical subgoals
we consider computationally efficient incentive compatiblemechanisms that use the vcg payment scheme and study how well theycan approximate the social welfare in auction settings we present anovel technique for setting lower bounds on the approximation ratioof this type of mechanisms specifically for combinatorial auctionsamong submodular and thus also subadditive bidders we prove an lower bound which is close to the knownupper bound of and qualitatively higher than theconstant factor approximation possible from purely computationalpoint of view
embedded systems have been traditional area of strength in the research agenda of the university of california at berkeley in parallel to this effort pattern of graduate and undergraduate classes has emerged that is the result of distillation process of the research results in this paper we present the considerations that are driving our curriculum development and we review our undergraduate and graduate program in particular we describe in detail graduate class eecs design of embedded systems modeling validation and synthesis that has been taught for six years common feature of our education agenda is the search for fundamentals of embedded system science rather than embedded system design techniques an approach that today is rather unique
pointer analysis classic problem in software program analysis has emerged as an important problem to solve in design automation at time when complex designs specified in the form of code need to be synthesized or verified however precise pointer analysis algorithms that are both context and flow sensitive fscs have not been shown to scale in this paper we report new solution for fscs analysis which can evaluate the program states of all program points under billions of different calling paths our solution extends the recently proposed symbolic pointer analysis spa technology which exploits the efficiency of binary decision diagrams bdds with our new strategy of problem solving called superposed symbolic computation and its application on our generic pointer analysis framework we are able to report the first result on all spec benchmarks that completes context sensitive flow insensitive analysis in seconds and context sensitive flow sensitive analysis in minutes
we present novel technique called radiance scaling for the depiction of surface shape through shading it adjusts reflected light intensities in way dependent on both surface curvature and material characteristics as result diffuse shading or highlight variations become correlated to surface feature variations enhancing surface concavities and convexities this approach is more versatile compared to previous methods first it produces satisfying results with any kind of material we demonstrate results obtained with phong and ashikmin brdfs cartoon shading sub lambertian materials and perfectly reflective or refractive objects second it imposes no restriction on lighting environment it does not require dense sampling of lighting directions and works even with single light third it makes it possible to enhance surface shape through the use of precomputed radiance data such as ambient occlusion prefiltered environment maps or lit spheres our novel approach works in real time on modern graphics hardware
mobile nodes in some challenging network scenarios eg battlefield and disaster recovery scenarios suffer from intermittent connectivity and frequent partitions disruption tolerant network dtn technologies are designed to enable communications in such environments several dtn routing schemes have been proposed however not much work has been done on designing schemes that provide efficient information access in such challenging network scenarios in this paper we explore how content based information retrieval system can be designed for dtns there are three important design issues namely how data should be replicated and stored at multiple nodes how query is disseminated in sparsely connected networks and how query response is routed back to the issuing node we first describe how to select nodes for storing the replicated copies of data items we consider the random and the intelligent caching schemes in the random caching scheme nodes that are encountered first by data generating node are selected to cache the extra copies while in the intelligent caching scheme nodes that can potentially meet more nodes eg faster nodes are selected to cache the extra data copies the number of replicated data copies can be the same for all data items or varied depending on the access frequencies of the data items in this work we consider fixed proportional and square root replication schemes then we describe two query dissemination schemes copy selective query spraying wss scheme and hop neighborhood spraying lns scheme in the wss scheme nodes that can move faster are selected to cache the queries while in the lns scheme nodes that are within hops of querying node will cache the queries for message routing we use an enhanced prophet scheme where next hop node is selected only if its predicted delivery probability to the destination is higher than certain threshold we conduct extensive simulation studies to evaluate different combinations of the replication and query dissemination algorithms our results reveal that the scheme that performs the best is the one that uses the wss scheme combined with binary spread of replicated data copies the wss scheme can achieve higher query success ratio when compared to scheme that does not use any data and query replication furthermore the square root and proportional replication schemes provide higher query success ratio than the fixed copy approach with varying node density in addition the intelligent caching approach can further improve the query success ratio by with varying node density our results using different mobility models reveal that the query success ratio degrades at most when the community based model is used compared to the random waypoint rwp model broch et al performance comparison of multihop wireless ad hoc network routing protocols acm mobicom pp compared to the rwp and the community based mobility models the umassbusnet model from the dieselnet project zhang et al modeling of bus based disruption tolerant network trace proceedings of acm mobihoc achieves much lower query success ratio because of the longer inter node encounter time
we propose an automatic instrumentation method for embedded software annotation to enable performance modeling in high level hardware software co simulation environments the proposed cross annotation technique consists of extending retargetable compiler infrastructure to allow the automatic instrumentation of embedded software at the basic block level thus target and annotated native binaries are guaranteed to have isomorphic control flow graphs cfg the proposed method takes into account the processor specific optimizations at the compiler level and proves to be accurate with low simulation overhead
one approach to prolong the lifetime of wireless sensor network wsn is to deploy some relay nodes to communicate with the sensor nodes other relay nodes and the base stations the relay node placement problem for wireless sensor networks is concerned with placing minimum number of relay nodes into wireless sensor network to meet certain connectivity or survivability requirements previous studies have concentrated on the unconstrained version of the problem in the sense that relay nodes can be placed anywhere in practice there may be some physical constraints on the placement of relay nodes to address this issue we study constrained versions of the relay node placement problem where relay nodes can only be placed at set of candidate locations in the connected relay node placement problem we want to place minimum number of relay nodes to ensure that each sensor node is connected with base station through bidirectional path in the survivable relay node placement problem we want to place minimum number of relay nodes to ensure that each sensor node is connected with two base stations or the only base station in case there is only one base station through two node disjoint bidirectional paths for each of the two problems we discuss its computational complexity and present framework of polynomial time approximation algorithms with small approximation ratios extensive numerical results showthat our approximation algorithms can produce solutions very close to optimal solutions
we propose low leakage cache architecture based on the observation of the spatio temporal properties of data caches in particular we exploit the fact that during the program lifetime few data values tend to exhibit both spatial and temporal locality in cache ie values that are simultaneously stored by several lines at the same time leakage energy can be reduced by turning off those lines and storing these values in smaller separate memory in this work we introduce an architecture that implements such scheme as well as an algorithm to detect these special values we show that by using as few as four values we can achieve leakage energy savings with an additional reduction of dynamic energy as consequence of reduced average cache access cost
in november the fcc ruled that the digital tv whitespaces be used for unlicensed access this is an exciting development because dtv whitespaces are in the low frequency range mhz compared to typical cellular and ism bands thus resulting in much better propagation characteristics and much higher spectral efficiencies the fcc has also mandated certain guidelines for short range unlicensed access so as to avoid any interference to dtv receivers we consider the problem of wifi like access popularly referred to as wifi for enterprizes we assume that the access points and client devices are equipped with cognitive radios ie they can adaptively choose the center frequency bandwidth and ower of operation the access points can be equipped with one or more radios our goal is to design complete system which does not violate the fcc mandate ii dynamically assigns center frequency and bandwidth to each access point based on their demands and iii squeezes the maximum efficiency from the available spectrum this problem is far more general than prior work that investigated dynamic spectrum allocation in cellular and ism bands due to the non homogenous nature of the whitespaces ie different whitespace widths in different parts of the spectrum and the large range of frequency bands with different propagation characteristics this calls for more holistic approach to system design that also accounts for frequency dependent propagation characteristics and radio frontend characteristics in this paper we first propose design rules for holistic system design we then describe an architecture derived from our design rules finally we propose demand based dynamic spectrum allocation algorithms with provable worst case guarantees we provide extensive simulation results showing that the performance of our algorithm is within of the optimal in typical settings and ii and the dtv whitespaces can provide significantly higher data rates compared to the ghz ism band our approach is general enough for designing any system with access to wide range of spectrum
opinion mining is the task of extracting from set of documents opinions expressed by source on specified target this article presents comparative study on the methods and resources that can be employed for mining opinions from quotations reported speech in newspaper articles we show the difficulty of this task motivated by the presence of different possible targets and the large variety of affect phenomena that quotes contain we evaluate our approaches using annotated quotations extracted from news provided by the emm news gathering engine we conclude that generic opinion mining system requires both the use of large lexicons as well as specialised training and testing data
we consider the setting of multiprocessor where the speeds of the processors can be individually scaled jobs arrive over time and have varying degrees of parallelizability nonclairvoyant scheduler must assign the jobs to processors and scale the speeds of the processors we consider the objective of energy plus flow time for jobs that may have side effects or that are not checkpointable we show an bound on the competitive ratio of any deterministic algorithm here is the number of processors and is the exponent of the power function for checkpointable jobs without side effects we give an log competitive algorithm thus for jobs that may have side effects or that are not checkpointable the achievable competitive ratio grows quickly with the number of processors but for checkpointable jobs without side effects the achievable competitive ratio grows slowly with the number of processors we then show lower bound of log on the competitive ratio of any algorithm for checkpointable jobs without side effects finally we slightly improve the upper bound on the competitive ratio for the single processor case which is equivalent to the case that all jobs are fully parallelizable by giving an improved analysis of previously proposed algorithm
model to query document databases by both their content and structure is presented the goal is to obtain query language that is expressive in practice while being efficiently implementable features not present at the same time in previous work the key ideas of the model are set oriented query language based on operations on nearby structure elements of one or more hierarchies together with content and structural indexing and bottom up evaluation the model is evaluated in regard to expressiveness and efficiency showing that it provides good trade off between both goals finally it is shown how to include in the model other media different from text
the state explosion problem of formal verification has obstructed its application to large scale software systems in this article we introduce set of new condensation theories iot failure equivalence iot state equivalence and firing dependence theory to cope with this problem our condensation theories are much weaker than current theories used for the compositional verification of petri nets more significantly our new condensation theories can eliminate the interleaved behaviors caused by asynchronously sending actions therefore our technique provides much more powerful means for the compositional verification of asynchronous processes our technique can efficiently analyze several state based properties boundedness reachable markings reachable submarkings and deadlock states based on the notion of our new theories we develop set of condensation rules for efficient verification of large scale software systems the experimental results show significant improvement in the analysis large scale concurrent systems
we consider the distribution of channels of live multimedia content eg radio or tv broadcasts via multiple content aggregators in our work an aggregator receives channels from content sources and redistributes them to potentially large number of mobile hosts each aggregator can offer channel in various configurations to cater for different wireless links mobile hosts and user preferences as result mobile host can generally choose from different configurations of the same channel offered by multiple alternative aggregators which may be available through different interfaces eg in hotspot mobile host may need to handoff to another aggregator once it receives channel to prevent service disruption mobile host may for instance need to handoff to another aggregator when it leaves the subnets that make up its current aggregator's service area eg hotspot or cellular network in this paper we present the design of system that enables multi homed mobile hosts to seamlessly handoff from one aggregator to another so that they can continue to receive channel wherever they go we concentrate on handoffs between aggregators as result of mobile host crossing subnet boundary as part of the system we discuss lightweight application level protocol that enables mobile hosts to select the aggregator that provides the best configuration of channel the protocol comes into play when mobile host begins to receive channel and when it crosses subnet boundary while receiving the channel we show how our protocol can be implemented using the standard ietf session control and description protocols sip and sdp the implementation combines sip and sdp's offer answer model in novel way
in this paper we describe the design implementation and evaluation of software framework that supports the development of mobile context aware trails based applications trail is contextually scheduled collection of activities and represents generic model that can be used to satisfy the activity management requirements of wide range of context based time management applications trails overcome limitations with traditional time management techniques based on static to do lists by dynamically reordering activities based on emergent context
in multiuser multimedia information systems eg movie on demand digital editing scheduling the retrievals of continuous media objects becomes challenging task this is because of both intra and inter iobject time dependencies intraobject time dependency refers to the real time display requirement of continuous media object interobject time dependency is the temporal relationships defined among multiple continuous media objects in order to compose tailored multimedia presentations user might define complex time dependencies among multiple continuous media objects with various lengths and display bandwidths scheduling the retrieval tasks corresponding to the components of such presentation in order to respect both inter and intra task time dependencies is the focus of this study to tackle this task scheduling problem crs we start with simpler scheduling problem ars where there is no inter task time dependency eg movie on demand next we investigate an augmented version of ars termed ars where requests reserve displays in advance eg reservation based movie on demand finally we extend our techniques proposed for ars and ars to address the crs problem we also provide formal definition of these scheduling problems and proof of their np hardness
in automatic software verification we have observed theoretical convergence of model checking and program analysis in practice however model checkers are still mostly concerned with precision eg the removal of spurious counterexamples for this purpose they build and refine reachability trees lattice based program analyzers on the other hand are primarily concerned with efficiency we designed an algorithm and built tool that can be configured to perform not only purely tree based or purely lattice based analysis but offers many intermediate settings that have not been evaluated before the algorithm and tool take one or more abstract interpreters such as predicate abstraction and shape analysis and configure their execution and interaction using several parameters our experiments show that such customization may lead to dramatic improvements in the precision efficiency spectrum
in web database that dynamically provides information in response to user queries two distinct schemas interface schema the schema users can query and result schema the schema users can browse are presented to users each partially reflects the actual schema of the web database most previous work only studied the problem of schema matching across query interfaces of web databases in this paper we propose novel schema model that distinguishes the interface and the result schema of web database in specific domain in this model we address two significant web database schema matching problems intra site and inter site the first problem is crucial in automatically extracting data from web databases while the second problem plays significant role in meta retrieving and integrating data from different web databases we also investigate unified solution to the two problems based on query probing and instance based schema matching techniques using the model cross validation technique is also proposed to improve the accuracy of the schema matching our experiments on real web databases demonstrate that the two problems can be solved simultaneously with high precision and recall
in this paper we propose run time strategy for allocating application tasks to embedded multiprocessor systems on chip platforms where communication happens via the network on chip approach as novel contribution we incorporate the user behavior information in the resource allocation process this allows the system to better respond to real time changes and to adapt dynamically to different user needs several algorithms are proposed for solving the task allocation problem while minimizing the communication energy consumption and network contention when the user behavior is taken into consideration we observe more than communication energy savings with negligible energy and run time overhead compared to an arbitrary contiguous task allocation strategy
automatically recognising which html documents on the web contain items of interest for user is non trivial as step toward solving this problem we propose an approach based on information extraction ontologies given html documents tables and forms our document recognition system extracts expected ontological vocabulary keywords and keyword phrases and expected ontological instance data particular values for ontological concepts we then use machine learned rules over this extracted information to determine whether an html document contains items of interest experimental results show that our ontological approach to categorisation works well having achieved measures above for all applications we tried
we present new approach to accelerate collision detection for deformable models our formulation applies to all triangulated models and significantly reduces the number of elementary tests between features of the mesh ie vertices edges and faces we introduce the notion of representative triangles standard geometric triangles augmented with mesh feature information and use this representation to achieve better collision query performance the resulting approach can be combined with bounding volume hierarchies and works well for both inter object and self collision detection we demonstrate the benefit of representative triangles on continuous collision detection for cloth simulation and body collision scenarios we observe up to one order of magnitude reduction in feature pair tests and up to improvement in query time
graph theory has been shown to provide powerful tool for representing and tackling machine learning problems such as clustering semi supervised learning and feature ranking this paper proposes graph based discrete differential operator for detecting and eliminating competence critical instances and class label noise from training set in order to improve classification performance results of extensive experiments on artificial and real life classification problems substantiate the effectiveness of the proposed approach
there has been little research into how end users might be able to communicate advice to machine learning systems if this resource the users themselves could somehow work hand in hand with machine learning systems the accuracy of learning systems could be improved and the users understanding and trust of the system could improve as well we conducted think aloud study to see how willing users were to provide feedback and to understand what kinds of feedback users could give users were shown explanations of machine learning predictions and asked to provide feedback to improve the predictions we found that users had no difficulty providing generous amounts of feedback the kinds of feedback ranged from suggestions for reweighting of features to proposals for new features feature combinations relational features and wholesale changes to the learning algorithm the results show that user feedback has the potential to significantly improve machine learning systems but that learning algorithms need to be extended in several ways to be able to assimilate this feedback
information visualisation has become increasingly important in science engineering and commerce as tool to convey and explore complex sets of information this paper introduces visualisation schema which uses visual attributes as the principle components of visualisation we present new classification of visual attributes according to information accuracy information dimension and spatial requirements and obtain values for the information content and information density of each attribute the classification applies only to the perception of quantitative information and initial results of experiments suggest that it can not be extended to other visual processing tasks such as preattentive target detectionthe classification in combination with additional guidelines given in this paper provide the reader with useful tool for creating visualisations which convey complex sets of information more effectively
in this article we examine how clausal resolution can be applied to specific but widely used nonclassical logic namely discrete linear temporal logic thus we first define normal form for temporal formulae and show how arbitrary temporal formulae can be translated into the normal form while preserving satisfiability we then introduce novel resolution rules that can be applied to formulae in this normal form provide range of examples and examine the correctness and complexity of this approach finally we describe related work and future developments concerning this work
bitwise operations are commonly used in low level systems code to access multiple data fields that have been packed into single word program analysis tools that reason about such programs must model the semantics of bitwise operations precisely in order to capture program control and data flow through these operations we present type system for subword data structures that explitictly tracks the flow of bit values in the program and identifies consecutive sections of bits as logical entities manipulated atomically by the programmer our type inference algorithm tags each integer value of the program with bitvector type that identifies the data layout at the subword level these types are used in translation phase to remove bitwise operations from the program thereby allowing verification engines to avoid the expensive low level reasoning required for analyzing bitvector operations we have used software model checker to check properties of translated versions of linux device driver and memory protection system the resulting verification runs could prove many more properties than the naive model checker that did not reason about bitvectors and could prove properties much faster than model checker that did reason about bitvectors we have also applied our bitvector type inference algorithm to generate program documentation for virtual memory subsystem of an os kernel while we have applied the type system mainly for program understanding and verification bitvector types also have applications to better variable ordering heuristics in boolean model checking and memory optimizations in compilers for embedded software
in art grouping plays major role to convey relationships of objects and the organization of scenes it is separated from style which only determines how groups are rendered to achieve visual abstraction of the depicted scene we present an approach to interactively derive grouping information in dynamic scene our solution is simple and general the resulting grouping information can be used as an input to any rendering style we provide an efficient solution based on an extended mean shift algorithm customized by user defined criteria the resulting system is temporally coherent and real time the computational cost is largely determined by the scene's structure rather than by its geometric complexity
this paper describes an approach spy to recovering the specification of software component from the observation of its run time behavior it focuses on components that behave as data abstractions components are assumed to be black boxes that do not allow any implementation inspection the inferred description may help understand what the component does when no formal specification is available spy works in two main stages first it builds deterministic finite state machine that models the partial behavior of instances of the data abstraction this is then generalized via graph transformation rules the rules can generate possibly infinite number of behavior models which generalize the description of the data abstraction under an assumption of regularity with respect to the observed behavior the rules can be viewed as likely specification of the data abstraction we illustrate how spy works on relevant examples and we compare it with competing methods
this paper presents concept hierarchy based approach to privacy preserving data collection for data mining called the level model the level model allows data providers to divulge information at any chosen privacy level level on any attribute data collected at high level signifies divulgence at higher conceptual level and thus ensures more privacy providing guarantees prior to release such as satisfying anonymity samarati sweeney can further protect the collected data set from privacy breaches due to linking the released data set with external data sets however the data mining process which involves the integration of various data values can constitute privacy breach if combinations of attributes at certain levels result in the inference of knowledge that exists at lower level this paper describes the level reduction phenomenon and proposes methods to identify and control the occurrence of this privacy breach
replay is an important technique in program analysis allowing to reproduce bugs to track changes and to repeat executions for better understanding of the results unfortunately since re executing concurrent program does not necessarily produce the same ordering of events replay of such programs becomes difficult task the most common approach to replay of concurrent programs is based on analyzing the logical dependencies among concurrent events and requires complete recording of the execution we are trying to replay as well as complete control over the program's scheduler in realistic settings we usually have only partial recording of the execution and only partial control over the scheduling decisions thus such an analysis is often impossible in this paper we present an approach for replay in the presence of partial information and partial control our approach is based on novel application of the cross entropy method and it does not require any logical analysis of dependencies among concurrent events roughly speaking given partial recording of an execution we define performance function on executions which reaches its maximum on or any other execution that coincides with on the recorded events then the program is executed many times in iterations on each iteration adjusting the probabilistic scheduling decisions so that the performance function is maximized our method is also applicable to debugging of concurrent programs in which the program is changed before it replayed in order to increase the information from its execution we implemented our replay method on concurrent java programs and we show that it consistently achieves close replay in presence of incomplete information and incomplete control as well as when the program is changed before it is replayed
the work in this paper is motivated by the real world problems such as mining frequent traversal path patterns from very large web logs generalized suffix trees over very large alphabet can be used to solve such problems however traditional algorithms such as the weiner ukkonen and mccreight algorithms are not sufficient assurance of practicality because of large magnitudes of the alphabet and the set of strings in those real world problems two new algorithms are designed for fast construction of generalized suffix trees over very large alphabet and their performance is analyzed in comparison with the well known ukkonen algorithm it is shown that these two algorithms have better performance and can deal with large alphabets and large string sets well
we present texture synthesis scheme based on neighborhood matching with contributions in two areas parallelism and control our scheme defines an infinite deterministic aperiodic texture from which windows can be computed in real time on gpu we attain high quality synthesis using new analysis structure called the gaussian stack together with coordinate upsampling step and subpass correction approach texture variation is achieved by multiresolution jittering of exemplar coordinates combined with the local support of parallel synthesis the jitter enables intuitive user controls including multiscale randomness spatial modulation over both exemplar and output feature drag and drop and periodicity constraints we also introduce synthesis magnification fast method for amplifying coarse synthesis results to higher resolution
traditional behavior based worm detection can't eliminate the influence of the worm like pp traffic effectively as well as detect slow worms to try to address these problems this paper first presents user habit model to describe the factors which influent the generation of network traffic then design of hpbrwd host packet behavior ranking based worm detection and some key issues about it are introduced this paper has three contributions to the worm detection presenting hierarchical user habit model using normal software and time profile to eliminate the worm like pp traffic and accelerate the detection of worms presenting hpbrwd to effectively detect worms experiments results show that hpbrwd is effective to detect worms
we present the design and implementation of new garbage collection framework that significantly generalizes existing copying collectors the beltway framework exploits and separates object age and incrementality it groups objects in one or more increments on queues called belts collects belts independently and collects increments on belt in first in first out order we show that beltway configurations selected by command line options act and perform the same as semi space generational and older first collectors and encompass all previous copying collectors of which we are aware the increasing reliance on garbage collected languages such as java requires that the collector perform well we show that the generality of beltway enables us to design and implement new collectors that are robust to variations in heap size and improve total execution time over the best generational copying collectors of which we are aware by up to and on average by to for small to moderate heap sizes new garbage collection algorithms are rare and yet we define not just one but new family of collectors that subsumes previous work this generality enables us to explore larger design space and build better collectors
comprehending and modifying software is at the heart of many software engineering tasks and this explains the growing interest that software reverse engineering has gained in the last years broadly speaking reverse engineering is the process of analyzing subject system to create representations of the system at higher level of abstraction this paper briefly presents an overview of the field of reverse engineering reviews main achievements and areas of application and highlights key open research issues for the future
we present shape retrieval methodology based on the theory of spherical harmonics using properties of spherical harmonics scaling and axial flipping invariance is achieved rotation normalization is performed by employing the continuous principal component analysis along with novel approach which applies pca on the face normals of the model the model is decomposed into set of spherical functions which represents not only the intersections of the corresponding surface with rays emanating from the origin but also points in the direction of each ray which are closer to the origin than the furthest intersection point the superior performance of the proposed methodology is demonstrated through comparison against state of the art approaches on standard databases
how to allocate computing and communication resources in way that maximizes the effectiveness of control and signal processing has been an important area of research the characteristic of multi hop real time wireless sensor network raises new challenges first the constraints are more complicated and new solution method is needed second distributed solution is needed to achieve scalability this article presents solutions to both of the new challenges the first solution to the optimal rate allocation is centralized solution that can handle the more general form of constraints as compared with prior research the second solution is distributed version for large sensor networks using pricing scheme it is capable of incremental adjustment when utility functions change this article also presents new sensor device network backbone architecture real time independent channels rich which can easily realize multi hop real time wireless sensor networking
data mining mechanisms have widely been applied in various businesses and manufacturing companies across many industry sectors sharing data or sharing mined rules has become trend among business partnerships as it is perceived to be mutually benefit way of increasing productivity for all parties involved nevertheless this has also increased the risk of unexpected information leaks when releasing data to conceal restrictive itemsets patterns contained in the source database sanitization process transforms the source database into released database that the counterpart cannot extract sensitive rules from the transformed result also conceals non restrictive information as an unwanted event called side effect or the misses cost the problem of finding an optimal sanitization method which conceals all restrictive itemsets but minimizes the misses cost is np hard to address this challenging problem this study proposes the maximum item conflict first micf algorithm experimental results demonstrate that the proposed method is effective has low sanitization rate and can generally achieve significantly lower misses cost than those achieved by the minfia maxfia iga and algob methods in several real and artificial datasets
networked games can provide groupware developers with important lessons in how to deal with real world networking issues such as latency limited bandwidth and packet loss games have similar demands and characteristics to groupware but unlike the applications studied by academics games have provided production quality real time interaction for many years the techniques used by games have not traditionally been made public but several game networking libraries have recently been released as open source providing the opportunity to learn how games achieve network performance we examined five game libraries to find networking techniques that could benefit groupware this paper presents the concepts most valuable to groupware developers including techniques to deal with limited bandwidth reliability and latency some of the techniques have been previously reported in the networking literature therefore the contribution of this paper is to survey which techniques have been shown to work over several years and then to link these techniques to quality requirements specific to groupware by adopting these techniques groupware designers can dramatically improve network performance on the real world internet
recent trend in interface design for classrooms in developing regions has many students interacting on the same display using mice text entry has emerged as an important problem preventing such mouse based singledisplay groupware systems from offering compelling interactive activities we explore the design space of mouse based text entry and develop techniques with novel characteristics suited to the multiple mouse scenario we evaluated these in phase study over days with students in developing region schools the results show that one technique effectively balanced all of our design dimensions another was most preferred by students and both could benefit from augmentation to support collaborative interaction our results also provide insights into the factors that create an optimal text entry technique for single display groupware systems
search engines need to evaluate queries extremely fast challenging task given the quantities of data being indexed significant proportion of the queries posed to search engines involve phrases in this article we consider how phrase queries can be efficiently supported with low disk overheads our previous research has shown that phrase queries can be rapidly evaluated using nextword indexes but these indexes are twice as large as conventional inverted files alternatively special purpose phrase indexes can be used but it is not feasible to index all phrases we propose combinations of nextword indexes and phrase indexes with inverted files as solution to this problem our experiments show that combined use of partial nextword partial phrase and conventional inverted index allows evaluation of phrase queries in quarter the time required to evaluate such queries with an inverted file alone the additional space overhead is only percnt of the size of the inverted file
the concept of privacy preserving has recently been proposed in response to the concerns of preserving personal or sensible information derived from data mining algorithms for example through data mining sensible information such as private information or patterns may be inferred from non sensible information or unclassified data there have been two types of privacy concerning data mining output privacy tries to hide the mining results by minimally altering the data input privacy tries to manipulate the data so that the mining result is not affected or minimally affectedfor output privacy in hiding association rules current approaches require hidden rules or patterns to be given in advance this selection of rules would require data mining process to be executed first based on the discovered rules and privacy requirements hidden rules or patterns are then selected manually however for some applications we are interested in hiding certain constrained classes of association rules such as collaborative recommendation association rules to hide such rules the pre process of finding these hidden rules can be integrated into the hiding process as long as the recommended items are given in this work we propose two algorithms dcis decrease confidence by increase support and dcds decrease confidence by decrease support to automatically hiding collaborative recommendation association rules without pre mining and selection of hidden rules examples illustrating the proposed algorithms are given numerical simulations are performed to show the various effects of the algorithms recommendations of appropriate usage of the proposed algorithms based on the characteristics of databases are reported
in recent years peer to peer pp file sharing systems have evolved to accommodate growing numbers of participating peers in particular new features have changed the properties of the unstructured overlay topologies formed by these peers little is known about the characteristics of these topologies and their dynamics in modern file sharing applications despite their importance this paper presents detailed characterization of pp overlay topologies and their dynamics focusing on the modern gnutella network we present cruiser fast and accurate pp crawler which can capture complete snapshot of the gnutella network of more than one million peers in just few minutes and show how inaccuracy in snapshots can lead to erroneous conclusions such as power law degree distribution leveraging recent overlay snapshots captured with cruiser we characterize the graph related properties of individual overlay snapshots and overlay dynamics across slices of back to back snapshots our results reveal that while the gnutella network has dramatically grown and changed in many ways it still exhibits the clustering and short path lengths of small world network furthermore its overlay topology is highly resilient to random peer departure and even systematic attacks more interestingly overlay dynamics lead to an onion like biased connectivity among peers where each peer is more likely connected to peers with higher uptime therefore long lived peers form stable core that ensures reachability among peers despite overlay dynamics
while soft processor cores provided by fpga vendors offer designers with increased flexibility such processors typically incur penalties in performance and energy consumption compared to hard processor core alternatives the recently developed technology of warp processing can help reduce those penalties warp processing is the dynamic and transparent transformation of critical software regions from microprocessor execution to much faster circuit execution on an fpga in this article we describe an implementation of warp processor on xilinx virtex ii pro and spartan fpgas incorporating one or more microblaze soft processor cores we further provide detailed analysis of the energy overhead of dynamically partitioning an application's kernels to hardware executing within an fpga considering an implementation that periodically partitions the executing application once every minute microblaze based warp processor implemented on spartan fpga achieves average speedups of times and energy reductions of percnt compared to the microblaze soft processor core alone mdash providing competitive performance and energy consumption compared to existing hard processor cores
this paper proposes and evaluates single isa heterogeneousmulti core architectures as mechanism to reduceprocessor power dissipation our design incorporatesheterogeneous cores representing different points inthe power performance design space during an application'sexecution system software dynamically chooses themost appropriate core to meet specific performance andpower requirementsour evaluation of this architecture shows significant energybenefits for an objective function that optimizes forenergy efficiency with tight performance threshold for spec benchmarks our results indicate average energyreduction while only sacrificing in performancean objective function that optimizes for energy delay withlooser performance bounds achieves on average nearly afactor of three improvement in energy delay product whilesacrificing only in performance energy savings aresubstantially more than chip wide voltage frequency scaling
skip graphs are novel distributed data structure based on skip lists that provide the full functionality of balanced tree in distributed system where elements are stored in separate nodes that may fail at any time they are designed for use in searching peer to peer networks and by providing the ability to perform queries based on key ordering they improve on existing search tools that provide only hash table functionality unlike skip lists or other tree data structures skip graphs are highly resilient tolerating large fraction of failed nodes without losing connectivity in addition constructing inserting new elements into searching skip graph and detecting and repairing errors in the data structure introduced by node failures can be done using simple and straight forward algorithms
algorithmic solutions can help reduce energy consumption in computing environs
collaborative filtering has become an established method to measure users similarity and to make predictions about their interests however prediction accuracy comes at the cost of user's privacy in order to derive accurate similarity measures users are required to share their rating history with each other in this work we propose new measure of similarity which achieves comparable prediction accuracy to the pearson correlation coefficient and that can successfully be estimated without breaking users privacy this novel method works by estimating the number of concordant discordant and tied pairs of ratings between two users with respect to shared random set of ratings in doing so neither the items rated nor the ratings themselves are disclosed thus achieving strictly private collaborative filtering the technique has been evaluated using the recently released netflix prize dataset
this article introduces novel representation for three dimensional objects in terms of local affine invariant descriptors of their images and the spatial relationships between the corresponding surface patches geometric constraints associated with different views of the same patches under affine projection are combined with normalized representation of their appearance to guide matching and reconstruction allowing the acquisition of true affine and euclidean models from multiple unregistered images as well as their recognition in photographs taken from arbitrary viewpoints the proposed approach does not require separate segmentation stage and it is applicable to highly cluttered scenes modeling and recognition results are presented
we study an approach to text categorization that combines distributional clustering of words and support vector machine svm classifier this word cluster representation is computed using the recently introduced information bottleneck method which generates compact and efficient representation of documents when combined with the classification power of the svm this method yields high performance in text categorization this novel combination of svm with word cluster representation is compared with svm based categorization using the simpler bag of words bow representation the comparison is performed over three known datasets on one of these datasets the newsgroups the method based on word clusters significantly outperforms the word based representation in terms of categorization accuracy or representation efficiency on the two other sets reuters and webkb the word based representation slightly outperforms the word cluster representation we investigate the potential reasons for this behavior and relate it to structural differences between the datasets
we consider the problem of finding highly correlated pairs in large data set that is given threshold not too small we wish to report all the pairs of items or binary attributes whose pearson correlation coefficients are greater than the threshold correlation analysis is an important step in many statistical and knowledge discovery tasks normally the number of highly correlated pairs is quite small compared to the total number of pairs identifying highly correlated pairs in naive way by computing the correlation coefficients for all the pairs is wasteful with massive data sets where the total number of pairs may exceed the main memory capacity the computational cost of the naive method is prohibitive in their kdd paper hui xiong et al address this problem by proposing the taper algorithm the algorithm goes through the data set in two passes it uses the first pass to generate set of candidate pairs whose correlation coefficients are then computed directly in the second pass the efficiency of the algorithm depends greatly on the selectivity pruning power of its candidate generating stagein this work we adopt the general framework of the taper algorithm but propose different candidate generation method for pair of items taper's candidate generation method considers only the frequencies supports of individual items our method also considers the frequency support of the pair but does not explicitly count this frequency support we give simple randomized algorithm whose false negative probability is negligible the space and time complexities of generating the candidate set in our algorithm are asymptotically the same as taper's we conduct experiments on synthesized and real data the results show that our algorithm produces greatly reduced candidate set one that can be several orders of magnitude smaller than that generated by taper because of this our algorithm uses much less memory and can be faster the former is critical for dealing with massive data
this study uses neural field model to investigate computational aspects of population coding and decoding when the stimulus is single variable general prototype model for the encoding process is proposed in which neural responses are correlated with strength specified by gaussian function of their difference in preferred stimuli based on the model we study the effect of correlation on the fisher information compare the performances of three decoding methods that differ in the amount of encoding information being used and investigate the implementation of the three methods by using recurrent network this study not only rediscovers main results in existing literatures in unified way but also reveals important new features especially when the neural correlation is strong as the neural correlation of firing becomes larger the fisher information decreases drastically we confirm that as the width of correlation increases the fisher information saturates and no longer increases in proportion to the number of neurons however we prove that as the width increases further wider than times the effective width of the turning function the fisher information increases again and it increases without limit in proportion to the number of neurons furthermore we clarify the asymptotic efficiency of the maximum likelihood inference mli type of decoding methods for correlated neural signals it shows that when the correlation covers nonlocal range of population excepting the uniform correlation and when the noise is extremely small the mli type of method whose decoding error satisfies the cauchy type distribution is not asymptotically efficient this implies that the variance is no longer adequate to measure decoding accuracy
recently there has been growing interest in gossip based protocols that employ randomized communication to ensure robust information dissemination in this paper we present novel gossip based scheme using which all the nodes in an node overlay network can compute the common aggregates of min max sum average and rank of their values using log log messages within log log log rounds of communication to the best of our knowledge ours is the first result that shows how to compute these aggregates with high probability using only log log messages in contrast the best known gossip based algorithm for computing these aggregates requires nlog messages and log rounds thus our algorithm allows system designers to trade off small increase in round complexity with significant reduction in message complexity this can lead to dramatically lower network congestion and longer node lifetimes in wireless and sensor networks where channel bandwidth and battery life are severely constrained
partitioning an application among software running on microprocessor and hardware co processors in on chip configurable logic has been shown to improve performance and energy consumption in embedded systems meanwhile dynamic software optimization methods have shown the usefulness and feasibility of runtime program optimization but those optimizations do not achieve as much as partitioning we introduce first approach to dynamic hardware software partitioning we describe our system architecture and initial on chip tools including profiler decompiler synthesis and placement and routing tools for simplified configurable logic fabric able to perform dynamic partitioning of real benchmarks we show speedups averaging for five benchmarks taken from powerstone netbench and our own benchmarks
developing desirable framework for handling inconsistencies in software requirements specifications is challenging problem it has been widely recognized that the relative priority of requirements can help developers to make some necessary trade off decisions for resolving con flicts however for most distributed development such as viewpoints based approaches different stakeholders may assign different levels of priority to the same shared requirements statement from their own perspectives the disagreement in the local levels of priority assigned to the same shared requirements statement often puts developers into dilemma during the inconsistency handling process the main contribution of this paper is to present prioritized merging based framework for handling inconsistency in distributed software requirements specifications given set of distributed inconsistent requirements collections with the local prioritization we first construct requirements specification with prioritization from an overall perspective we provide two approaches to constructing requirements specification with the global prioritization including merging based construction and priority vector based construction following this we derive proposals for handling inconsistencies from the globally prioritized requirements specification in terms of prioritized merging moreover from the overall perspective these proposals may be viewed as the most appropriate to modifying the given inconsistent requirements specification in the sense of the ordering relation over all the consistent subsets of the requirements specification finally we consider applying negotiation based techniques to viewpoints so as to identify an acceptable common proposal from these proposals
even the best laid plans can fail and robot plans executed in real world domains tend to do so often the ability of robot to reliably monitor the execution of plans and detect failures is essential to its performance and its autonomy in this paper we propose technique to increase the reliability of monitoring symbolic robot plans we use semantic domain knowledge to derive implicit expectations of the execution of actions in the plan and then match these expectations against observations we present two realizations of this approach crisp one which assumes deterministic actions and reliable sensing and uses standard knowledge representation system loom and probabilistic one which takes into account uncertainty in action effects in sensing and in world states we perform an extensive validation of these realizations through experiments performed both in simulation and on real robots
database queries are often exploratory and users often find their queries return too many answers many of them irrelevant existing work either categorizes or ranks the results to help users locate interesting results the success of both approaches depends on the utilization of user preferences however most existing work assumes that all users have the same user preferences but in real life different users often have different preferences this paper proposes two step solution to address the diversity issue of user preferences for the categorization approach the proposed solution does not require explicit user involvement the first step analyzes query history of all users in the system offline and generates set of clusters over the data each corresponding to one type of user preferences when user asks query the second step presents to the user navigational tree over clusters generated in the first step such that the user can easily select the subset of clusters matching his needs the user then can browse rank or categorize the results in selected clusters the navigational tree is automatically constructed using cost based algorithm which considers the cost of visiting both intermediate nodes and leaf nodes in the tree an empirical study demonstrates the benefits of our approach
database applications often require to evaluate queries containing quantifiers or disjunctions eg for handling general integrity constraints existing efficient methods for processing quantifiers depart from the relational model as they rely on non algebraic procedures looking at quantified query evaluation from new angle we propose an approach to process quantifiers that makes use of relational algebra operators only our approach performs in two phases the first phase normalizes the queries producing canonical form this form permits to improve the translation into relational algebra performed during the second phase the improved translation relies on new operator the complement join that generalizes the set difference on algebraic expressions of universal quantifiers that avoid the expensive division operator in many cases and on special processing of disjunctions by means of constrained outer joins our method achieves an efficiency at least comparable with that of previous proposals better in most cases furthermore it is considerably simpler to implement as it completely relies on relational data structures and operators
peer to peer pp model being widely adopted in todays internet computing suffers from the problem of topology mismatch between the overlay networks and the underlying physical network traditional topology optimization techniques identify physical closer nodes to connect as overlay neighbors but could significantly shrink the search scope recent efforts have been made to address the mismatch problem without sacrificing search scope but they either need time synchronization among peers or have low convergent speed in this paper we propose scalable bipartite overlay sbo scheme to optimize the overlay topology by identifying and replacing the mismatched connections in sbo we employ an efficient strategy for distributing optimization tasks in peers with different colors we conducted comprehensive simulations to evaluate this design the results show that sbo achieves approximately reduction on traffic cost and about reduction on query response time our comparisons with previous approaches to address the topology mismatch problem have shown that sbo can achieve fast convergent speed without the need of time synchronization among peers
bytecodes and virtual machines vm are prevailing programming facilities in contemporary software industry due to their ease of portability across various platforms thus it is critical to improve their trustworthiness this paper addresses the interesting and challenging problem of certifying bytecode programs over certified vms our solutions to this problem include logical systems cbp for bytecode machine is built to modularly certify bytecode programs with abstract control stacks and unstructured control flows and the corresponding stack based virtual machine is implemented and certified simulation relation between bytecode program and vm implementation is developed and proved to achieve the objective that once some safety property of bytecode program is certified in cbp system the property will be preserved on any certified vm we prove the soundness and demonstrate its power by certifying some example programs with the coq proof assistant this work not only provides solid theoretical foundation for reasoning about bytecode programs but also gains insight into building proof preserving compilers
multiple clock domain mcd processor addresses the challenges of clock distribution and power dissipation by dividing chip into several coarse grained clock domains allowing frequency and voltage to be reduced in domains that are not currently on the application's critical path given reconfiguration mechanism capable of choosing appropriate times and values for voltage frequency scaling an mcd processor has the potential to achieve significant energy savings with low performance degradationearly work on mcd processors evaluated the potential for energy savings by manually inserting reconfiguration instructions into applications or by employing an oracle driven by off line analysis of identical prior program runs subsequent work developed hardware based on line mechanism that averages of the energy delay improvement achieved via off line analysisin this paper we consider the automatic insertion of reconfiguration instructions into applications using profile driven binary rewriting profile based reconfiguration introduces the need for training runs prior to production use of given application but avoids the hardware complexity of on line reconfiguration it also has the potential to yield significantly greater energy savings experimental results training on small data sets and then running on larger alternative data sets indicate that the profile driven approach is more stable than hardware based reconfiguration and yields virtually all of the energy delay improvement achieved via off line analysis
an accurate and rapid method is required to retrieve the overwhelming majority of digital images to date image retrieval methods include content based retrieval and keyword based retrieval the former utilizing visual features such as color and brightness and the latter utilizing keywords that describe the image however the effectiveness of these methods in providing the exact images the user wants has been under scrutiny hence many researchers have been working on relevance feedback process in which responses from the user are given as feedback during the retrieval session in order to define user's need and provide an improved result methods that employ relevance feedback however do have drawbacks because several pieces of feedback are necessary to produce an appropriate result and the feedback information cannot be reused in this paper novel retrieval model is proposed which annotates an image with keywords and modifies the confidence level of the keywords in response to the user's feedback in the proposed model not only the images that have been given feedback but also other images with visual features similar to the features used to distinguish the positive images are subjected to confidence modification this allows for modification of large number of images with relatively little feedback ultimately leading to faster and more accurate retrieval results an experiment was performed to verify the effectiveness of the proposed model and the result demonstrated rapid increase in recall and precision using the same amount of feedback
we present framework for designing efficient distributed data structures for multi dimensional data our structures which we call skip webs extend and improve previous randomized distributed data structures including skipnets and skip graphs our framework applies to general class of data querying scenarios which include linear one dimensional data such as sorted sets as well as multi dimensional data such as dimensional octrees and digital tries of character strings defined over fixed alphabetwe show how to perform query over such set of items spread among hosts using log log log messages for one dimensional data or log messages for fixed dimensional data while using only log space per host we also show how to make such structures dynamic so as to allow for insertions and deletions in log messages for quadtrees octrees and digital tries and log log log messages for one dimensional data finally we show how to apply blocking strategy to skip webs to further improve message complexity for one dimensional data when hosts can store more data
by using elliptic curve cryptography ecc it has been recently shown that public key cryptography pkc is indeed feasible on resource constrained nodes this feasibility however does not necessarily mean attractiveness as the obtained results are still not satisfactory enough in this paper we present results on implementing ecc as well as the related emerging field of pairing based cryptography pbc on two of the most popular sensor nodes by doing that we show that pkc is not only viable but in fact attractive for wsns as far as we know pairing computations presented in this paper are the most efficient results on the mica bit mhz atmegal and tmote sky bit mhz msp nodes
animation techniques for controlling passive simulation are commonly based on an optimization paradigm the user provides goals priori and sophisticated numerical methods minimize cost function that represents these goals unfortunately for multibody systems with discontinuous contact events these optimization problems can be highly nontrivial to solve and many hour offline optimizations unintuitive parameters and convergence failures can frustrate end users and limit usage on the other hand users are quite adaptable and systems which provide interactive feedback via an intuitive interface can leverage the user's own abilities to quickly produce interesting animations however the online computation necessary for interactivity limits scene complexity in practice we introduce many worlds browsing method which circumvents these limits by exploiting the speed of multibody simulators to compute numerous example simulations in parallel offline and online and allow the user to browse and modify them interactively we demonstrate intuitive interfaces through which the user can select among the examples and interactively adjust those parts of the scene that do not match his requirements we show that using combination of our techniques unusual and interesting results can be generated for moderately sized scenes with under an hour of user time scalability is demonstrated by sampling much larger scenes using modest offline computations
current capacity planning practices based on heavy over provisioning of power infrastructure hurt the operational costs of data centers as well as ii the computational work they can support we explore combination of statistical multiplexing techniques to improve the utilization of the power hierarchy within data center at the highest level of the power hierarchy we employ controlled underprovisioning and over booking of power needs of hosted workloads at the lower levels we introduce the novel notion of soft fuses to flexibly distribute provisioned power among hosted workloads based on their needs our techniques are built upon measurement driven profiling and prediction framework to characterize key statistical properties of the power needs of hosted workloads and their aggregates we characterize the gains in terms of the amount of computational work cpu cycles per provisioned unit of power computation per provisioned watt cpw our technique is able to double the cpwoffered by power distribution unit pdu running the commerce benchmark tpc compared to conventional provisioning practices over booking the pdu by based on tails of power profiles yields further improvement of reactive techniques implemented on our xen vmm based servers dynamically modulate cpu dvfs states to ensure power draw below the limits imposed by soft fuses finally information captured in our profiles also provide ways of controlling application performance degradation despite overbooking the th percentile of tpc session response time only grew from sec to sec degradation of
resource aware random key predistribution schemes have been proposed to overcome the limitations of energy constrained wireless sensor networks wsns in most of these schemes each sensor node is loaded with key ring neighbouring nodes are considered to be connected through secure link if they share common key nodes which are not directly connected establish secure path which is then used to negotiate symmetric key however since different symmetric keys are used for different links along the secure path each intermediate node must first decrypt the message received from the upstream node notice that during this process the negotiated key is revealed to each node along the secure path the objective of this paper is to address this shortcoming to this end we propose an end to end pairwise key establishment scheme which uses properly selected set of node disjoint paths to securely negotiate symmetric keys between sensor nodes we show through analysis and simulation that our scheme is highly secure against node captures in wsns
silicon technology advances have made it possible to pack millions of transistors switching at high clock speeds on single chip while these advances bring unprecedented performance to electronic products they also pose difficult power energy consumption problems for example large number of transistors in dense on chip cache memories consume significant static leakage power even if the cache is not used by the current computation while previous compiler research studied code and data restructuring for improving data cache performance to our knowledge there exists no compiler based study that targets data cache leakage power consumption in this paper we present code restructuring techniques for array based and pointer intensive applications for reducing data cache leakage energy consumption the idea is to let the compiler analyze the application code and insert instructions that turn off cache lines that keep variables not used by the current computation this turning off does not destroy contents of cache line and waking up the cache line when it is accessed later does not incur much overhead due to inherent data locality in applications we find that at given time only small portion of the data cache needs to be active the remaining part can be placed into leakage saving mode state ie they can be turned off our experimental results indicate that the proposed compiler based strategy reduces the cache energy consumption significantly we also demonstrate how different compiler optimizations can increase the effectiveness of our strategy
modern source control systems such as subversion preserve change sets of files as atomic commits however the specific ordering information in which files were changed is typically not found in these source code repositories in this paper set of heuristics for grouping change sets ie log entries found in source code repositories is presented given such groups of change sets sequences of files that frequently change together are uncovered this approach not only gives the unordered sets of files but supplements them with partial temporal ordering information the technique is demonstrated on subset of kde source code repository the results show that the approach is able to find sequences of changed files
it is crucial to maximize targeting efficiency and customer satisfaction in personalized marketing state of the art techniques for targeting focus on the optimization of individual campaigns our motivation is the belief that the effectiveness of campaign with respect to customer is affected by how many precedent campaigns have been recently delivered to the customer we raise the multiple recommendation problem which occurs when performing several personalized campaigns simultaneously we formulate the multicampaign assignment problem to solve this issue and propose algorithms for the problem the algorithms include dynamic programming and efficient heuristic methods we verify by experiments the effectiveness of the problem formulation and the proposed algorithms
given an incorrect value produced during failed program run eg wrong output value or value that causes the program to crash the backward dynamic slice of the value very frequently captures the faulty code responsible for producing the incorrect value although the dynamic slice often contains only small percentage of the statements executed during the failed program run the dynamic slice can still be large and thus considerable effort may be required by the programmer to locate the faulty codein this paper we develop strategy for pruning the dynamic slice to identify subset of statements in the dynamic slice that are likely responsible for producing the incorrect value we observe that some of the statements used in computing the incorrect value may also have been involved in computing correct values eg value produced by statement in the dynamic slice of the incorrect value may also have been used in computing correct output value prior to the incorrect value for each such executed statement in the dynamic slice using the value profiles of the executed statements we compute confidence value ranging from to higher confidence value corresponds to greater likelihood that the execution of the statement produced correct value given failed run involving execution of single error we demonstrate that the pruning of dynamic slice by excluding only the statements with the confidence value of is highly effective in reducing the size of the dynamic slice while retaining the faulty code in the slice our experiments show that the number of distinct statements in pruned dynamic slice are to times less than the full dynamic slice confidence values also prioritize the statements in the dynamic slice according to the likelihood of them being faulty we show that examining the statements in the order of increasing confidence values is an effective strategy for reducing the effort of fault location
dynamic inference techniques have been demonstrated to provide useful support for various software engineering tasks including bug finding test suite evaluation and improvement and specification generation to date however dynamic inference has only been used effectively on small programs under controlled conditions in this paper we identify reasons why scaling dynamic inference techniques has proven difficult and introduce solutions that enable dynamic inference technique to scale to large programs and work effectively with the imperfect traces typically available in industrial scenarios we describe our approximate inference algorithm present and evaluate heuristics for winnowing the large number of inferred properties to manageable set of interesting properties and report on experiments using inferred properties we evaluate our techniques on jboss and the windows kernel our tool is able to infer many of the properties checked by the static driver verifier and leads us to discover previously unknown bug in windows
the last fifteen years has seen vast proliferation of middleboxes to solve all manner of persistent limitations in the internet protocol suite examples include firewalls nats load balancers traffic shapers deep packet intrusion detection virtual private networks network monitors transparent web caches content delivery networks and the list goes on and on however most smaller networks in homes small businesses and the developing world are left without this level of support further the management burden and limitations of middleboxes are apparent even in enterprise networks we argue for shift from using proprietary middle box harware as the dominant tool for managing networks toward using open software running on end hosts we show that functionality that seemingly must be in the network such as nats and traffic prioritization can be more cheaply flexibly and securely provided by distributed software running on end hosts working in concert with vastly simplified physical network hardware
set valued ordered information systems can be classified into two categories disjunctive and conjunctive systems through introducing two new dominance relations to set valued information systems we first introduce the conjunctive disjunctive set valued ordered information systems and develop an approach to queuing problems for objects in presence of multiple attributes and criteria then we present dominance based rough set approach for these two types of set valued ordered information systems which is mainly based on substitution of the indiscernibility relation by dominance relation through the lower upper approximation of decision some certain possible decision rules from so called set valued ordered decision table can be extracted finally we present attribute reduction also called criteria reduction in ordered information systems approaches to these two types of ordered information systems and ordered decision tables which can be used to simplify set valued ordered information system and find decision rules directly from set valued ordered decision table these criteria reduction approaches can eliminate those criteria that are not essential from the viewpoint of the ordering of objects or decision rules
nowadays embedded systems are growing at an impressive rate and provide more and more sophisticated applications characterized by having complex array index manipulation and large number of data accesses those applications require high performance specific computation that general purpose processors can not deliver at reasonable energy consumption very long instruction word architectures seem good solution providing enough computational performance at low power with the required programmability to speed up the time to market those architectures rely on compiler effort to exploit the available instruction and data parallelism to keep the data path busy all the time with the density of transistors doubling each months more and more sophisticated architectures with high number of computational resources running in parallel are emerging with this increasing parallel computation the access to data is becoming the main bottleneck that limits the available parallelism to alleviate this problem in current embedded architectures special unit works in parallel with the main computing elements to ensure efficient feed and storage of the data the address generator unit which comes in many flavors future architectures will have to deal with enormous memory bandwidth in distributed memories and the development of address generators units will be crucial for effective next generation of embedded processors where global trade offs between reaction time bandwidth energy and area must be achieved this paper provides survey of methods and techniques that optimize the address generation process for embedded systems explaining current research trends and needs for future
students studying topics in cyber security benefit from working with realistic training labs that test their knowledge of network security cost space time and reproducibility are major factors that prevent instructors from building realistic networks for their students this paper explores the ways that existing virtualization technologies could be packaged to provide more accessible comprehensive and realistic training and education environment the paper focuses on ways to leverage technologies such as operating system virtualization and other virtualization techniques to recreate an entire network environment consisting of dozens of nodes on moderately equipped hardware
in this paper we consider the problem of web page usage prediction in web site by modeling users navigation history with weighted suffix trees this user's navigation prediction can be exploited either in an on line recommendation system in website or in web page cache system the method proposed has the advantage that it demands constant amount of computational effort per user action and consumes relatively small amount of extra memory space these features make the method ideal for an on line working environment finally we have performed an evaluation of the proposed scheme with experiments on various website logfiles and we have found that its prediction quality is fairly good in many cases outperforming existing solutions
current systems on chip soc execute applications that demand extensive parallel processing networks on chip noc provide structured way of realizing interconnections on silicon and obviate the limitations of bus based solution nocs can have regular or ad hoc topologies and functional validation is essential to assess their correctness and performance in this paper we present flexible emulation environment implemented on an fpga that is suitable to explore evaluate and compare wide range of noc solutions with very limited effort our experimental results show speed up of four orders of magnitude with respect to cycle accurate hdl simulation while retaining cycle accuracy with our emulation framework designers can explore and optimize various range of solutions as well as characterize quickly performance figures
approaches for indexing proteins and for fast and scalable searching for structures similar to query structure have important applications such as protein structure and function prediction protein classification and drug discovery in this paper we develop new method for extracting local structural or geometric features from protein structures these feature vectors are in turn converted into set of symbols which are then indexed using suffix tree for given query the suffix tree index can be used effectively to retrieve the maximal matches which are then chained to obtain the local alignments finally similar proteins are retrieved by their alignment score against the query our results show classification accuracy up to and at the topology and class level according to the cath classification these results outperform the best previous methods we also show that psist is highly scalable due to the external suffix tree indexing approach it uses it is able to index about domains from scop in under an hour
we introduce rich language of descriptions for semistructured tree like data and we explain how such descriptions relate to the data they describe various query languages and data schemas can be based on such descriptions
internet routing is mostly based on static information it's dynamicity is limited to reacting to changes in topology adaptive performance based routing decisions would not only improve the performance itself of the internet but also its security and availability however previous approaches for making internet routing adaptive based on optimizing network wide objectives are not suited for an environment in which autonomous and possibly malicious entities interact in this paper we propose different framework for adaptive routing decisions based on regret minimizing online learning algorithms these algorithms as applied to routing are appealing because adopters can independently improve their own performance while being robust to adversarial behavior however in contrast to approaches based on optimization theory that provide guarantees from the outset about network wide behavior the network wide behavior if online learning algorithms were to interact with each other is less understood in this paper we study this interaction in realistic internet environment and find that the outcome is stable state and that the optimality gap with respect to the network wide optimum is small our findings suggest that online learning may be suitable framework for adaptive routing decisions in the internet
real time embedded systems are typically constrained in terms of three system performance criteria space time and energy the performance requirements are directly translated into constraints imposed on the system's resources such as code size execution time and energy consumption these resource constraints often interact or even conflict with each other in complex manner making it difficult for system developer to apply well defined design methodology in developing real time embedded system motivated by this observation we propose design framework that can flexibly balance the tradeoff involving the system's code size execution time and energy consumption given system specification and an optimization criteria the proposed technique generates set of design parameters in such way that system cost function is minimized while the given resource constraints are satisfied specifically the technique derives code generation decision for each task so that specific version of code is selected among number of different ones that have distinct characteristics in terms of code size and execution time in addition the design framework determines the voltage frequency setting for variable voltage processor whose supply voltage can be adjusted at runtime in order to minimize the energy consumption while execution performance is degraded accordingly the proposed technique formulates this design process as constrained optimization problem we show that this optimization problem is np hard and then provide heuristic solution to it we show that these seemingly conflicting design goals can be pursued by using simple optimization algorithm that works with single optimization criteria moreover the optimization is driven by an abstract system specification given by the system developer so that the system development process can be automated the results from our simulation show that the proposed algorithm finds solution that is close to the optimal one with the average error smaller than percent
the goal of the research described here is to develop multistrategy classifier system that can be used for document categorization the system automatically discovers classification patterns by applying several empirical learning methods to different representations for preclassified documents belonging to an imbalanced sample the learners work in parallel manner where each learner carries out its own feature selection based on evolutionary techniques and then obtains classification model in classifying documents the system combines the predictions of the learners by applying evolutionary techniques as well the system relies on modular flexible architecture that makes no assumptions about the design of learners or the number of learners available and guarantees the independence of the thematic domain
we show that for several natural classes of structured matrices including symmetric circulant hankel and toeplitz matrices approximating the permanent modulo prime is as hard as computing its exact value results of this kind are well known for arbitrary matrices however the techniques used do not seem to apply to structured matrices our approach is based on recent advances in the hidden number problem introduced by boneh and venkatesan in combined with some bounds of exponential sums motivated by the waring problem in finite fields
nowadays it is widely accepted that the data warehouse design task should be largely automated furthermore the data warehouse conceptual schema must be structured according to the multidimensional model and as consequence the most common way to automatically look for subjects and dimensions of analysis is by discovering functional dependencies as dimensions functionally depend on the fact over the data sources most advanced methods for automating the design of the data warehouse carry out this process from relational oltp systems assuming that rdbms is the most common kind of data source we may find and taking as starting point relational schema in contrast in our approach we propose to rely instead on conceptual representation of the domain of interest formalized through domain ontology expressed in the dl lite description logic we propose an algorithm to discover functional dependencies from the domain ontology that exploits the inference capabilities of dl lite thus fully taking into account the semantics of the domain we also provide an evaluation of our approach in real world scenario
while set associative caches incur fewer misses than direct mapped caches they typically have slower hit times and higher power consumption when multiple tag and data banks are probed in parallel this paper presents the location cache structure which significantly reduces the power consumption for large set associative caches we propose to use small cache called location cache to store the location of future cache references if there is hit in the location cache the supported cache is accessed as direct mapped cache otherwise the supported cache is referenced as conventional set associative cachethe worst case access latency of the location cache system is the same as that of conventional cache the location cache is virtually indexed so that operations on it can be performed in parallel with the tlb address translation these advantages make it ideal for cache systems where traditional way predication strategies perform poorlywe used the cacti cache model to evaluate the power con sumption and access latency of proposed cache architecture simplescalar cpu simulator was used to produce final results it is shown that the proposed location cache architecture is power efficient in the simulated cache configurations up to of cache accessing energy and of average cache access latency can be reduced
middleware is often built using layered architectural style layered design provides good separation of the different concerns of middleware such as communication marshaling request dispatching thread management etc layered architecture helps in the development and evolution of the middleware it also provides tactical side benefits layers provide convenient protection boundaries for enforcing security policies however the benefits of this layered structure come at cost layered designs can hinder performance related optimizations and actually make it more difficult to adapt systems to conveniently address late bound requirements such as dependability access control virus protection and so on we present some examples of this issue and outline new approach under investigation at uc davis which includes ideas in middleware architectures and programming models
symmetric multiprocessor smp servers provide superior performance for the commercial workloads that dominate the internet our simulation results show that over one third of cache misses by these applications result in cache to cache transfers where the data is found in another processor's cache rather than in memory smps are optimized for this case by using snooping protocols that broadcast address transactions to all processors conversely directory based shared memory systems must indirectly locate the owner and sharers through directory resulting in larger average miss latenciesthis paper proposes timestamp snooping technique that allows smps to utilize high speed switched interconnection networks and ii exploit physical locality by delivering address transactions to processors and memories without regard to order traditional snooping requires physical ordering of transactions timestamp snooping works by processing address transactions in logical order logical time is maintained by adding few bits per address transaction and having network switches perform handshake to ensure on time delivery processors and memories then reorder transactions based on their timestamps to establish total orderwe evaluate timestamp snooping with commercial workloads on processor sparc system using the simics full system simulator we simulate both an indirect butterfly and direct torus network design for oltp dss web serving web searching and one scientific application timestamp snooping with the butterfly network runs faster than directories at cost of more link traffic similarly with the torus network timestamp snooping runs faster for more link traffic thus timestamp snooping is worth considering when buying more interconnect bandwidth is easier than reducing interconnect latency
scientific research and practical applications of solar physics require data and computational services to be integrated seamlessly and efficiently the european grid for solar observations egso leverages grid oriented concepts and technology to provide high performance infrastructure for solar applications in this paper an architecture for data brokerage service is proposed brokers interact with providers and consumers in order to build profile of both parties in particular broker interacts with providers in order to gather information on the data potentially available to consumers and with the consumers in order to identify the set of providers that are most likely to satisfy specific data needs the brokerage technique is based on multi tier management of metadata copyright copy john wiley sons ltd
superimposition is composition technique that has been applied successfully in many areas of software development although superimposition is general purpose concept it has been re invented and implemented individually for various kinds of software artifacts we unify languages and tools that rely on superimposition by using the language independent model of feature structure trees fsts on the basis of the fst model we propose general approach to the composition of software artifacts written in different languages furthermore we offer supporting framework and tool chain called featurehouse we use attribute grammars to automate the integration of additional languages in particular we have integrated java haskell javacc and xml several case studies demonstrate the practicality and scalability of our approach and reveal insights into the properties language must have in order to be ready for superimposition
probabilistic top ranking queries have been extensively studied due to the fact that data obtained can be uncertain in many real applications probabilistic top ranking query ranks objects by the interplay of score and probability with an implicit assumption that both scores based on which objects are ranked and probabilities of the existence of the objects are stored in the same relation we observe that in general scores and probabilities are highly possible to be stored in different relations for example in column oriented dbmss and in data warehouses in this paper we study probabilistic top ranking queries when scores and probabilities are stored in different relations we focus on reducing the join cost in probabilistic top ranking we investigate two probabilistic score functions discuss the upper lower bounds in random access and sequential access and provide insights on the advantages and disadvantages of random sequential access in terms of upper lower bounds we also propose random sequential and hybrid algorithms to conduct probabilistic top ranking we conducted extensive performance studies using real and synthetic datasets and report our findings in this paper
we describe polynomial time algorithm for global value numbering which is the problem of discovering equivalences among program sub expressions we treat all conditionals as non deterministic and all program operators as uninterpreted we show that there are programs for which the set of all equivalences contains terms whose value graph representation requires exponential size our algorithm discovers all equivalences among terms of size at most in time that grows linearly with for global value numbering it suffices to choose to be the size of the program earlier deterministic algorithms for the same problem are either incomplete or take exponential time we provide detailed analytical comparison of some of these algorithms
level one cache normally resides on processor's critical path which determines the clock frequency directmapped caches exhibit fast access time but poor hit rates compared with same sized set associative caches due to nonuniform accesses to the cache sets which generate more conflict misses in some sets while other sets are underutilized we propose technique to reduce the miss rate of direct mapped caches through balancing the accesses to cache sets we increase the decoder length and thus reduce the accesses to heavily used sets without dynamically detecting the cache set usage information we introduce replacement policy to direct mapped cache design and increase the access to the underutilized cache sets with the help of programmable decoders on average the proposed balanced cache or bcache achieves and miss rate reductions on all speck benchmarks for the instruction and data caches respectively this translates into an average ipc improvement of the cache consumes more power per access but exhibits total memory access related energy saving due to the miss rate reductions and hence the reduction to applications execution time compared with previous techniques that aim at reducing the miss rate of direct mapped caches our technique requires only one cycle to access all cache hits and has the same access time of direct mapped cache
the problem of results merging in distributed information retrieval environments has been approached by two different directions in research estimation approaches attempt to calculate the relevance of the returned documents through ad hoc methodologies weighted score merging regression etc while download approaches download all the documents locally partially or completely in order to estimate first hand their relevance both have their advantages and disadvantages it is assumed that download algorithms are more effective but they are very expensive in terms of time and bandwidth estimation approaches on the other hand usually rely on document relevance scores being returned by the remote collections in order to achieve maximum performance in addition to that regression algorithms which have proved to be more effective than weighted scores merging rely on significant number of overlap documents in order to function effectively practically requiring multiple interactions with the remote collections the new algorithm that is introduced reconciles the above two approaches combining their strengths while minimizing their weaknesses it is based on downloading limited selected number of documents from the remote collections and estimating the relevance of the rest through regression methodologies the proposed algorithm is tested in variety of settings and its performance is found to be better than estimation approaches while approximating that of download
it is well known that grid technology has the ability to coordinate shared resources and scheduled tasks however the problem of resource management and task scheduling has always been one of the main challenges in this paper we present performance effective pre scheduling strategy for dispatching tasks onto heterogeneous processors the main extension of this study is the consideration of heterogeneous communication overheads in grid systems one significant improvement of our approach is that average turnaround time could be minimized by selecting the processor that has the smallest communication ratio first the other advantage of the proposed method is that system throughput can be increased by dispersing processor idle time our proposed technique can be applied on heterogeneous cluster systems as well as computational grid environments in which the communication costs vary in different clusters to evaluate performance of the proposed techniques we have implemented the proposed algorithms along with previous methods the experimental results show that our techniques outperform other algorithms in terms of lower average turnaround time higher average throughput less processor idle time and higher processors utilization
recent years have witnessed large body of research work on mining concept drifting data streams where primary assumption is that the up to date data chunk and the yet to come data chunk share identical distributions so classifiers with good performance on the up to date chunk would also have good prediction accuracy on the yet to come data chunk this stationary assumption however does not capture the concept drifting reality in data streams more recently learnable assumption has been proposed and allows the distribution of each data chunk to evolve randomly although this assumption is capable of describing the concept drifting in data streams it is still inadequate to represent real world data streams which usually suffer from noisy data as well as the drifting concepts in this paper we propose realistic assumption which asserts that the difficulties of mining data streams are mainly caused by both concept drifting and noisy data chunks consequently we present new aggregate ensemble ae framework which trains base classifiers using different learning algorithms on different data chunks all the base classifiers are then combined to form classifier ensemble through model averaging experimental results on synthetic and real world data show that ae is superior to other ensemble methods under our new realistic assumption for noisy data streams
it is becoming apparent that the next generation ip route lookup architecture needs to achieve speeds of gbps and beyond while supporting both ipv and ipv with fast real time updates to accommodate ever growing routing tables some of the proposed multibit trie based schemes such as tree bitmap have been used in today's high end routers however their large data structure often requires multiple external memory accesses for each route lookup pipelining technique is widely used to achieve high speed lookup with cost of using many external memory chips pipelining also often leads to poor memory load balancing in this paper we propose new ip route lookup architecture called flashtrie that overcomes the shortcomings of the multibit trie based approach we use hash based membership query to limit off chip memory accesses per lookup to one and to balance memory utilization among the memory modules we also develop new data structure called prefix compressed trie that reduces the size of bitmap by more than our simulation and implementation results show that flashtrie can achieve gbps worst case throughput while simultaneously supporting prefixes for ipv and prefixes for ipv using one fpga chip and four ddr sdram chips flashtrie also supports incremental real time updates
we present an adaptive fault tolerant wormhole routing algorithm for hypercubes by using virtual networks the routing algorithm can tolerate at least faulty nodes and can route message via path of length no more than the shortest path plus four previous algorithms which achieve the same fault tolerant ability need virtual networks simulation results are also given in this paper
the advent of strong multi level partitioners has made topdown min cut placers favored choice for modern placer implementations we examine terminal propagation an important step in min cut placers because it is responsible for translating partitioning results into global placement wirelength assumptions in this work we identify previously overlooked problem ambiguous terminal propagation and propose solution based on the concept of feedback from automatic control systems implementing our approach in capo version and applying it to standard benchmark circuits yields up to wirelength reductions for the ibm benchmarks and reductions for peko instances experiments also show consistent improvements for routed wirelength yielding up to wirelength reductions with practical increase in placement runtime in addition our method significantly improves routability without building congestion maps and reduces the number of vias
efficient fine grain synchronization is extremely important to effectively harness the computational power of many core architectures however designing and implementing finegrain synchronization in such architectures presents several challenges including issues of synchronization induced overhead storage cost scalability and the level of granularity to which synchronization is applicable this paper proposes the synchronization state buffer ssb scalable architectural design for fine grain synchronization that efficiently performs synchronizations between concurrent threads the design of ssb is motivated by the following observation at any instance during the parallel execution only small fraction of memory locations are actively participating in synchronization based on this observation we present fine grain synchronization design that records and manages the states of frequently synchronized data using modest hardware support we have implemented the ssb design in the context of the core ibm cyclops architecture using detailed simulation we present our experience for set of benchmarks with different workload characteristics
pattern matching in concrete syntax is very useful in program manipulation tools in particular user defined extensions to such tools are written much easier using concrete syntax patterns few advanced frameworks for language development implement support for concrete syntax patterns but mainstream frameworks used today still do not support them this prevents most existing program manipulation tools from using concrete syntax matching which in particular severely limits the writing of tool extensions to few language experts this paper argues that the major implementation obstacle to the pervasive use of concrete syntax patterns is the pattern parser we propose an alternative approach based on unparsed patterns which are concrete syntax patterns that can be efficiently matched without being parsed this lighter approach gives up static checks that parsed patterns usually do in turn it can be integrated within any existing parser based software tool almost for free one possible consequence is enabling widespread adoption of extensible program manipulation tools by the majority of programmers unparsed patterns can be used in any programing language including multi lingual environments to demonstrate our approach we implemented it both as minimal patch for the gcc compiler allowing to scan source code for user defined patterns and as stand alone prototype called matchbox
depth of field refers to the swath through scene that is imaged in acceptable focus through an optics system such as camera lens control over depth of field is an important artistic tool that can be used to emphasize the subject of photograph in real camera the control over depth of field is limited by the nature of the image formation process and by physical constraints the depth of field effect has been simulated in computer graphics but with the same limited control as found in real camera lenses in this paper we use diffusion in non homogeneous medium to generalize depth of field in computer graphics by enabling the user to independently specify the degree of blur at each point in three dimensional space generalized depth of field provides novel tool to emphasize an area of interest within scene to pick objects out of crowd and to render busy complex picture more understandable by focusing only on relevant details that may be scattered throughout the scene our algorithm operates by blurring sequence of nonplanar layers that form the scene choosing suitable blur algorithm for the layers is critical thus we develop appropriate blur semantics such that the blur algorithm will properly generalize depth of field we found that diffusion in non homogeneous medium is the process that best suits these semantics
this paper introduces the expander new object oriented oo programming language construct designed to support object adaptation expanders allow existing classes to be noninvasively updated with new methods fields and superinterfaces each client can customize its view of class by explicitly importing any number of expanders this view then applies to all instances of that class including objects passed to the client from other components form of expander overriding allows expanders to interact naturally with oo style inheritancewe describe the design implementation and evaluation of ejava an extension to java supporting expanders we illustrate ejava's syntax and semantics through several examples the statically scoped nature of expander usage allows for modular static type system that prevents several important classes of errors we describe this modular static type system informally formalize ejava and its type system in an extension to featherweight java and prove type soundness theorem for the formalization we also describe modular compilation strategy for ejava which we have implemented using the polyglot extensible compiler framework finally we illustrate the practical benefits of ejava by using this compiler in two experiments
methods for triangle mesh decimation are common however most existing techniques operate only on static geometry in this paper we present view and pose independent method for the automatic simplification of skeletally articulated meshes such meshes have associated kinematic skeletons that are used to control their deformation with the position of each vertex influenced by linear combination of bone transformations our method extends the commonly used quadric error metric by incorporating knowledge of potential poses into probability function we minimize the average error of the deforming mesh over all possible configurations weighted by the probability this is possible by transforming the quadrics from each configuration into common coordinate system our simplification algorithm runs as preprocess and the resulting meshes can be seamlessly integrated into existing systems we demonstrate the effectiveness of this approach for generating highly simplified models while preserving necessary detail in deforming regions near joints
reliable broadband communication is becoming increasingly important during disaster recovery and emergency response operations in situations where infrastructure based communication is not available or has been disrupted an incident area network needs to be dynamically deployed ie temporary network that provides communication services for efficient crisis management at an incident site wireless mesh networks wmns are multi hop wireless networks with self healing and self configuring capabilities these features combined with the ability to provide wireless broadband connectivity at comparably low cost make wmns promising technology for incident management communications this paper specifically focuses on hybrid wmns which allow both mobile client devices as well as dedicated infrastructure nodes to form the network and provide routing and forwarding functionality hybrid wmns are the most generic and most flexible type of mesh networks and are ideally suited to meet the requirements of incident area communications however current wireless mesh and ad hoc routing protocols do not perform well in hybrid wmn and are not able to establish stable and high throughput communication paths one of the key reasons for this is their inability to exploit the typical high degree of heterogeneity in hybrid wmns safemesh the routing protocol presented in this paper addresses the limitations of current mesh and ad hoc routing protocols in the context of hybrid wmns safemesh is based on the well known aodv routing protocol and implements number of modifications and extensions that significantly improve its performance in hybrid wmns this is demonstrated via an extensive set of simulation results we further show the practicality of the protocol through prototype implementation and provide performance results obtained from small scale testbed deployment
topic modeling has been key problem for document analysis one of the canonical approaches for topic modeling is probabilistic latent semantic indexing which maximizes the joint probability of documents and terms in the corpus the major disadvantage of plsi is that it estimates the probability distribution of each document on the hidden topics independently and the number of parameters in the model grows linearly with the size of the corpus which leads to serious problems with overfitting latent dirichlet allocation lda is proposed to overcome this problem by treating the probability distribution of each document over topics as hidden random variable both of these two methods discover the hidden topics in the euclidean space however there is no convincing evidence that the document space is euclidean or flat therefore it is more natural and reasonable to assume that the document space is manifold either linear or nonlinear in this paper we consider the problem of topic modeling on intrinsic document manifold specifically we propose novel algorithm called laplacian probabilistic latent semantic indexing lapplsi for topic modeling lapplsi models the document space as submanifold embedded in the ambient space and directly performs the topic modeling on this document manifold in question we compare the proposed lapplsi approach with plsi and lda on three text data sets experimental results show that lapplsi provides better representation in the sense of semantic structure
the system and network architecture for static sensor nets is largely solved today with many stable commercial solutions now available and standardization efforts underway at the ieee ietf isa and within many industry groups as result many researchers have begun to explore new domains like mobile sensor networks or mobiscopes since they enable new applications in the home and office for health and safety and in transportation and asset management this paper argues that mobility invalidates many assumptions implicit in low power static designs so the architecture for micropower mobiscopes is still very much an open research question in this paper we explore several mobile sensing applications identify research challenges to their realization and explore how emerging technologies and real time motion data could help ease these challenges
software maintenance tools for program analysisand refactoring rely on meta model capturing the relevantproperties of programs however what is considered relevantmay change when the tools are extended with new analyses andrefactorings and new programming languages this paper proposesa language independent meta model and an architecture toconstruct instances thereof which is extensible for new analyses refactorings and new front ends of programming languages dueto the loose coupling between analysis refactoring and frontend components new components can be added independentlyand reuse existing ones two maintenance tools implementingthe meta model and the architecture vizzanalyzer and xdevelop serve as proof of concept
this article explores the architectural challenges introduced by emerging bottom up fabrication of nanoelectronic circuits the specific nanotechnology we explore proposes patterned dna nanostructures as scaffold for the placement and interconnection of carbon nanotube or silicon nanorod fets to create limited size circuit node three characteristics of this technology that significantly impact architecture are limited node size random node interconnection and high defect rates we present and evaluate an accumulator based active network architecture that is compatible with any technology that presents these three challenges this architecture represents an initial unoptimized solution for understanding the implications of dna guide self assembly
structured peer to peer pp overlays have been successfully employed in many applications to locate content however they have been less effective in handling massive amounts of data because of the high overhead of maintaining indexes in this paper we propose pisces peer based system that indexes selected content for efficient search unlike traditional approaches that index all data pisces identifies subset of tuples to index based on some criteria such as query frequency update frequency index cost etc in addition coarse grained range index is built to facilitate the processing of queries that cannot be fully answered by the tuple level index more importantly pisces can adaptively self tune to optimize the subset of tuples to be indexed that is the partial index in pisces is built in just in time jit manner beneficial tuples for current users are pulled for indexing while indexed tuples with infrequent access and high maintenance cost are discarded we also introduce light weight monitoring scheme for structured networks to collect the necessary statistics we have conducted an extensive experimental study on planetlab to illustrate the feasibility practicality and efficiency of pisces the results show that pisces incurs lower maintenance cost and offers better search and query efficiency compared to existing methods
vast amount of valuable information produced and consumed by people and institutions is currently stored in relational databases for many purposes there is an ever increasing demand for having these databases published on the web so that users can query the data available in them an important requirement for this to happen is that query interfaces must be as simple and intuitive as possible in this paper we present labrador system for efficiently publishing relational databases on the web by using simple text box query interface the system operates by taking an unstructured keyword based query posed by user and automatically deriving an equivalent sql query that fits the user's information needs as expressed by the original query the sql query is then sent to dbms and its results are processed by labrador to create relevance based ranking of the answers experiments we present show that labrador can automatically find the most suitable sql query in more than of the cases and that the overhead introduced by the system in the overall query processing time is almost insignificant furthermore the system operates in non intrusive way since it requires no modifications to the target database schema
discriminative probabilistic models are very popular in nlp because of the latitude they afford in designing features but training involves complex trade offs among weights which can be dangerous few highly indicative features can swamp the contribution of many individually weaker features causing their weights to be undertrained such model is less robust for the highly indicative features may be noisy or missing in the test data to ameliorate this weight undertraining we introduce several new feature bagging methods in which separate models are trained on subsets of the original features and combined using mixture model or product of experts these methods include the logarithmic opinion pools used by smith et al we evaluate feature bagging on linear chain conditional random fields for two natural language tasks on both tasks the feature bagged crf performs better than simply training single crf on all the features
java programmers can document that the relationship between two objects is unchanging by declaring the field that encodes that relationship to be final this information can be used in program understanding and detection of errors in new code additions unfortunately few fields in programs are actually declared final programs often contain fields that could be final but are not declared so moreover the definition of final has restrictions on initializationthat limit its applicability we introduce stationary fields as generalization of final field in program is stationary if for every object that contains it all writes to the field occur before all the reads unlike the definition of final fields there can be multiple writes during initialization and initialization can span multiple methods we have developed an efficient algorithm for inferring which fields are stationary in program based on the observation that many fields acquire their value very close to object creation we presume that an object's initialization phase has concluded when its reference is saved in some heap object we perform precise analysis only regarding recently created objects applying our algorithm to real world java programs demonstrates that stationary fields are more common than final fields vs respectively in our benchmarks these surprising results have several significant implications first substantial portions of java programs appear to be written in functional style second initialization of these fields occurs very close to object creation when very good alias information is available these results open the door for more accurate and efficient pointer alias analysis
this article proposes statistical approach for fast articulated body tracking similar to the loose limbed model but using the factor graph representation and fast estimation algorithm fast nonparametric belief propagation on factor graphs is used to estimate the current marginal for each limb all belief propagation messages are represented as sums of weighted samples the resulting algorithm corresponds to set of particle filters one for each limb where an extra step recomputes the weight of each sample by taking into account the links between limbs applied to upper body tracking with stereo and colour images the resulting algorithm estimates the body pose in quasi real time hz results on sequences illustrate the effectiveness of this approach
we focus on collaborative filtering dealing with self organizing communities host mobility wireless access and ad hoc communications in such domain knowledge representation and users profiling can be hard remote servers can be often unreachable due to client mobility and feedback ratings collected during random connections to other users ad hoc devices can be useless because of natural differences between human beings our approach is based on so called affinity networks and on novel system called mobhinter that epidemically spreads recommendations through spontaneous similarities between users main results of our study are two fold firstly we show how to reach comparable recommendation accuracies in the mobile domain as well as in complete knowledge scenario secondly we propose epidemic collaborative strategies that can reduce rapidly and realistically the cold start problem
different notions of provenance for database queries have been proposed and studied in the past few years in this article we detail three main notions of database provenance some of their applications and compare and contrast amongst them specifically we review why how and where provenance describe the relationships among these notions of provenance and describe some of their applications in confidence computation view maintenance and update debugging and annotation propagation
we present novel approach for classifying documents that combines different pieces of evidence eg textual features of documents links and citations transparently through data mining technique which generates rules associating these pieces of evidence to predefined classes these rules can contain any number and mixture of the available evidence and are associated with several quality criteria which can be used in conjunction to choose the best rule to be applied at classification time our method is able to perform evidence enhancement by link forwarding backwarding ie navigating among documents related through citation so that new pieces of link based evidence are derived when necessary furthermore instead of inducing single model or rule set that is good on average for all predictions the proposed approach employs lazy method which delays the inductive process until document is given for classification therefore taking advantage of better qualitative evidence coming from the document we conducted systematic evaluation of the proposed approach using documents from the acm digital library and from brazilian web directory our approach was able to outperform in both collections all classifiers based on the best available evidence in isolation as well as state of the art multi evidence classifiers we also evaluated our approach using the standard webkb collection where our approach showed gains of in accuracy being times faster further our approach is extremely efficient in terms of computational performance showing gains of more than one order of magnitude when compared against other multi evidence classifiers
distributed digital libraries dls integration is significant for the enforcement of novel searching mechanisms in the internet the great heterogeneity of systems storing and providing digital content requires the introduction of interoperability aspects in order to resolve integration problems in flexible and dynamic way our approach introduces an innovative service oriented pp system which initialises distributed ontology schema semantically describing and indexing the digital content stored in distributed dls the proposed architecture enforces the distributed semantic index by defining virtual clusters consisting of nodes peers with similar or related content in order to provide efficient searching and recommendation mechanisms furthermore use case example is presented in order to demonstrate the functionalities of the proposed architecture in pp network with dls containing cultural material
the maintenance of an existing database depends on the depth of understanding of its characteristics such an understanding is easily lost when the developers disperse the situation becomes worse when the related documentation is missing this paper addresses this issue by extracting the extended entity relationship schema from the relational schema we developed algorithms that investigate characteristics of an existing legacy database in order to identify candidate keys of all relations in the relational schema to locate foreign keys and to decide on the appropriate links between the given relations based on this analysis graph consistent with the entity relationship diagram is derived to contain all possible uniary and binary relationships between the given relations the minimum and maximum cardinalities of each link in the mentioned graph are determined and extra links within the graph are identified and categorized if any the latter information is necessary to optimize foreign keys related information finally the last steps in the process involve when applicable suggesting improvements on the original conceptual design deciding on relationships with attributes many to many and ary relationships and identifying is links user involvement in the process is minimized to the case of having multiple choices where the system does not have the semantic knowledge required to decide on certain choice
in this paper we propose new learning method for extracting bilingual word pairs from parallel corpora in various languages in cross language information retrieval the system must deal with various languages therefore automatic extraction of bilingual word pairs from parallel corpora with various languages is important however previous works based on statistical methods are insufficient because of the sparse data problem our learning method automatically acquires rules which are effective to solve the sparse data problem only from parallel corpora without any prior preparation of bilingual resource eg bilingual dictionary machine translation system we call this learning method inductive chain learning icl moreover the system using icl can extract bilingual word pairs even from bilingual sentence pairs for which the grammatical structures of the source language differ from the grammatical structures of the target language because the acquired rules have the information to cope with the different word orders of source language and target language in local parts of bilingual sentence pairs evaluation experiments demonstrated that the recalls of systems based on several statistical approaches were improved through the use of icl
distributed shared memory dsm is an abstraction of shared memory on distributed memory machine hardware dsm systems support this abstraction at the architecture level software dsm systems support the abstraction within the runtime system one of the key problems in building an efficient software dsm system is to reduce the amount of communication needed to keep the distributed memories consistent in this article we present four techniques for doing so software release consistency multiple consistency protocols write shared protocols and an update with timeout mechanism these techniques have been implemented in the munin dsm system we compare the performance of seven munin application programs first to their performance when implemented using message passing and then to their performance when running on conventional software dsm system that does not embody the preceding techniques on processor cluster of workstations munin's performance is within of message passing for four out of the seven applications for the other three performance is within to detailed analysis of two of these three applications indicates that the addition of function shipping capability would bring their performance to within of the message passing performance compared to conventional dsm system munin achieves performance improvements ranging from few to several hundred percent depending on the application
in this paper we present our experiences concerning the enforcement of access rights extracted from odrl based digital contracts we introduce the generalized contract schema cosa which is an approach to provide generic representation of contract information on top of rights expression languages we give an overview of the design and implementation of the xorelinterpreter software component in particular the xorelinterpreter interprets digital contracts that are based on rights expression languages eg odrl or xrml and builds runtime cosa object model we describe how the xorbac access control component and the xorelinterpreter component are used to enforce access rights that we extract from odrl based digital contracts thus our approach describes how odrl based contracts can be used as means to disseminate certain types of access control information in distributed systems
we propose framework and methodology for quantifying the effect of denial of service dos attacks on distributed system we present systematic study of the resistance of gossip based multicast protocols to dos attacks we show that even distributed and randomized gossip based protocols which eliminate single points of failure do not necessarily eliminate vulnerabilities to dos attacks we propose drum simple gossip based multicast protocol that eliminates such vulnerabilities drum was implemented in java and tested on large cluster we show using closed form mathematical analysis simulations and empirical tests that drum survives severe dos attacks
we propose general approach for frequency based string mining which has many applications eg in contrast data mining our contribution is novel algorithm based on deferred data structure despite its simplicity our approach is up to times faster and uses about half the memory compared to the best known algorithm of fischer et al applications in various string domains eg natural language dna or protein sequences demonstrate the improvement of our algorithm
volumetric displays which provide view of imagery illuminated in true space are promising platform for interactive applications however presenting text in volumetric displays can be challenge as the text may not be oriented towards the user this is especially problematic with multiple viewers as the text could for example appear forwards to one user and backwards to another in first experiment we determined the effects of rotations on text readability based on the results we developed and evaluated new technique which optimizes text orientation for multiple viewers this technique provided faster group reading times in collaborative experimental task
although data broadcast has been shown to be an efficient method for disseminating data items in mobile computing systems the issue on how to ensure consistency and currency of data items provided to mobile transactions mt which are generated by mobile clients has not been examined adequately while data items are being broadcast update transactions may install new values for them if the executions of update transactions and the broadcast of data items are interleaved without any control mobile transactions may observe inconsistent data values the problem will be more complex if the mobile clients maintain some cached data items for their mobile transactions in this paper we propose concurrency control method called ordered update first with order oufo for the mobile computing systems where mobile transaction consists of sequence of read operations and each mt is associated with time constraint on its completion time besides ensuring data consistency and maximizing currency of data to mobile transactions oufo also aims at reducing data access delay of mobile transactions using client caches hybrid re broadcast invalidation report ir mechanism is designed in oufo for checking the validity of cached data items so as to improve cache consistency and minimize the overhead of transaction restarts due to data conflicts this is highly important to the performance of the mobile computing systems where the mobile transactions are associated with deadline constraint on their completion times extensive simulation experiments have been performed to compare the performance of oufo with two other efficient schemes the multi version broadcast method and the periodic ir method the performance results show that oufo offers better performance in most aspects even when network disconnection is common
at present the search for specific information on the world wide web is faced with several problems which arise on the one hand from the vast number of information sources available and on the other hand from their intrinsic heterogeneity since standards are missing promising approach for solving the complex problems emerging in this context is the use of multi agent systems of information agents which cooperatively solve advanced information retrieval problems this requires advanced capabilities to address complex tasks such as search and assessment of information sources query planning information merging and fusion dealing with incomplete information and handling of inconsistency in this paper our interest lies in the role which some methods from the field of declarative logic programming can play in the realization of reasoning capabilities for information agents in particular we are interested to see how they can be used extended and further developed for the specific needs of this application domain we review some existing systems and current projects which typically address information integration problems we then focus on declarative knowledge representation methods and review and evaluate approaches and methods from logic programming and nonmonotonic reasoning for information agents we discuss advantages and drawbacks and point out the possible extensions and open issues
overall performance of the data mining process depends not just on the value of the induced knowledge but also on various costs of the process itself such as the cost of acquiring and pre processing training examples the cpu cost of model induction and the cost of committed errors recently several progressive sampling strategies for maximizing the overall data mining utility have been proposed all these strategies are based on repeated acquisitions of additional training examples until utility decrease is observed in this paper we present an alternative projective sampling strategy which fits functions to partial learning curve and partial run time curve obtained from small subset of potentially available data and then uses these projected functions to analytically estimate the optimal training set size the proposed approach is evaluated on variety of benchmark datasets using the rapidminer environment for machine learning and data mining processes the results show that the learning and run time curves projected from only several data points can lead to cheaper data mining process than the common progressive sampling methods
as most current query processing architectures are already pipelined it seems logical to apply them to data streams however two classes of query operators are impractical for processing long or infinite data streams unbounded stateful operators maintain state with no upper bound in size and so run out of memory blocking operators read an entire input before emitting single output and so might never produce result we believe that priori knowledge of data stream can permit the use of such operators in some cases we discuss kind of stream semantics called punctuated streams punctuations in stream mark the end of substreams allowing us to view an infinite stream as mixture of finite streams we introduce three kinds of invariants to specify the proper behavior of operators in the presence of punctuation pass invariants define when results can be passed on keep invariants define what must be kept in local state to continue successful operation propagation invariants define when punctuation can be passed on we report on our initial implementation and show strategy for proving implementations of these invariants are faithful to their relational counterparts
with the explosive growth of the world wide web the public is gaining access to massive amounts of information however locating needed and relevant information remains difficult task whether the information is textual or visual text search engines have existed for some years now and have achieved certain degree of success however despite the large number of images available on the web image search engines are still rare in this article we show that in order to allow people to profit from all this visual information there is need to develop tools that help them to locate the needed images with good precision in reasonable time and that such tools are useful for many applications and purposes the article surveys the main characteristics of the existing systems most often cited in the literature such as imagerover webseek diogenes and atlas wise it then examines the various issues related to the design and implementation of web image search engine such as data gathering and digestion indexing query specification retrieval and similarity web coverage and performance evaluation general discussion is given for each of these issues with examples of the ways they are addressed by existing engines and related references are given some concluding remarks and directions for future research are also presented
load sensitive faults cause program to fail when it is executed under heavy load or over long period of time but may have no detrimental effect under small loads or short executions in addition to testing the functionality of these programs testing how well they perform under stress is very important current approaches to stress or load testing treat the system as black box generating test data based on parameters specified by the tester within an operational profile in this paper we advocate structural approach to load testing there exist many structural testing methods however their main goal is generating test data for executing all statements branches definition use pairs or paths of program at least once without consideration for executing any particular path extensivelyour initial work has focused on the identification of potentially load sensitive modules based on static analysis of the module's code and then limiting the stress testing to the regions of the modules that could be the potential causes of the load sensitivity this analysis will be incorporated into testing tool for structural load testing which takes program as input and automatically determines whether that program needs to be load tested and if so automatically generates test data for structural load testing of the program
developing countries face significant challenges in network access making even simple network tasks unpleasant many standard techniques caching and predictive prefetching help somewhat but provide little or no assistance for personal data that is needed only by single user sulula addresses this problem by leveraging the near ubiquity of cellular phones able to send and receive simple sms messages rather than visit kiosk and fetch data on demand tiresome process at best users request future visit if capacity exists the kiosk can schedule secure retrieval of that user's data saving time and more efficiently utilizing the kiosk's limited connectivity when the user arrives at provisioned kiosk she need only obtain the session key on demand and thereafter has instant access in addition sulula allows users to schedule data uploads experimental results show significant gains for the end user saving tens of minutes of time for typical email news reading session we also describe small ongoing deployment in country for proof of concept lessons learned from that experience and provide discussion on pricing and marketplace issues that remain to be addressed to make the system viable for developing world access
process support systems psss are software systems supporting the modeling enactment monitoring and analysis of business processes process automation technology can be fully exploited when predictable and repetitive processes are executed unfortunately many processes are faced with the need of managing exceptional situations that may occur during their execution and possibly even more exceptions and failures can occur when the process execution is supported by pss exceptional situations may be caused by system hardware or software failures or may by related to the semantics of the business processin this paper we introduce taxonomy of failures and exceptions and discuss the effect that they can have on pss and on its ability to support business processes then we present the main approaches that commercial psss and research prototypes offer in order to capture and react to exceptional situations and we show which classes of failure or exception can be managed by each approach
we report on model based approach to system software co engineering which is tailored to the specific characteristics of critical on board systems for the aerospace domain the approach is supported by system level integrated modeling slim language by which engineers are provided with convenient ways to describe nominal hardware and software operation probabilistic faults and their propagation error recovery and degraded modes of operationcorrectness properties safety guarantees and performance and dependability requirements are given using property patterns which act as parameterized templates to the engineers and thus offer comprehensible and easy to use framework for requirement specification instantiated properties are checked on the slim specification using state of the art formal analysis techniques such as bounded sat based and symbolic model checking and probabilistic variants thereof the precise nature of these techniques together with the formal slim semantics yield trustworthy modeling and analysis framework for system and software engineers supporting among others automated derivation of dynamic ie randomly timed fault trees fmea tables assessment of fdir and automated derivation of observability requirements
there is tremendous amount of web content available today but it is not always in form that supports end users needs in many cases all of the data and services needed to accomplish goal already exist but are not in form amenable to an end user to address this problem we have developed an end user programming tool called marmite which lets end users create so called mashups that re purpose and combine existing web content and services in this paper we present the design implementation and evaluation of marmite an informal user study found that programmers and some spreadsheet users had little difficulty using the system
we present system that composes realistic picture from simple freehand sketch annotated with text labels the composed picture is generated by seamlessly stitching several photographs in agreement with the sketch and text labels these are found by searching the internet although online image search generates many inappropriate results our system is able to automatically select suitable photographs to generate high quality composition using filtering scheme to exclude undesirable images we also provide novel image blending algorithm to allow seamless image composition each blending result is given numeric score allowing us to find an optimal combination of discovered images experimental results show the method is very successful we also evaluate our system using the results from two user studies
in large scale sensor networks sensor nodes are at high risk of being captured and compromised once sensor node is compromised all the secret keys data and code stored on it are exposed to the attacker the attacker can insert arbitrary malicious code in the compromised node moreover he can easily replicate it in large number of clones and deploy them on the network this node replication attack can form the basis of variety of attacks such as dos attacks and sybil attacks previous studies of node replication attacks have had some drawbacks they need central trusted entity or they become vulnerable when many nodes are compromised therefore we propose distributed protocol for detecting node replication attacks that is resilient to many compromised nodes our method does not need any reliable entities and has high detection rate of replicated nodes our analysis and simulations demonstrate our protocol is effective even when there are large number of compromised nodes
developing and maintaining open source software has become an important source of profit for many companies change prone classes in open source products increase project costs by requiring developers to spend effort and time identifying and characterizing change prone classes can enable developers to focus timely preventive actions for example peer reviews and inspections on the classes with similar characteristics in the future releases or products in this study we collected set of static metrics and change data at class level from two open source projects koffice and mozilla using these data we first tested and validated pareto's law which implies that great majority around of change is rooted in small proportion around of classes then we identified and characterized the change prone classes in the two products by producing tree based models in addition using tree based models we suggested prioritization strategy to use project resources for focused preventive actions in an efficient manner our empirical results showed that this strategy was effective for prioritization purposes this study should provide useful guidance to practitioners involved in development and maintenance of large scale open source products
we study auctions whose bidders are embedded in social or economic network as result even bidders who do not win the auction themselves might derive utility from the auction namely when friend wins on the other hand when an enemy or competitor wins bidder might derive negative utility such spite and altruism will alter the bidding strategies simple and natural model for bidders utilities in these settings posits that the utility of losing bidder as result of bidder winning is constant positive or negative fraction of bidder j's utilitywe study such auctions under bayesian model in which all valuations are distributed independently according to known distribution but the actual valuations are private we describe and analyze nash equilibrium bidding strategies in two broad classes regular friendship networks with arbitrary valuation distributions and arbitrary friendship networks with identical uniform valuation distributions
the process of network debugging is commonly guided by decision trees that describe and attempt to address the most common failure modes we show that troubleshooting can be made more effective by converting decision trees into suites of convergent troubleshooting scripts that do not change network attributes unless these are out of compliance with accepted norms maelstrom is tool for managing and coordinating execution of these scripts maelstrom exploits convergence of individual scripts to dynamically infer an appropriate execution order for the scripts it accomplishes this in procedure trials where is the number of troubleshooting scripts this greatly eases adding scripts to troubleshooting scheme and thus makes it easier for people to cooperate in producing more exhaustive and effective troubleshooting schemes
recent studies have indicated an increase in customer profiling techniques used by commerce businesses commerce businesses are creating maintaining and utilising customer profiles to assist in personalisation personalisation can help improve customers satisfaction levels purchasing behaviour loyalty and subsequently improve sales the continuously changing customer needs and preferences pose challenge to commerce businesses on how to maintain and update individual customer profiles to reflect any changes in customers needs and preferences this research set out to investigate how dynamic customer profile for on line customers can be updated and maintained taking into consideration individual web visitors activities the research designed and implemented decision model that analysed on line customers activities during interaction sessions and determined whether to update customers profiles or not evaluation results indicated that the model was able to analyse the on line customers activities from log file and successfully updated the customers profiles based on the customer activities undertaken during the interaction session
unlike conventional rule based knowledge bases kbs that support monotonic reasoning key correctness issue ie the correctness of sub kb with respect to the full kb arises when using kb represented by non monotonic reasoning languages such as answer set programming asp since user may have rights to access only subset of kb the non monotonic nature of asp may cause the occurrence of consequences which are erroneous in the sense that the consequences are not reasonable in the full kb this paper proposes an approach dealing with the problem the main idea is to let the usage of closed world assumptions cwas for literals in kb satisfy certain constraints two kinds of access right propositions are created rule retrieval right propositions to control the access to rules and cwa right propositions to control the usage of cwas for literals based on these right propositions this paper first defines an algorithm for translating an original kb into kb tagged by right propositions and then discusses the right dependency in kb and proposes methods for checking and obtaining set of rights that is closed under set of dependency rules finally several results on the correctness of set of rights in kb are presented which serve as guidelines for the correct use of kb as an example kb of illness related financial support for teachers of university is presented to illustrate the application of our approach
the minimum energy broadcast routing problem was extensively studied during the last years given sample space where wireless devices are distributed the aim is to perform the broadcast communication pattern from given source while minimising the total energy consumption while many papers deal with the dimensional case where the sample space is given by plain area few results are known about the more interesting and practical dimensional case in this paper we study this case and we present tighter analysis of the minimum spanning tree heuristic in order to considerably decrease its approximation factor from the known to roughly this decreases the gap with the known lower bound of given by the so called dimensional kissing number
the collection of digital information by governments corporations and individuals has created tremendous opportunities for knowledge and information based decision making driven by mutual benefits or by regulations that require certain data to be published there is demand for the exchange and publication of data among various parties data in its original form however typically contains sensitive information about individuals and publishing such data will violate individual privacy the current practice in data publishing relies mainly on policies and guidelines as to what types of data can be published and on agreements on the use of published data this approach alone may lead to excessive data distortion or insufficient protection privacy preserving data publishing ppdp provides methods and tools for publishing useful information while preserving data privacy recently ppdp has received considerable attention in research communities and many approaches have been proposed for different data publishing scenarios in this survey we will systematically summarize and evaluate different approaches to ppdp study the challenges in practical data publishing clarify the differences and requirements that distinguish ppdp from other related problems and propose future research directions
customized processors use compiler analysis and design automation techniques to take generalized architectural model and create specific instance of it which is optimized to given application or set of applications these processors offer the promise of satisfying the high performance needs of the embedded community while simultaneously shrinking design times finite state machines fsm are fundamental building block in computer architecture and are used to control and optimize all types of prediction and speculation now even in the embedded space they are used for branch prediction cache replacement policies and confidence estimation and accuracy counters for variety of optimizations in this paper we present framework for automated design of small fsm predictors for customized processors our approach can be used to automatically generate small fsm predictors to perform well over suite of applications tailored to specific application or even specific instruction we evaluate the use of these customized fsm predictors for branch prediction over set of benchmarks
scaling of interconnects exacerbates the already challenging reliability of on chip networks although many researchers have provided various fault handling techniques in chip multi processors cmps the fault tolerance of the interconnection network is yet to adequately evolve as an end to end recovery approach delays fault detection and complicates recovery to consistent global state in such system link level retransmission is endorsed for recovery making higher level protocol simple in this paper we introduce fault tolerant flow control scheme for soft error handling in on chip networks the fault tolerant flow control recovers errors at link level by requesting retransmission and ensures an error free transmission on flit basis with incorporation of dynamic packet fragmentation dynamic packet fragmentation is adopted as part of fault tolerant flow control to disengage flits from the fault containment and recover the faulty flit transmission thus the proposed router provides high level of dependability at the link level for both datapath and control planes in simulation with injected faults the proposed router is observed to perform well gracefully degrading while exhibiting error coverage in datapath elements the proposed router has been implemented using tsmc nm standard cell library as compared to router which employs triple modular redundancy tmr in datapath elements the proposed router takes less area and consumes less energy per packet on average
systems providing only exact match answers without allowing any kind of preference or approximate queries are not sufficient in many contexts many different approaches have been introduced often incompatible in their setup or proposed implementation this work shows how different kinds of preference queries prefer preference sql and skyline can be combined and answered efficiently using bit sliced index bsi arithmetic this approach has been implemented in dbms and performance results are included showing that the bit sliced index approach is efficient not only in prototype system but in real system
the management of hierarchically organized data is starting to play key role in the knowledge management community due to the proliferation of topic hierarchies for text documents the creation and maintenance of such organized repositories of information requires great deal of human interventionthe machine learning community has partially addressed this problem by developing hierarchical supervised classifiers that help people categorize new resources within given hierarchies the worst problem of hierarchical supervised classifiers however is their high demand in terms of labeled examples the number of examples required is related to the number of topics in the taxonomy bootstrapping huge hierarchy with proper set of labeled examples is therefore critical issuethis paper proposes some solutions for the bootstrapping problem that implicitly or explicitly use taxonomy definition baseline approach that classifies documents according to the class terms and two clustering approaches whose training is constrained by the priori knowledge encoded in the taxonomy structure which consists of both terminological and relational aspects in particular we propose the tax som model that clusters set of documents in predefined hierarchy of classes directly exploiting the knowledge of both their topological organization and their lexical description experimental evaluation was performed on set of taxonomies taken from the googletm and looksmarttm web directories obtaining good results
the traditional interaction mechanism with database system is through the use of query language the most widely used one being sql however when one is facing situation where he or she has to make minor modification to previously issued sql query either the whole query has to be written from scratch or one has to invoke an editor to edit the query this however is not the way we converse with each other as humans during the course of conversation the preceding interaction is used as context within which many incomplete and or incremental phrases are uniquely and unambiguously interpreted sparing the need to repeat the same things again and again in this paper we present an effective mechanism that allows user to interact with database system in way similar to the way humans converse more specifically incomplete sql queries are accepted as input which are then matched to identified parts of previously issued queries disambiguation is achieved by using various types of semantic information the overall method works independently of the domain under which it is used ie independently of the database schema several algorithms that are variations of the same basic mechanism are proposed they are mutually compared with respect to efficiency and accuracy through limited set of experiments on human subjects the results have been encouraging especially when semantic knowledge from the schema is exploited laying potential foundation for conversational querying in databases
learned activity specific motion models are useful for human pose and motion estimation nevertheless while the use of activity specific models simplifies monocular tracking it leaves open the larger issues of how one learns models for multiple activities or stylistic variations and how such models can be combined with natural transitions between activities this paper extends the gaussian process latent variable model gp lvm to address some of these issues we introduce new approach to constraining the latent space that we refer to as the locally linear gaussian process latent variable model ll gplvm the ll gplvm allows for an explicit prior over the latent configurations that aims to preserve local topological structure in the training data we reduce the computational complexity of the gplvm by adapting sparse gaussian process regression methods to the gp lvm by incorporating sparsification dynamics and back constraints within the ll gplvm we develop general framework for learning smooth latent models of different activities within shared latent space allowing the learning of specific topologies and transitions between different activities
effective system verification requires good specifications the lack of sufficient specifications can lead to misses of critical bugs design re spins and time to market slips in this paper we present new technique for mining temporal specifications from simulation or execution traces of digital hardware design given an execution trace we mine recurring temporal behaviors in the trace that match set of pattern templates subsequently we synthesize them into complex patterns by merging events in time and chaining the patterns using inference rules we specifically designed our algorithm to make it highly efficient and meaningful for digital circuits in addition we propose pattern mining diagnosis framework where specifications mined from correct and erroneous traces are used to automatically localize an error we demonstrate the effectiveness of our approach on industrial size examples by mining specifications from traces of over million cycles in few minutes and use them to successfully localize errors of different types to within module boundaries
mobile robots that interact with humans in an intuitive way must be able to follow directions provided by humans in unconstrained natural language in this work we investigate how statistical machine translation techniques can be used to bridge the gap between natural language route instructions and map of an environment built by robot our approach uses training data to learn to translate from natural language instructions to an automatically labeled map the complexity of the translation process is controlled by taking advantage of physical constraints imposed by the map as result our technique can efficiently handle uncertainty in both map labeling and parsing our experiments demonstrate the promising capabilities achieved by our approach
haskell programmers often use multi parameter type class in which one or more type parameters are functionally dependent on the first although such functional dependencies have proved quite popular in practice they express the programmer's intent somewhat indirectly developing earlier work on associated data types we propose to add functionally dependent types as type synonyms to type class bodies these associated type synonyms constitute an interesting new alternative to explicit functional dependencies
software monitoring is technique that is well suited to supporting the development of dependable system it has been widely applied not only for this purpose but also for other purposes such as debugging security performance evaluation and enhancement etc however there is an inherent gap between the levels of abstraction of the information that is collected during software monitoring the implementation level and that of the software architecture level where many design decisions are made unless an immediate structural one to one architecture to implementation mapping takes place we need specification language to describe how low level events are related to higher level ones although some specification languages for monitoring have been proposed in the literature they do not provide support up to the software architecture level in addition these languages make it harder to link to and reuse information from other event based models often employed for reliability analysis in this paper we discuss the importance of event description as an integration element for architecting dependable systems
there is growing interest in algorithms for processing and querying continuous data streams ie data seen only once in fixed order with limited memory resources in its most general form data stream is actually an update stream ie comprising data item deletions as well as insertions such massive update streams arise naturally in several application domains eg monitoring of large ip network installations or processing of retail chain transactions estimating the cardinality of set expressions defined over several possibly distributed update streams is perhaps one of the most fundamental query classes of interest as an example such query may ask xc what is the number of distinct ip source addresses seen in passing packets from both router sub sub and sub sub but not router sub sub xd earlier work only addressed very restricted forms of this problem focusing solely on the special case of insert only streams and specific operators eg union in this paper we propose the first space efficient algorithmic solution for estimating the cardinality of full fledged set expressions over general update streams our estimation algorithms are probabilistic in nature and rely on novel hash based synopsis data structure termed xd level hash sketch xd we demonstrate how our level hash sketch synopses can be used to provide low error high confidence estimates for the cardinality of set expressions including operators such as set union intersection and difference over continuous update streams using only space that is significantly sublinear in the sizes of the streaming input multi sets furthermore our estimators never require rescanning or resampling of past stream items regardless of the number of deletions in the stream we also present lower bounds for the problem demonstrating that the space usage of our estimation algorithms is within small factors of the optimal finally we propose an optimized time efficient stream synopsis based on level hash sketches that provides similar strong accuracy space guarantees while requiring only guaranteed logarithmic maintenance time per update thus making our methods applicable for truly rapid rate data streams our results from an empirical study of our synopsis and estimation techniques verify the effectiveness of our approach
internet hosting centers serve multiple service sites from common hardware base this paper presents the design and implementation of an architecture for resource management in hosting center operating system with an emphasis on energy as driving resource management issue for large server clusters the goals are to provision server resources for co hosted services in way that automatically adapts to offered load improve the energy efficiency of server clusters by dynamically resizing the active server set and respond to power supply disruptions or thermal events by degrading service in accordance with negotiated service level agreements slas our system is based on an economic approach to managing shared server resources in which services bid for resources as function of delivered performance the system continuously monitors load and plans resource allotments by estimating the value of their effects on service performance greedy resource allocation algorithm adjusts resource prices to balance supply and demand allocating resources to their most efficient use reconfigurable server switching infrastructure directs request traffic to the servers assigned to each service experimental results from prototype confirm that the system adapts to offered load and resource availability and can reduce server energy usage by or more for typical web workload
user behavior information analysis has been shown important for optimization and evaluation of web search and has become one of the major areas in both information retrieval and knowledge management researches this paper focuses on users searching behavior reliability study based on large scale query and click through logs collected from commercial search engines the concept of reliability is defined in probabilistic notion the context of user click behavior on search results is analyzed in terms of relevance five features namely query number click entropy first click ratio last click ratio and rank position are proposed and studied to separate reliable user clicks from the others experimental results show that the proposed method evaluates the reliability of user behavior effectively the auc value of the roc curve is and the algorithm maintains relevant clicks when filtering out low quality clicks
we present our experience in implementing group communication toolkit in objective caml dialect of the ml family of programming languages we compare the toolkit both quantitatively and qualitatively to predecessor toolkit which was implemented in our experience shows that using the high level abstraction features of ml gives substantial advantages some of these features such as automatic memory management and message marshalling allowed us to concentrate on those pieces of the implementation which required careful attention in order to achieve good performance we conclude with set of suggested changes to ml implementations
soft state is an often cited yet vague concept in network protocol design in which two or more network entities intercommunicate in loosely coupled often anonymous fashion researchers often define this concept operationally if at all rather than analytically source of soft state transmits periodic refresh messages over lossy communication channel to one or more receivers that maintain copy of that state which in turn expires if the periodic updates cease though number of crucial internet protocol building blocks are rooted in soft state based designs eg rsvp refresh messages pim membership updates various routing protocol updates rtcp control messages directory services like sap and so forth controversy is building as to whether the performance overhead of soft state refresh messages justify their qualitative benefit of enhanced system robustness we believe that this controversy has risen not from fundamental performance tradeoffs but rather from our lack of comprehensive understanding of soft state to better understand these tradeoffs we propose herein formal model for soft state communication based on probabilistic delivery model with relaxed reliability using this model we conduct queueing analysis and simulation to characterize the data consistency and performance tradeoffs under range of workloads and network loss rates we then extend our model with feedback and show through simulation that adding feedback dramatically improves data consistency by up to without increasing network resource consumption our model not only provides foundation for understanding soft state but also induces new fundamental transport protocol based on probabilistic delivery toward this end we sketch our design of the soft state transport protocol sstp which enjoys the robustness of soft state while retaining the performance benefit of hard state protocols like tcp through its judicious use of feedback
the development of the semantic web will require agents to use common domain ontologies to facilitate communication of conceptual knowledge however the proliferation of domain ontologies may also result in conflicts between the meanings assigned to the various terms that is agents with diverse ontologies may use different terms to refer to the same meaning or the same term to refer to different meanings agents will need method for learning and translating similar semantic concepts between diverse ontologies only until recently have researchers diverged from the last decade's ldquo common ontology rdquo paradigm to paradigm involving agents that can share knowledge using diverse ontologies this paper describes how we address this agent knowledge sharing problem of how agents deal with diverse ontologies by introducing methodology and algorithms for multi agent knowledge sharing and learning in peer to peer setting we demonstrate how this approach will enable multi agent systems to assist groups of people in locating translating and sharing knowledge using our distributed ontology gathering group integration environment doggie and describe our proof of concept experiments doggie synthesizes agent communication machine learning and reasoning for information sharing in the web domain
we propose an energy efficient framework called saf for approximate querying and clustering of nodes in sensor network saf uses simple time series forecasting models to predict sensor readings the idea is to build these local models at each node transmit them to the root of the network the sink and use them to approximately answer user queries our approach dramatically reduces communication relative to previous approaches for querying sensor networks by exploiting properties of these local models since each sensor communicates with the sink only when its local model varies due to changes in the underlying data distribution in our experimental results performed on trace of real data we observed on average about message transmissions from each sensor over week including the learning phase to correctly predict temperatures to within csaf also provides mechanism to detect data similarities between nodes and organize nodes into clusters at the sink at no additional communication cost this is again achieved by exploiting properties of our local time series models and by means of novel definition of data similarity between nodes that is based not on raw data but on the prediction values our clustering algorithm is both very efficient and provably optimal in the number of clusters our clusters have several interesting features first they can capture similarity between far away nodes that are not geographically adjacent second cluster membership to variations in sensors local models third nodes within cluster are not required to track the membership of other nodes in the cluster we present number of simulation based experimental results that demonstrate these properties of saf
good spatial locality alleviates both the latency and bandwidth problem of memory by boosting the effect of prefetching and improving the utilization of cache however conventional definitions of spatial locality are inadequate for programmer to precisely quantify the quality of program to identify causes of poor locality and to estimate the potential by which spatial locality can be improved this paper describes new component based model for spatial locality it is based on measuring the change of reuse distances as function of the data block size it divides spatial locality into components at program and behavior levels while the base model is costly because it requires the tracking of the locality of every memory access the overhead can be reduced by using small inputs and by extending sampling based tool the paper presents the result of the analysis for large set of benchmarks the cost of the analysis and the experience of user study in which the analysis helped to locate data layout problem and improve performance by with line change in an application with over lines
this paper presents novel technique anatomy for publishing sensitive data anatomy releases all the quasi identifier and sensitive values directly in two separate tables combined with grouping mechanism this approach protects privacy and captures large amount of correlation in the microdata we develop linear time algorithm for computing anatomized tables that obey the diversity privacy requirement and minimize the error of reconstructing the microdata extensive experiments confirm that our technique allows significantly more effective data analysis than the conventional publication method based on generalization specifically anatomy permits aggregate reasoning with average error below which is lower than the error obtained from generalized table by orders of magnitude
in response to long lasting anticipation by the java community version of the java platform referred to as java introduced generic types and methods to the java language the java generics are significant enhancement to the language expressivity because they allow straightforward composition of new generic classes from existing ones while reducing the need for plethora of type casts while the java generics are expressive the chosen implementation method type erasure has triggered undesirable orthogonality violations this paper identifies six cases of orthogonality violations in the java generics and demonstrates how these violations are mandated by the use of type erasure the paper also compares the java cases of orthogonality violations to compatible cases in and nextgen and analyzes the tradeoffs in the three approaches the conclusion is that java users face new challenges number of generic type expressions are forbidden while others that are allowed are left unchecked by the compiler
compared to traditional text classification with flat category set or small hierarchy of categories classifying web pages to large scale hierarchy such as open directory project odp and yahoo directory is challenging while recently proposed deep classification method makes the problem tractable it still suffers from low classification performance major problem is the lack of training data which is unavoidable with such huge hierarchy training pages associated with the category nodes are short and their distributions are skewed to alleviate the problem we propose new training data selection strategy and na iuml ve bayes combination model which utilize both local and global information we conducted series of experiments with the odp hierarchy containing more than categories to show that the proposed method of using both local and global information indeed helps avoiding the training data sparseness problem outperforming the state of art method
without proper simplification techniques database integrity checking can be prohibitively time consuming several methods have been developed for producing simplified incremental checks for each update but none until now of sufficient quality and generality for providing true practical impact and the present paper is an attempt to fill this gap on the theoretical side general characterization is introduced of the problem of simplification of integrity constraints and natural definition is given of what it means for simplification procedure to be ideal we prove that ideality of simplification is strictly related to query containment in fact an ideal simplification pro cedure can only exist in database languages for which query containment is decidable however simplifications that do not qualify as ideal may also be relevant for practical purposes we present concrete approach based on transformation operators that apply to integrity constraints written in rich datalog like language with negation the resulting procedure produces at design time simplified constraints for parametric transaction patterns which can then be instantiated and checked for consistency at run time these tests take place before the execution of the update so that only consistency preserving updates are eventually given to the database the extension to more expressive languages and the application of the framework to other contexts such as data integration and concurrent database systems are also discussed our experiments show that the simplifications obtained with our method may give rise to much better performance than with previous methods and that further improvements are achieved by checking consistency before executing the update
what distinguishes commerce from ordinary commerce what distinguishes it from distributed computation in this paper we propose performative theory of commerce drawing on speech act theory in which commerce exchanges are promises of future commercial actions whose real world meanings are constructed jointly and incrementally we then define computational model for this theory called posit spaces along with the syntax and semantics for an agent interaction protocol the posit spaces protocol or psp this protocol enables participants in multi agent commercial interaction to propose accept modify and revoke joint commitments our work integrates three strands of prior research the theory of tuple spaces in distributed computation formal dialogue games from argumentation theory and the study of commitments in multi agent systems
traffic anomalies such as failures and attacks are increasing in frequency and severity and thus identifying them rapidly and accurately is critical for large network operators the detection typically treats the traffic as collection of flows and looks for heavy changes in traffic patterns eg volume number of connections however as link speeds and the number of flows increase keeping per flow state is not scalable the recently proposed sketch based schemes are among the very few that can detect heavy changes and anomalies over massive data streams at network traffic speeds however sketches do not preserve the key eg source ip address of the flows hence even if anomalies are detected it is difficult to infer the culprit flows making it big practical hurdle for online deployment meanwhile the number of keys is too large to record to address this challenge we propose efficient reversible hashing algorithms to infer the keys of culprit flows from sketches without storing any explicit key information no extra memory or memory accesses are needed for recording the streaming data meanwhile the heavy change detection daemon runs in the background with space complexity and computational time sublinear to the key space size this short paper describes the conceptual framework of the reversible sketches as well as some initial approaches for implementation see for the optimized algorithms in details comment we further apply various emph ip mangling algorithms and emph bucket classification methods to reduce the false positives and false negatives evaluated with netflow traffic traces of large edge router we demonstrate that the reverse hashing can quickly infer the keys of culprit flows even for many changes with high accuracy
software bugs in routers lead to network outages security vulnerabilities and other unexpected behavior rather than simply crashing the router bugs can violate protocol semantics rendering traditional failure detection and recovery techniques ineffective handling router bugs is an increasingly important problem as new applications demand higher availability and networks become better at dealing with traditional failures in this paper we tailor software and data diversity sdd to the unique properties of routing protocols so as to avoid buggy behavior at run time our bug tolerant router executes multiple diverse instances of routing software and uses voting to determine the output to publish to the forwarding table or to advertise to neighbors we design and implement router hypervisor that makes this parallelism transparent to other routers handles fault detection and booting of new router instances and performs voting in the presence of routing protocol dynamics without needing to modify software of the diverse instances experiments with bgp message traces and open source software running on our linux based router hypervisor demonstrate that our solution scales to large networks and efficiently masks buggy behavior
commercially available database systems do not meet the information and processing needs of design and manufacturing environments new generation of systems engineering information systems must be built to meet these needs the architectural and computational aspects of such systems are addressed and solutions are proposed the authors argue that mainframe workstation architecture is needed to provide distributed functionality while ensuring high availability and low communication overhead that explicit control of metaknowledge is needed to support extendibility and evolution that large rule bases are needed to make the knowledge of the systems active and that incremental computation models are needed to achieve the required performance of such engineering information systems
large number of call graph construction algorithms for object oriented and functional languages have been proposed each embodying different tradeoffs between analysis cost and call graph precision in this article we present unifying framework for understanding call graph construction algorithms and an empirical comparison of representative set of algorithms we first present general parameterized algorithm that encompasses many well known and novel call graph construction algorithms we have implemented this general algorithm in the vortex compiler infrastructure mature multilanguage optimizing compiler the vortex implementation provides level playing field for meaningful cross algorithm performance comparisons the costs and benefits of number of call graph construction algorithms are empirically assessed by applying their vortex implementation to suite of sizeable to lines of code cecil and java programs for many of these applications interprocedural analysis enabled substantial speed ups over an already highly optimized baseline furthermore significant fraction of these speed ups can be obtained through the use of scalable near linear time call graph construction algorithm
in this paper we present the java aspect components jac framework for building aspect oriented distributed applications in java this paper describes the aspect oriented programming model and the architectural details of the framework implementation the framework enables extension of application semantics for handling well separated concerns this is achieved with software entity called an aspect component ac acs provide distributed pointcuts dynamic wrappers and metamodel annotations distributed pointcuts are key feature of our framework they enable the definition of crosscutting structures that do not need to be located on single host acs are dynamic they can be added removed and controlled at runtime this enables our framework to be used in highly dynamic environments where adaptable software is needed
scheduling algorithms used in compilers traditionally focus on goals such as reducing schedule length and register pressure or producing compact code in the context of hardware synthesis system where the schedule is used to determine various components of the hardware including datapath storage and interconnect the goals of scheduler change drastically in addition to achieving the traditional goals the scheduler must proactively make decisions to ensure efficient hardware is produced this paper proposes two exact solutions for cost sensitive modulo scheduling one based on an integer linear programming formulation and another based on branch and bound search to achieve reasonable compilation times decomposition techniques to break down the complex scheduling problem into phase ordered sub problems are proposed the decomposition techniques work either by partitioning the dataflow graph into smaller subgraphs and optimally scheduling the subgraphs or by splitting the scheduling problem into two phases time slot and resource assignment the effectiveness of cost sensitive modulo scheduling in minimizing the costs of function units register structures and interconnection wires are evaluated within fully automatic synthesis system for loop accelerators the cost sensitive modulo scheduler increases the efficiency of the resulting hardware significantly compared to both traditional cost unaware and greedy cost aware modulo schedulers
this study develops theory of personal values and trust in mobile commerce commerce service systems research hypotheses involve the satisfaction formation processes and usage continuance intentions in commerce service context and the participator is selected from three private wireless telecommunication service providers in taiwan results showed consumers might not use the commerce service in the future without being totally satisfied with the system and their customer values mobile technology trusting expectations were very important in the continued commerce service usage behaviour and the providers might not fulfil the commerce service need for consumers but satisfied with the commerce service delivered
files classes or methods have frequently been investigated in recent research on co change in this paper we present first study at the level of lines to identify line changes across several versions we define the annotation graph which captures how lines evolve over time the annotation graph provides more fine grained software evolution information such as life cycles of each line and related changes whenever developer changed line of versiontxt she also changed line of libraryjava
we consider lock free synchronization for dynamic embedded real time systems that are subject to resource overloads and arbitrary activity arrivals we model activity arrival behaviors using the unimodal arbitrary arrival model or uam uam embodies stronger ldquo adversary rdquo than most traditional arrival models we derive an upper bound on lock free retries under the uam with utility accrual scheduling mdash the first such result we establish the tradeoffs between lock free and lock based sharing under uam these include conditions under which activities accrued timeliness utility is greater under lock free than lock based and the consequent lower and upper bound on the total accrued utility that is possible with lock free and lock based sharing we confirm our analytical results with posix rtos implementation
in this paper study is described which investigates differences in game experience between the use of iconic and symbolic tangibles in digital tabletop interaction to enable this study new game together with two sets of play pieces iconic and symbolic was developed and used in an experiment with participants in this experiment the understanding of the game the understanding of the play pieces and the fun experience were tested both the group who played with iconic play pieces and the group who played with symbolic play pieces were proven to have comparable fun experience and understanding of the game however the understanding of the play pieces was higher in the iconic group and large majority of both groups preferred to play with iconic play pieces rather then symbolic play pieces
packet based on chip networks are increasingly being adopted in complex system on chip soc designs supporting numerous homogeneous and heterogeneous functional blocks these network on chip noc architectures are required to not only provide ultra low latency but also occupy small footprint and consume as little energy as possible further reliability is rapidly becoming major challenge in deep sub micron technologies due to the increased prominence of permanent faults resulting from accelerated aging effects and manufacturing testing challenges towards the goal of designing low latency energyefficient and reliable on chip communication networks we propose novel fine grained modular router architecture the proposed architecture employs decoupled parallel arbiters and uses smaller crossbars for row and column connections to reduce output port contention probabilities as compared to existing designs furthermore the router employs new switch allocation technique known as mirroring effect to reduce arbitration depth and increase concurrency in addition the modular design permits graceful degradation of the network in the event of permanent faults and also helps to reduce the dynamic power consumption our simulation results indicate that in an mesh network the proposed architecture reduces packet latency by and power consumption by as compared to two existing router architectures evaluation using combined performance energy and fault tolerance metric indicates that the proposed architecture provides overall improvement compared to the two earlier routers
the world wide web has undergone major changes in recent years the idea to see the web as platform for services instead of one way source of information has come along with number of new applications such as photo and video sharing portals and wikisin this paper we study how these changes affect the nature of the data distributed over the world wide web to do so we compare two data traces collected at the web proxy server of the rwth aachen the first trace was recorded in the other more than seven years later in we show the major differences and the similarities between the two traces and compare our observations with other work the results indicate that traditional proxy caching is no longer effective in typical university networks
transactions have been around since the seventies to provide reliable information processing in automated information systems originally developed for simple debit credit style database operations in centralized systems they have moved into much more complex application domains including aspects like distribution process orientation and loose coupling the amount of published research work on transactions is huge and number of overview papers and books already exist concise historic analysis providing an overview of the various phases of development of transaction models and mechanisms in the context of growing complexity of application domains is still missing however to fill this gap this paper presents historic overview of transaction models organized in several transaction management eras thereby investigating numerous transaction models ranging from the classical flat transactions via advanced and workflow transactions to the web services and grid transaction models the key concepts and techniques with respect to transaction management are investigated placing well known research efforts in historical perspective reveals specific trends and developments in the area of transaction management as such this paper provides comprehensive structured overview of developments in the area
we describe compression model for semistructured documents called structural contexts model scm which takes advantage of the context information usually implicit in the structure of the text the idea is to use separate model to compress the text that lies inside each different structure type eg different xml tag the intuition behind scm is that the distribution of all the texts that belong to given structure type should be similar and different from that of other structure types we mainly focus on semistatic models and test our idea using word based huffman method this is the standard for compressing large natural language text databases because random access partial decompression and direct search of the compressed collection is possible this variant dubbed scmhuff retains those features and improves huffman's compression ratios we consider the possibility that storing separate models may not pay off if the distribution of different structure types is not different enough and present heuristic to merge models with the aim of minimizing the total size of the compressed database this gives an additional improvement over the plain technique the comparison against existing prototypes shows that among the methods that permit random access to the collection scmhuff achieves the best compression ratios better than the closest alternative from purely compression aimed perspective we combine scm with ppm modeling separate ppm model is used to compress the text that lies inside each different structure type the result scmppm does not permit random access nor direct search in the compressed text but it gives better compression ratios than other techniques for texts longer than mb
this paper presents general framework for determining average program execution times and their variance based on the program's interval structure and control dependence graph average execution times and variance values are computed using frequency information from an optimized counter based execution profile of the program
using analysis simulation and experimentation we examine the threat against anonymous communications posed by passive logging attacks in previous work we analyzed the success of such attacks under various assumptions here we evaluate the effects of these assumptions more closely first we analyze the onion routing based model used in prior work in which fixed set of nodes remains in the system indefinitely we show that for this model by removing the assumption of uniformly random selection of nodes for placement in the path initiators can greatly improve their anonymity second we show by simulation that attack times are significantly lower in practice than bounds given by analytical results from prior work third we analyze the effects of dynamic membership model in which nodes are allowed to join and leave the system we show that all known defenses fail more quickly when the assumption of static node set is relaxed fourth intersection attacks against peer to peer systems are shown to be an additional danger either on their own or in conjunction with the predecessor attack finally we address the question of whether the regular communication patterns required by the attacks exist in real traffic we collected and analyzed the web requests of users to determine the extent to which basic patterns can be found we show that for our study frequent and repeated communication to the same web site is common
methods of top search with no random access can be used to find best objects using sorted access to the sources of attribute values in this paper we present new heuristics over the nra algorithm that can be used for fast search of top objects using wide range of user preferences nra algorithm usually needs periodical scan of large number of candidates during the computation in this paper we propose methods of no random access top search that optimize the candidate list maintenance during the computation to speed up the search the proposed methods are compared to table scan method typically used in databases we present results of experiments showing speed improvement depending on number of object attributes expressed in user preferences or selectivity of user preferences
the mapreduce framework is increasingly being used to analyze large volumes of data one important type of data analysis done with mapreduce is log processing in which click stream or an event log is filtered aggregated or mined for patterns as part of this analysis the log often needs to be joined with reference data such as information about users although there have been many studies examining join algorithms in parallel and distributed dbmss the mapreduce framework is cumbersome for joins mapreduce programmers often use simple but inefficient algorithms to perform joins in this paper we describe crucial implementation details of number of well known join strategies in mapreduce and present comprehensive experimental comparison of these join techniques on node hadoop cluster our results provide insights that are unique to the mapreduce platform and offer guidance on when to use particular join algorithm on this platform
we propose dialogue game protocol for purchase negotiation dialogues which identifies appropriate speech acts defines constraints on their utterances and specifies the different sub tasks agents need to perform in order to engage in dialogues according to this protocol our formalism combines dialogue game similar to those in the philosophy of argumentation with model of rational consumer purchase decision behaviour adopted from marketing theory in addition to the dialogue game protocol we present portfolio of decision mechanisms for the participating agents engaged in the dialogue and use these to provide our formalism with an operational semantics we show that these decision mechanisms are sufficient to generate automated purchase decision dialogues between autonomous software agents interacting according to our proposed dialogue game protocol
the increasing size and complexity of many software systems demand greater emphasis on capturing and maintaining knowledge at many different levels within the software development process this knowledge includes descriptions of the hardware and software components and their behavior external and internal design specifications and support for system testing the knowledge based software engineering kbse research paradigm is concerned with systems that use formally represented knowledge with associated inference precedures to support the various subactivities of software development as they growing scale kbse systems must balance expressivity and inferential power with the real demands of knowledge base construction maintenance performance and comprehensibility description logics dls possess several features mdash terminological orientation formal semantics and efficient reasoning procedures mdash which offer an effective tradeoff of these factors we discuss three kbse systems in which dls capture some of the requisite knowledge needed to support design coding and testing activities we then survey some alternative approaches to dls in kbse systems we close with discussion of the benefits of dls and ways to address some of their limitations
this paper shows an asymptotically tight analysis of the certified write all algorithm called awt that was introduced by anderson and woll siam comput and method for creating near optimal instances of the algorithm this algorithm is the best known deterministic algorithm that can be used to simulate synchronous parallel processors on asynchronous processors the algorithm is instantiated with permutations on where can be chosen from wide range of values when implementing simulation on specific parallel system with processors one would like to select the best possible value of and the best possible permutations in order to maximize the efficiency of the simulationthis paper shows that work complexity of any instance of awt is logq where is the number of permutations selected and is value related to their combinatorial properties the choice of turns out to be critical for obtaining an instance of the awt algorithm with near optimal work for any and any large enough work of any instance of the algorithm must be at least ln ln ln under certain conditions however that is about ln ln ln and for infinitely many large enough this lower bound can be nearly attained by instances of the algorithm that use certain permutations and have work at most ln ln ln the paper also shows penalty for not selecting well when is significantly away from ln ln ln then work of any instance of the algorithm with this displaced must be considerably higher than otherwise
two or more components eg objects modules or programs interoperate when they exchange data such as xml data using application programming interface api calls exported by xml parsers remains primary mode of accessing and manipulating xml and these api calls lead to various run time errors in components that exchange xml data currently no tool checks the source code of interoperating components for potential flaws caused by third party api calls that lead to incorrect xml data exchanges and runtime errors even when components are located within the same application our solution combines program abstraction and symbolic execution in order to reengineer the approximate schema of xml data that would be output by component this schema is compared using bisimulation with the schema of xml data that is expected by some other components we describe our approach and give our error checking algorithm we implemented our approach in tool that we used on open source and commercial systems and discovered errors that were not detected during their design and testing
outsourcing the training of support vector machines svm to external service providers benefits the data owner who is not familiar with the techniques of the svm or has limited computing resources in outsourcing the data privacy is critical issue for some legal or commercial reasons since there may be sensitive information contained in the data existing privacy preserving svm works are either not applicable to outsourcing or weak in security in this paper we propose scheme for privacy preserving outsourcing the training of the svm without disclosing the actual content of the data to the service provider in the proposed scheme the data sent to the service provider is perturbed by random transformation and the service provider trains the svm for the data owner from the perturbed data the proposed scheme is stronger in security than existing techniques and incurs very little redundant communication and computation cost
representation exposure is well known problem in the object oriented realm object encapsulation mechanisms have established tradition for solving this problem based on principle of reference containment this paper proposes novel type system which is based on different principle we call effect encapsulation which confines side effects rather than object references according to an ownership structure compared to object encapsulation effect encapsulation liberates us from the restriction on object referenceability and offers more flexibility in this paper we show that effect encapsulation can be statically type checked
set based program analysis establishes constraints between sets of abstract values for all expressions in program solving the system of constraints produces conservative approximation to the program's runtime flow of valuessome practical set based analyses use explicit selectors to extract the relevant values from an approximation set for example if the analysis needs to determine the possible return values of procedure it uses the appropriate selector to extract the relevant component from the abstract representation of the procedurein this paper we show that this selector based approach complicates the constraint solving phase of the analysis too much and thus fails to scale up to realistic programming languages we demonstrate this claim with full fledged value flow analysis for case lambda multi branched version of lambda we show how both the theoretical underpinnings and the practical implementation become too complex in response we present variant of set based closure analysis that computes equivalent results in much more efficient manner
we argue that runtime program transformation partial evaluation and dynamic compilation are essential tools for automated generation of flexible highly interactive graphical interfaces in particular these techniques help bridge the gap between high level functional description and an efficient implementation to support our claim we describe our application of these techniques to functional implementation of vision real time visualization system that represents multivariate relations as nested interactors and to auto visual rule based system that designs vision visualizations from high level task specifications vision visualizations are specified using simple functional language these programs are transformed into cached dataflow graph partial evaluator is used on particular computation intensive function applications and the results are compiled to native code the functional representation simplifies generation of correct code and the program transformations ensure good performance we demonstrate why these transformations improve performance and why they cannot be done at compile time
building rules on top of ontologies is the ultimate goal of the logical layer of the semantic web to this aim an ad hoc markup language for this layer is currently under discussion it is intended to follow the tradition of hybrid knowledge representation and reasoning systems such as inline graphic mime subtype gif xlink sinline alt text mathcal al alt text inline graphic log that integrates the description logic inline graphic mime subtype gif xlink sinline alt text mathcal alc alt text inline graphic and the function free horn clausal language datalog in this paper we consider the problem of automating the acquisition of these rules for the semantic web we propose general framework for rule induction that adopts the methodological apparatus of inductive logic programming and relies on the expressive and deductive power of inline graphic mime subtype gif xlink sinline alt text mathcal al alt text inline graphic log the framework is valid whatever the scope of induction description versus prediction is yet for illustrative purposes we also discuss an instantiation of the framework which aims at description and turns out to be useful in ontology refinement
we present set of techniques for reducing the memory consumption of object oriented programs these techniques include analysis algorithms and optimizations that use the results of these analyses to eliminate fields with constant values reduce the sizes of fields based on the range of values that can appear in each field and eliminate fields with common default values or usage patterns we apply these optimizations both to fields declared by the programmer and to implicit fields in the runtime object header although it is possible to apply these techniques to any object oriented program we expect they will be particularly appropriate for memory limited embedded systemswe have implemented these techniques in the mit flex compiler system and applied them to the programs in the specjvm benchmark suite our experimental results show that our combined techniques can reduce the maximum live heap size required for the programs in our benchmark suite by as much as some of the optimizations reduce the overall execution time others may impose modest performance penalties
next generation decision support applications besides being capable of processing huge amounts of data require the ability to integrate and reason over data from multiple heterogeneous data sources often these data sources differ in variety of aspects such as their data models the query languages they support and their network protocols also typically they are spread over wide geographical area the cost of processing decision support queries in such setting is quite high however processing these queries often involves redundancies such as repeated access of same data source and multiple execution of similar processing sequences minimizing these redundancies would significantly reduce the query processing cost in this paper we propose an architecture for processing complex decision support queries involving multiple heterogeneous data sources introduce the notion of transient views mdash materialized views that exist only in the context of execution of query mdash that is useful for minimizing the redundancies involved in the execution of these queries develop cost based algorithm that takes query plan as input and generates an optimal ldquo covering plan rdquo by minimizing redundancies in the original plan validate our approach by means of an implementation of the algorithms and detailed performance study based on tpc benchmark queries on commercial database system and finally compare and contrast our approach with work in related areas in particular the areas of answering queries using views and optimization using common sub expressions our experiments demonstrate the practicality and usefulness of transient views in significantly improving the performance of decision support queries
editor's note this article describes web based energy estimation tool for embedded systems an interesting feature of this tool is that it performs real time cycle accurate energy measurements on hardware prototype of the processor the authors describe the various steps involved in using the tool and present case studies to illustrate its utility anand raghunathan nec laboratories
warping the pointer across monitor bezels has previously been demonstrated to be both significantly faster and preferred to the standard mouse behavior when interacting across displays in homogeneous multi monitor configurations complementing this work we present user study that compares the performance of four pointer warping strategies including previously untested frame memory placement strategy in heterogeneous multi monitor environments where displays vary in size resolution and orientation our results show that new frame memory pointer warping strategy significantly improved targeting performance up to in some cases in addition our study showed that when transitioning across screens the mismatch between the visual and the device space has significantly bigger impact on performance than the mismatch in orientation and visual size alone for mouse operation in highly heterogeneous multi monitor environment all our participants strongly preferred using pointer warping over the regular mouse behavior
cloud computing is disruptive trend that is changing the way we use computers the key underlying technology in cloud infrastructures is virtualization so much so that many consider virtualization to be one of the key features rather than simply an implementation detail unfortunately the use of virtualization is the source of significant security concern because multiple virtual machines run on the same server and since the virtualization layer plays considerable role in the operation of virtual machine malicious party has the opportunity to attack the virtualization layer successful attack would give the malicious party control over the all powerful virtualization layer potentially compromising the confidentiality and integrity of the software and data of any virtual machine in this paper we propose removing the virtualization layer while retaining the key features enabled by virtualization our nohype architecture named to indicate the removal of the hypervisor addresses each of the key roles of the virtualization layer arbitrating access to cpu memory and devices acting as network device eg ethernet switch and managing the starting and stopping of guest virtual machines additionally we show that our nohype architecture may indeed be no hype since nearly all of the needed features to realize the nohype architecture are currently available as hardware extensions to processors and devices
effort prediction is very important issue for software project management historical project data sets are frequently used to support such prediction but missing data are often contained in these data sets and this makes prediction more difficult one common practice is to ignore the cases with missing data but this makes the originally small software project database even smaller and can further decrease the accuracy of prediction the alternative is missing data imputation there are many imputation methods software data sets are frequently characterised by their small size but unfortunately sophisticated imputation methods prefer larger data sets for this reason we explore using simple methods to impute missing data in small project effort data sets we propose class mean imputation cmi method based on the nn hot deck imputation method mini to impute both continuous and nominal missing data in small data sets we use an incremental approach to increase the variance of population to evaluate mini and nn and cmi methods as benchmarks we use data sets with cases and cases sampled from larger industrial data set with and missing data percentages respectively we also simulate missing completely at random mcar and missing at random mar missingness mechanisms the results suggest that the mini method outperforms both cmi and the nn methods we conclude that this new imputation technique can be used to impute missing values in small data sets
the number of mobile phone users has been steadily increasing due to the development of microtechnology and human needs for ubiquitous communication menu design features play significant role in cell phone design from the perspective of customer satisfaction moreover small screens of the type used on mobile phones are limited in the amount of available space therefore it is important to obtain good menu design review of previous menu design studies for human computer interaction suggests that design guidelines for mobile phones need to be reappraised especially display features we propose conceptual model for cell phone menu design with displays the three main factors included in the model are the number of items task complexity and task type
an ad hoc data source is any semistructured data source for which useful data analysis and transformation tools are not readily available such data must be queried transformed and displayed by systems administrators computational biologists financial analysts and hosts of others on regular basis in this paper we demonstrate that it is possible to generate suite of useful data processing tools including semi structured query engine several format converters statistical analyzer and data visualization routines directly from the ad hoc data itself without any human intervention the key technical contribution of the work is multi phase algorithm that automatically infers the structure of an ad hoc data source and produces format specification in the pads data description language programmers wishing to implement custom data analysis tools can use such descriptions to generate printing and parsing libraries for the data alternatively our software infrastructure will push these descriptions through the pads compiler creating format dependent modules that when linked with format independent algorithms for analysis and transformation result infully functional tools we evaluate the performance of our inference algorithm showing it scales linearlyin the size of the training data completing in seconds as opposed to the hours or days it takes to write description by hand we also evaluate the correctness of the algorithm demonstrating that generating accurate descriptions often requires less than of theavailable data
to manage the evolution of software systems effectively software developers must understand software systems identify and evaluate alternative modification strategies implement appropriate modifications and validate the correctness of the modifications one analysis technique that assists in many of these activities is program slicing to facilitate the application of slicing to large software systems we adapted control flow based interprocedural slicing algorithm so that it accounts for interprocedural control dependencies not recognized by other slicing algorithms and reuses slicing information for improved efficiency our initial studies suggest that additional slice accuracy and slicing efficiency may be achieved with our algorithm
this paper addresses necessary modification and extensions to existing grid computing approaches in order to meet modern business demand grid computing has been traditionally used to solve large scientific problems focussing more on accumulative use of computing power and processing large input and output files typical for many scientific problems nowadays businesses have increasing computational demands such that grid technologies are of interest however the existing business requirements introduce new constraints on the design configuration and operation of the underlying systems including availability of resources performance monitoring aspects security and isolation issues this paper addresses the existing grid computing capabilities discussing the additional demands in detail this results in suggestion of problem areas that must be investigated and corresponding technologies that should be used within future business grid systems
existing work on scheduling with energy concern has focused on minimizing the energy for completing all jobs or achieving maximum throughput that is energy usage is secondary concern when compared to throughput and the schedules targeted may be very poor in energy efficiency in this paper we attempt to put energy efficiency as the primary concern and study how to maximize throughput subject to user defined threshold of energy efficiency we first show that all deterministic online algorithms have competitive ratio at least where is the max min ratio of job size nevertheless allowing the online algorithm to have slightly poorer energy efficiency leads to constant ie independent of competitive online algorithm on the other hand using randomization we can reduce the competitive ratio to logŒ¥ without relaxing the efficiency threshold finally we consider special case where no jobs are demanding and give deterministic online algorithm with constant competitive ratio for this case
mapreduce and similar systems significantly ease the task of writing data parallel code however many real world computations require pipeline of mapreduces and programming and managing such pipelines can be difficult we present flumejava java library that makes it easy to develop test and run efficient data parallel pipelines at the core of the flumejava library are couple of classes that represent immutable parallel collections each supporting modest number of operations for processing them in parallel parallel collections and their operations present simple high level uniform abstraction over different data representations and execution strategies to enable parallel operations to run efficiently flumejava defers their evaluation instead internally constructing an execution plan dataflow graph when the final results of the parallel operations are eventually needed flumejava first optimizes the execution plan and then executes the optimized operations on appropriate underlying primitives eg mapreduces the combination of high level abstractions for parallel data and computation deferred evaluation and optimization and efficient parallel primitives yields an easy to use system that approaches the efficiency of hand optimized pipelines flumejava is in active use by hundreds of pipeline developers within google
in clustering based wireless sensor network wsn cluster heads play an important role by serving as data forwarders amongst other network organisation functions thus malfunctioning and or compromised sensor nodes that serve as cluster head ch can lead to unreliable data delivery in this paper we propose scheme called secure low energy clustering seclec that incorporates secure cluster head selection and distributed cluster head monitoring to achieve reliable data delivery the seclec framework is flexible and can accommodate various tracking and monitoring mechanisms our goal in this paper is to understand the performance impact of adding such security mechanisms to the clustering architecture in terms of energy consumed and prevented data losses our experiments show that with seclec bs detects such anomalous nodes with the latency of one network operation round and the data loss due to malicious nodes is up to with reasonable communication overhead
we present new technique failure oblivious computing that enables servers to execute through memory errors without memory corruption our safe compiler for inserts checks that dynamically detect invalid memory accesses instead of terminating or throwing an exception the generated code simply discards invalid writes and manufactures values to return for invalid reads enabling the server to continue its normal execution path we have applied failure oblivious computing to set of widely used servers from the linux based open source computing environment our results show that our techniques make these servers invulnerable to known security attacks that exploit memory errors and enable the servers to continue to operate successfully to service legitimate requests and satisfy the needs of their users even after attacks trigger their memory errors we observed several reasons for this successful continued execution when the memory errors occur in irrelevant computations failure oblivious computing enables the server to execute through the memory errors to continue on to execute the relevant computation even when the memory errors occur in relevant computations failure oblivious computing converts requests that trigger unanticipated and dangerous execution paths into anticipated invalid inputs which the error handling logic in the server rejects because servers tend to have small error propagation distances localized errors in the computation for one request tend to have little or no effect on the computations for subsequent requests redirecting reads that would otherwise cause addressing errors and discarding writes that would otherwise corrupt critical data structures such as the call stack localizes the effect of the memory errors prevents addressing exceptions from terminating the computation and enables the server to continue on to successfully process subsequent requests the overall result is substantial extension of the range of requests that the server can successfully process
multidatabase system mdbs integrates information from autonomous local databases managed by heterogeneous database management systems dbms in distributed environment for query involving more than one database global query optimization should be performed to achieve good overall system performance the significant differences between an mdbs and traditional distributed database system ddbs make query optimization in the former more challenging than in the latter challenges for query optimization in an mdbs are discussed in this paper two phase optimization approach for processing query in an mdbs is proposed several global query optimization techniques suitable for an mdbs such as semantic query optimization query optimization via probing queries parametric query optimization and adaptive query optimization are suggested the architecture of global query optimizer incorporating these techniques is designed
we present the design implementation and evaluation of beepbeep high accuracy acoustic based ranging system it operates in spontaneous ad hoc and device to device context without leveraging any pre planned infrastructure it is pure software based solution and uses only the most basic set of commodity hardware speaker microphone and some form of device to device communication so that it is readily applicable to many low cost sensor platforms and to most commercial off the shelf mobile devices like cell phones and pdas it achieves high accuracy through combination of three techniques two way sensing self recording and sample counting the basic idea is the following to estimate the range between two devices each will emit specially designed sound signal beep and collect simultaneous recording from its microphone each recording should contain two such beeps one from its own speaker and the other from its peer by counting the number of samples between these two beeps and exchanging the time duration information with its peer each device can derive the two way time of flight of the beeps at the granularity of sound sampling rate this technique cleverly avoids many sources of inaccuracy found in other typical time of arrival schemes such as clock synchronization non real time handling software delays etc our experiments on two common cell phone models have shown that we can achieve around one or two centimeters accuracy within range of more than ten meters despite series of technical challenges in implementing the idea
the ability of reconfiguring software architectures in order to adapt them to new requirements or changing environment has been of growing interest we propose uniform algebraic approach that improves on previous formal work in the area due to the following characteristics first components are written in high level program design language with the usual notion of state second the approach deals with typical problems such as guaranteeing that new components are introduced in the correct state possibly transferred from the old components they replace and that the resulting architecture conforms to certain structural constraints third reconfigurations and computations are explicitly related by keeping them separate this is because the approach provides semantics to given architecture through the algebraic construction of an equivalent program whose computations can be mirrored at the architectural level
developers and designers always strive for quality software quality software tends to be robust reliable and easy to maintain and thus reduces the cost of software development and maintenance several methods have been applied to improve software quality refactoring is one of those methods the goal of this paper is to validate invalidate the claims that refactoring improves software quality we focused this study on different external quality attributes which are adaptability maintainability understandability reusability and testability we found that refactoring does not necessarily improve these quality attributes
we investigate new approach to editing spatially and temporally varying measured materials that adopts stroke based workflow in our system user specifies small number of editing constraints with painting interface which are smoothly propagated to the entire dataset through an optimization that enforces similar edits are applied to areas with similar appearance the sparse nature of this appearance driven optimization permits the use of efficient solvers allowing the designer to interactively refine the constraints we have found this approach supports specifying wide range of complex edits that would not be easy with existing techniques which present the user with fixed segmentation of the data furthermore it is independent of the underlying reflectance model and we show edits to both analytic and non parametric representations in examples from several material databases
the circular sensing model has been widely used to estimate performance of sensing applications in existing analysis and simulations while this model provides valuable high level guidelines the quantitative results obtained may not reflect the true performance of these applications due to the existence of obstacles and sensing irregularity introduced by insufficient hardware calibration in this project we design and implement two sensing area modeling sam techniques useful in the real world they complement each other in the design space sam provides accurate sensing area models for individual nodes using controlled or monitored events while sam provides continuous sensing similarity models using natural events in an environment with these two models we pioneer an investigation of the impact of sensing irregularity on application performance such as coverage scheduling we evaluate sam extensively in real world settings using three testbeds consisting of micaz motes and xsm motes to study the performance at scale we also provide an extensive node simulation evaluation results reveal several serious issues concerning circular models and demonstrate significant improvements
the successful design and implementation of secure systems must occur from the beginning component that must process data at multiple security levels is very critical and must go through additional evaluation to ensure the processing is secure it is common practice to isolate and separate the processing of data at different levels into different components in this paper we present architecture based refinement techniques for the design of multilevel secure systems we discuss what security requirements must be satisfied through the refinement process including when separation works and when it does not the process oriented approach will lead to verified engineering techniques for secure systems which should greatly reduce the cost of certification of those systems
this paper presents an analysis of the performance effects of burstiness in multi tiered systems we introduce compact characterization of burstiness based on autocorrelation that can be used in capacity planning performance prediction and admission control we show that if autocorrelation exists either in the arrival or the service process of any of the tiers in multi tiered system then autocorrelation propagates to all tiers of the system we also observe the surprising result that in spite of the fact that the bottleneck resource in the system is far from saturation and that the measured throughput and utilizations of other resources are also modest user response times are very high when autocorrelation is not considered this underutilization of resources falsely indicates that the system can sustain higher capacities we examine the behavior of small queuing system that helps us understand this counter intuitive behavior and quantify the performance degradation that originates from autocorrelated flows we present case study in an experimental multi tiered internet server and devise model to capture the observed behavior our evaluation indicates that the model is in excellent agreement with experimental results and captures the propagation of autocorrelation in the multi tiered system and resulting performance trends finally we analyze an admission control algorithm that takes autocorrelation into account and improves performance by reducing the long tail of the response time distribution
in this article we propose techniques that enable efficient exploration of the design space where each logical block can span more than one silicon layer fine grain integration provides reduced intrablock wire delay as well as improved power consumption however the corresponding power and performance advantage is usually underutilized since various implementations of multilayer blocks require novel physical design and microarchitecture infrastructure to explore microarchitecture design space we develop cubic packing engine which can simultaneously optimize physical and architectural design for efficient vertical integration this technique selects the individual unit designs from set of single layer or multilayer implementations to get the best microarchitectural design in terms of performance temperature or both our experimental results using design driver of high performance superscalar processor show percnt performance improvement over traditional for layers and percnt over with single layer unit implementations since thermal characteristics of integrated circuits are among the main challenges thermal aware floorplanning and thermal via insertion techniques are employed to keep the peak temperatures below threshold
large scheduling windows are an effective mechanism for increasing microprocessor performance through the extraction of instruction level parallelism current techniques do not scale effectively for very large windows leading to slow wakeup and select logic as well as large complicated bypass networks this paper introduces new instruction scheduler implementation referred to as hierarchical scheduling windows or hsw which exploits latency tolerant instructions in order to reduce implementation complexity hsw yields very large instruction window that tolerates wakeup select and bypass latency while extracting significant far flung ilpresults it is shown that hsw loses performance per additional cycle of bypass select wakeup latency as compared to monolithic window that loses per additional cycle also hsw achieves the performance of traditional implementations with only to the number of entries in the critical timing path
bubba is highly parallel computer system for data intensive applications the basis of the bubba design is scalable shared nothing architecture which can scale up to thousands of nodes data are declustered across the nodes ie horizontally partitioned via hashing or range partitioning and operations are executed at those nodes containing relevant data in this way parallelism can be exploited within individual transactions as well as among multiple concurrent transactions to improve throughput and response times for data intensive applications the current bubba prototype runs on commercial node multicomputer and includes parallelizing compiler distributed transaction management object management and customized version of unix the current prototype is described and the major design decisions that went into its construction are discussed the lessons learned from this prototype and its predecessors are presented
in this article we consider whether traditional index structures are effective in processing unstable nearest neighbors workloads it is known that under broad conditions nearest neighbors workloads become unstable distances between data points become indistinguishable from each other we complement this earlier result by showing that if the workload for an application is unstable you are not likely to be able to index it efficiently using almost all known multidimensional index structures for broad class of data distributions we prove that these index structures will do no better than linear scan of the data as dimensionality increasesour result has implications for how experiments should be designed on index structures such as trees trees and sr trees simply put experiments trying to establish that these index structures scale with dimensionality should be designed to establish crossover points rather than to show that the methods scale to an arbitrary number of dimensions in other words experiments should seek to establish the dimensionality of the dataset at which the proposed index structure deteriorates to linear scan for each data distribution of interest that linear scan will eventually dominate is givenan important problem is to analytically characterize the rate at which index structures degrade with increasing dimensionality because the dimensionality of real data set may well be in the range that particular method can handle the results in this article can be regarded as step toward solving this problem although we do not characterize the rate at which structure degrades our techniques allow us to reason directly about broad class of index structures rather than the geometry of the nearest neighbors problem in contrast to earlier work
modeling is core software engineering practice conceptual models are constructed to establish an abstract understanding of the domain among stakeholders these are then refined into computational models that aim to realize conceptual specification the refinement process yields sets of models that are initially incomplete and inconsistent by nature the aim of the engineering process is to negotiate consistency and completeness toward stable state sufficient for deployment implementation this paper presents the notion of model ecosystem which permits the capability to guide analyst edits toward stability by computing consistency and completeness equilibria for conceptual models during periods of model change
several forms of reasoning in ai like abduction closed world reasoning circumscription and disjunctive logic programming are well known to be intractable in fact many of the relevant problems are on the second or third level of the polynomial hierarchy in this paper we show how the notion of treewidth can be fruitfully applied to this area in particular we show that all these problems become tractable actually even solvable in linear time if the treewidth of the involved formulae or programs is bounded by some constant clearly these theoretical tractability results as such do not immediately yield feasible algorithms however we have recently established new method based on monadic datalog which allowed us to design an efficient algorithm for related problem in the database area in this work we exploit the monadic datalog approach to construct new algorithms for logic based abduction
we propose different implementations of the sparse matrix dense vector multiplication spmv for finite fields and rings we take advantage of graphic card processors gpu and multi core architectures our aim is to improve the speed of spmv in the linbox library and henceforth the speed of its black box algorithms besides we use this library and new parallelisation of the sigma basis algorithm in parallel block wiedemann rank implementation over finite fields
fairground thrill laboratory was series of live events that augmented the experience of amusement rides wearable telemetry system captured video audio heart rate and acceleration data streaming them live to spectator interfaces and watching audience in this paper we present study of this event which draws on video recordings and post event interviews and which highlights the experiences of riders spectators and ride operators our study shows how the telemetry system transformed riders into performers spectators into an audience and how the role of ride operator began to include aspects of orchestration with the relationship between all three roles also transformed critically the introduction of telemetry system seems to have had the potential to re connect riders performers back to operators orchestrators and spectators audience re introducing closer relationship that used to be available with smaller rides introducing telemetry to real world situation also creates significant complexity which we illustrate by focussing on moment of perceived crisis
in order to achieve good performance in object classification problems it is necessary to combine information from various image features because the large margin classifiers are constructed based on similarity measures between samples called kernels finding appropriate feature combinations boils down to designing good kernels among set of candidates for example positive mixtures of predetermined base kernels there are couple of ways to determine the mixing weights of multiple kernels uniform weights brute force search over validation set and multiple kernel learning mkl mkl is theoretically and technically very attractive because it learns the kernel weights and the classifier simultaneously based on the margin criterion however we often observe that the support vector machine svm with the average kernel works at least as good as mkl in this paper we propose as an alternative two step approach at first the kernel weights are determined by optimizing the kernel target alignment score and then the combined kernel is used by the standard svm with single kernel the experimental results with the voc data set show that our simple procedure outperforms the average kernel and mkl
reinhard wilhelm's career in computer science spans more than third of century during this time he has made numerous research contributions in the areas of programming languages compilers and compiler generators static program analysis program transformation algorithm animation and real time systems co founded company to transfer some of these ideas to industry held the chair for programming languages and compiler construction at saarland university and served since its inception as the scientific director of the international conference and research center for computer science at schlo√ü dagstuhl
in this paper we evaluate the atomic region compiler abstraction by incorporating it into commercial system we find that atomic regions are simple and intuitive to integrate into an binary translation system furthermore doing so trivially enables additional optimization opportunities beyond that achievable by high performance dynamic optimizer which already implements superblocks we show that atomic regions can suffer from severe performance penalties if misspeculations are left uncontrolled but that simple software control mechanism is sufficient to reign in all detrimental side effects we evaluate using full reference runs of the spec cpu integer benchmarks and find that atomic regions enable up to on average improvement beyond the performance of tuned product these performance improvements are achieved without any negative side effects performance side effects such as code bloat are absent with atomic regions in fact static code size is reduced the hardware necessary is synergistic with other needs and was already available on the commercial product used in our evaluation finally the software complexity is minimal as single developer was able to incorporate atomic regions into sophisticated line code base in three months despite never having seen the translator source code beforehand
twin page storage method which is an alternative to the twist twin slot approach by reuter
the primary business model behind web search is based on textual advertising where contextually relevant ads are displayed alongside search results we address the problem of selecting these ads so that they are both relevant to the queries and profitable to the search engine showing that optimizing ad relevance and revenue is not equivalent selecting the best ads that satisfy these constraints also naturally incurs high computational costs and time constraints can lead to reduced relevance and profitability we propose novel two stage approach which conducts most of the analysis ahead of time an offine preprocessing phase leverages additional knowledge that is impractical to use in real time and rewrites frequent queries in way that subsequently facilitates fast and accurate online matching empirical evaluation shows that our method optimized for relevance matches state of the art method while improving expected revenue when optimizing for revenue we see even more substantial improvements in expected revenue
modern web search engines use different strategies to improve the overall quality of their document rankings usually the strategy adopted involves the combination of multiple sources of relevance into single ranking this work proposes the use of evolutionary techniques to derive good evidence combination functions using three different sources of evidence of relevance the textual content of documents the reputation of documents extracted from the connectivity information available in the processed collection and the anchor text concatenation the combination functions discovered by our evolutionary strategies were tested using collection containing queries extracted from real nation wide search engine query log with over million documents the experiments performed indicate that our proposal is an effective and practical alternative for combining sources of evidence into single ranking we also show that different types of queries submitted to search engine can require different combination functions and that our proposal is useful for coping with such differences
we propose framework for integrating data from multiple relational sources into an xml document that both conforms to given dtd and satisfies predefined xml constraints the framework is based on specification language aig that extends dtd by associating element types with semantic attributes inherited and synthesized inspired by the corresponding notions from attribute grammars computing these attributes via parameterized sql queries over multiple data sources and incorporating xml keys and inclusion constraints the novelty of aig consists in semantic attributes and their dependency relations for controlling context dependent dtd directed construction of xml documents as well as for checking xml constraints in parallel with document generation we also present cost based optimization techniques for efficiently evaluating aigs including algorithms for merging queries and for scheduling queries on multiple data sources this provides new grammar based approach for data integration under both syntactic and semantic constraints
technology trends present new challenges for processor architectures and their instruction schedulers growing transistor density will increase the number of execution units on single chip and decreasing wire transmission speeds will cause long and variable on chip latencies these trends will severely limit the two dominant conventional architectures dynamic issue superscalars and static placement and issue vliws we present new execution model in which the hardware and static scheduler instead work cooperatively called static placement dynamic issue spdi this paper focuses on the static instruction scheduler for spdi we identify and explore three issues spdi schedulers must consider locality contention and depth of speculation we evaluate range of spdi scheduling algorithms executing on an explicit data graph execution edge architecture we find that surprisingly simple one achieves an average of instructions per cycle ipc for spec wide issue machine and is within of the performance without on chip latencies these results suggest that the compiler is effective at balancing on chip latency and parallelism and that the division of responsibilities between the compiler and the architecture is well suited to future systems
the operations and management activities of enterprises are mainly task based and knowledge intensive accordingly an important issue in deploying knowledge management systems is the provision of task relevant information codified knowledge to meet the information needs of knowledge workers during the execution of task codified knowledge extracted from previously executed tasks can provide valuable knowledge about conducting the task at hand current task and is valuable information source for constructing task profile that models worker's task needs ie information needs for the current task in this paper we propose novel task relevance assessment approach that evaluates the relevance of previous tasks in order to construct task profile for the current task the approach helps knowledge workers assess the relevance of previous tasks through linguistic evaluation and the collaboration of knowledge workers in addition applying relevance assessment to large number of tasks may create an excessive burden for workers thus we propose novel two phase relevance assessment method to help workers conduct relevance assessment effectively furthermore modified relevance feedback technique which is integrated with the task relevance assessment method is employed to derive the task profile for the task at hand consequently task based knowledge support can be enabled to provide knowledge workers with task relevant information based on task profiles empirical experiments demonstrate that the proposed approach models workers task needs effectively and helps provide task relevant knowledge
some significant progress related to multidimensional data analysis has been achieved in the past few years including the design of fast algorithms for computing datacubes selecting some precomputed group bys to materialize and designing efficient storage structures for multidimensional data however little work has been carried out on multidimensional query optimization issues particularly the response time or evaluation cost for answering several related dimensional queries simultaneously is crucial to the olap applications recently zhao et al first exploited this problem by presenting three heuristic algorithms in this paper we first consider in detail two cases of the problem in which all the queries are either hash based star joins or index based star joins only in the case of the hash based star join we devise polynomial approximation algorithm which delivers plan whose evaluation cost is epsilon times the optimal where is the number of queries and epsilon is fixed constant with epsilon leq we also present an exponential algorithm which delivers plan with the optimal evaluation cost in the case of the index based star join we present heuristic algorithm which delivers plan whose evaluation cost is times the optimal and an exponential algorithm which delivers plan with the optimal evaluation cost we then consider general case in which both hash based star join and index based star join queries are included for this case we give possible improvement on the work of zhao et al based on an analysis of their solutions we also develop another heuristic and an exact algorithm for the problem we finally conduct performance study by implementing our algorithms the experimental results demonstrate that the solutions delivered for the restricted cases are always within two times of the optimal which confirms our theoretical upper bounds actually these experiments produce much better results than our theoretical estimates to the best of our knowledge this is the only development of polynomial algorithms for the first two cases which are able to deliver plans with deterministic performance guarantees in terms of the qualities of the plans generated the previous approaches including that of zdns may generate feasible plan for the problem in these two cases but they do not provide any performance guarantee ie the plans generated by their algorithms can be arbitrarily far from the optimal one
recently method for removing shadows from colour images was developed finlayson et al in ieee trans pattern anal mach intell that relies upon finding special direction in chromaticity feature space this invariant direction is that for which particular colour features when projected into produce greyscale image which is approximately invariant to intensity and colour of scene illumination thus shadows which are in essence particular type of lighting are greatly attenuated the main approach to finding this special angle is camera calibration colour target is imaged under many different lights and the direction that best makes colour patch images equal across illuminants is the invariant direction here we take different approach in this work instead of camera calibration we aim at finding the invariant direction from evidence in the colour image itself specifically we recognize that producing projection in the correct invariant direction will result in distribution of pixel values that have smaller entropy than projecting in the wrong direction the reason is that the correct projection results in probability distribution spike for pixels all the same except differing by the lighting that produced their observed rgb values and therefore lying along line with orientation equal to the invariant direction hence we seek that projection which produces type of intrinsic independent of lighting reflectance information only image by minimizing entropy and from there go on to remove shadows as previously to be able to develop an effective description of the entropy minimization task we go over to the quadratic entropy rather than shannon's definition replacing the observed pixels with kernel density probability distribution the quadratic entropy can be written as very simple formulation and can be evaluated using the efficient fast gauss transform the entropy written in this embodiment has the advantage that it is more insensitive to quantization than is the usual definition the resulting algorithm is quite reliable and the shadow removal step produces good shadow free colour image results whenever strong shadow edges are present in the image in most cases studied entropy has strong minimum for the invariant direction revealing new property of image formation
we investigate the relationship between symmetry reduction and inductive reasoning when applied to model checking networks of featured components popular reduction techniques for combatting state space explosion in model checking like abstraction and symmetry reduction can only be applied effectively when the natural symmetry of system is not destroyed during specification we introduce property which ensures this is preserved open symmetry we describe template based approach for the construction of open symmetric promela specifications of featured systems for certain systems safely featured parameterised systems our generated specifications are suitable for conversion to abstract specifications representing any size of network this enables feature interaction analysis to be carried out via model checking and induction for systems of any number of featured components in addition we show how for any balanced network of components by using graphical representation of the features and the process communication structure group of permutations of the underlying state space of the generated specification can be determined easily due to the open symmetry of our promela specifications this group of permutations can be used directly for symmetry reduced model checking the main contributions of this paper are an automatic method for developing open symmetric specifications which can be used for generic feature interaction analysis and the novel application of symmetry detection and reduction in the context of model checking featured networks we apply our techniques to well known example of featured network an email system
log polar imaging consists of type of methods that represent visual information with space variant resolution inspired by the visual system of mammals it has been studied for about three decades and has surpassed conventional approaches in robotics applications mainly the ones where real time constraints make it necessary to utilize resource economic image representations and processing methodologies this paper surveys the application of log polar imaging in robotic vision particularly in visual attention target tracking egomotion estimation and perception the concise yet comprehensive review offered in this paper is intended to provide novel and experienced roboticists with quick and gentle overview of log polar vision and to motivate vision researchers to investigate the many open problems that still need solving to help readers identify promising research directions possible research agenda is outlined finally since log polar vision is not restricted to robotics couple of other areas of application are discussed
to understand how and why individuals make use of emerging information assimilation services on the web as part of their daily routine we combined video recordings of online activity with targeted interviews of eleven experienced web users from these observations we describe their choice of systems the goals they are trying to achieve their information diets the basic process they use for assimilating information and the impact of user interface speed
in this article we explore the syntactic and semantic properties of prepositions in the context of the semantic interpretation of nominal phrases and compounds we investigate the problem based on cross linguistic evidence from set of six languages english spanish italian french portuguese and romanian the focus on english and romance languages is well motivated most of the time english nominal phrases and compounds translate into constructions of the form in romance languages where the preposition may vary in ways that correlate with the semantics thus we present empirical observations on the distribution of nominal phrases and compounds and the distribution of their meanings on two different corpora based on two state of the art classification tag sets lauer's set of eight prepositions and our list of semantic relations mapping between the two tag sets is also provided furthermore given training set of english nominal phrases and compounds along with their translations in the five romance languages our algorithm automatically learns classification rules and applies them to unseen test instances for semantic interpretation experimental results are compared against two state of the art models reported in the literature
this paper investigates the data exchange problem among distributed independent sources it is based on previous works in in which declarative semantics for pp systems in this semantics only facts not making the local databases inconsistent are imported weak models and the preferred weak models are those in which peers import maximal sets of facts not violating integrity constraints the framework proposed in does not provide any mechanism to set priorities among mapping rules anyhow while collecting data it is quite natural for source peer to associate different degrees of reliability to the portion of data provided by its neighbor peers starting from this observation this paper enhances previous semantics by using priority levels among mapping rules in order to select the weak models containing maximum number of mapping atoms according to their importance we will call these weak models trusted weak models and we will show they can be computed as stable models of logic program with weak constraints
significant effort has been invested in developing expressive and flexible access control languages and systems however little has been done to evaluate these systems in practical situations with real users and few attempts have been made to discover and analyze the access control policies that users actually want to implement we report on user study in which we derive the ideal access policies desired by group of users for physical security in an office environment we compare these ideal policies to the policies the users actually implemented with keys and with smartphone based distributed access control system we develop methodology that allows us to show quantitatively that the smartphone system allowed our users to implement their ideal policies more accurately and securely than they could with keys and we describe where each system fell short
location based routing lbr is one of the most widely used routing strategies in large scale wireless sensor networks with lbr small cheap and resource constrained nodes can perform the routing function without the need of complex computations and large amounts of memory space further nodes do not need to send energy consuming periodic advertisements because routing tables in the traditional sense are not needed one important assumption made by most lbr protocols is the availability of location service or mechanism to find other nodes positions although several mechanisms exist most of them rely on some sort of flooding procedure unsuitable for large scale wireless sensor networks especially with multiple and moving sinks and sources in this paper we introduce the anchor location service als protocol grid based protocol that provides sink location information in scalable and efficient manner and therefore supports location based routing in large scale wireless sensor networks the location service is evaluated mathematically and by simulations and also compared with well known grid based routing protocol our results demonstrate that als not only provides an efficient and scalable location service but also reduces the message overhead and the state complexity in scenarios with multiple and moving sinks and sources which are not usually included in the literature
ontology mapping is mandatory requirement for enabling semantic interoperability among different agents and services relying on different ontologies this aspect becomes more critical in peer to peer pp networks for several reasons the number of different ontologies can dramatically increase ii mappings among peer ontologies have to be discovered on the fly and only on the parts of ontologies contextual to specific interaction in which peers are involved iii complex mapping strategies eg structural mapping based on graph matching cannot be exploited since peers are not aware of one another's ontologies in order to address these issues we developed new ontology mapping algorithm called semantic coordinator secco secco is composed by three individual matchers syntactic lexical and contextual the syntactic matcher in order to discover mappings exploits different kinds of linguistic information eg comments labels encoded in ontology entities the lexical matcher enables discovering mappings in semantic way since it interprets the semantic meaning of concepts to be compared the contextual matcher relies on how it fits strategy inspired by the contextual theory of meaning and by taking into account the contexts in which the concepts to be compared are used refines similarity values we show through experimental results that secco fulfills two important requirements fastness and accuracy ie quality of mappings secco differently from other semantic pp applications eg piazza gridvine that assume the preexistence of mappings for achieving semantic interoperability focuses on the problem of finding mappings therefore if coupled with pp platform it paves the way towards comprehensive semantic pp solution for content sharing and retrieval semantic query answering and query routing we report on the advantages of integrating secco in the link system
in this paper we develop an automatic wrapper for the extraction of multiple sections data records from search engine results pages in the information extraction world less attention has been focused on the development of wrappers for the extraction of multiple sections data records this is evidenced by the fact that there is only one automatic wrapper mse developed for this purpose using the separation distance of data records and sections mse is able to distinguish sections and data records and extract them from search engine results pages in this study our approach is the use of dom tree properties to develop an adaptive search method which is able to detect differentiate and partition sections and data records the multiple sections data records labeled are used to pass through few filtering stages each filter is designed to filter out particular group of irrelevant data until one data region containing the relevant records is found our filtering rules are designed based on visual cue such as text and image size obtained from the browser rendering engine experimental results show that our wrapper is able to obtain better results than the currently available mse wrapper
discovery of sequential patterns is an essential data mining task with broad applications among several variations of sequential patterns closed sequential pattern is the most useful one since it retains all the information of the complete pattern set but is often much more compact than it unfortunately there is no parallel closed sequential pattern mining method proposed yet in this paper we develop an algorithm called par csp parallel closed sequential pattern mining to conduct parallel mining of closed sequential patterns on distributed memory system par csp partitions the work among the processors by exploiting the divide and conquer property so that the overhead of interprocessor communication is minimized par csp applies dynamic scheduling to avoid processor idling moreover it employs technique called selective sampling to address the load imbalance problem we implement par csp using mpi on node linux cluster our experimental results show that par csp attains good parallelization efficiencies on various input datasets
spearman's footrule and kendall's tau are two well established distances between rankings they however fail to take into account concepts crucial to evaluating result set in information retrieval element relevance and positional information that is changing the rank of highly relevant document should result in higher penalty than changing the rank of an irrelevant document similar logic holds for the top versus the bottom of the result ordering in this work we extend both of these metrics to those with position and element weights and show that variant of the diaconis graham inequality still holds the generalized two measures remain within constant factor of each other for all permutations we continue by extending the element weights into distance metric between elements for example in search evaluation swapping the order of two nearly duplicate results should result in little penalty even if these two are highly relevant and appear at the top of the list we extend the distance measures to this more general case and show that they remain within constant factor of each other we conclude by conducting simple experiments on web search data with the proposed measures our experiments show that the weighted generalizations are more robust and consistent with each other than their unweighted counter parts
we propose new approach for constructing pp networks based on dynamic decomposition of continuous space into cells corresponding to servers we demonstrate the power of this approach by suggesting two new pp architectures and various algorithms for them the first serves as dht distributed hash table and the other is dynamic expander network the dht network which we call distance halving allows logarithmic routing and load while preserving constant degrees it offers an optimal tradeoff between degree and path length in the sense that degree guarantees path length of logd another advantage over previous constructions is its relative simplicity major new contribution of this construction is dynamic caching technique that maintains low load and storage even under the occurrence of hot spots our second construction builds network that is guaranteed to be an expander the resulting topologies are simple to maintain and implement their simplicity makes it easy to modify and add protocols small variation yields dht which is robust against random byzantine faults finally we show that using our approach it is possible to construct any family of constant degree graphs in dynamic environment though with worse parameters therefore we expect that more distributed data structures could be designed and implemented in dynamic environment
wormhole attack is particularly harmful against routing in sensor networks where an attacker receives packets at one location in the network tunnels and then replays them at another remote location in the network wormhole attack can be easily launched by an attacker without compromising any sensor nodes since most of the routing protocols do not have mechanisms to defend the network against wormhole attacks the route request can be tunneled to the target area by the attacker through wormholes thus the sensor nodes in the target area build the route through the attacker later the attacker can tamper the data messages or selectively forward data messages to disrupt the functions of the sensor network researchers have used some special hardware such as the directional antenna and the precise synchronized clock to defend the sensor network against wormhole attacks during the neighbor discovery process in this paper we propose secure routing protocol against wormhole attacks in sensor networks serwa serwa protocol avoids using any special hardware such as the directional antenna and the precise synchronized clock to detect wormhole moreover it provides real secure route against the wormhole attack simulation results show that serwa protocol only has very small false positives for wormhole detection during the neighbor discovery process less than the average energy usage at each node for serwa protocol during the neighbor discovery and route discovery is below mj which is much lower than the available energy kj at each node the cost analysis shows that serwa protocol only needs small memory usage at each node below kb if each node has neighbors which is suitable for the sensor network
exception handling mechanisms are intended to support the development of robust software however the implementation of such mechanisms with aspect oriented ao programming might lead to error prone scenarios as aspects extend or replace existing functionality at specific join points in the code execution aspects behavior may bring new exceptions which can flow through the program execution in unexpected ways this paper presents systematic study that assesses the error proneness of aop mechanisms on exception flows of evolving programs the analysis was based on the object oriented and the aspect oriented versions of three medium sized systems from different application domains our findings show that exception handling code in ao systems is error prone since all versions analyzed presented an increase in the number of uncaught exceptions and exceptions caught by the wrong handler the causes of such problems are characterized and presented as catalogue of bug patterns
service oriented systems are constructed using web services as first class programmable units and subsystems and there have been many successful applications of such systems however there is major unresolved problem with the software development and subsequent management of these applications and systems web service interfaces and implementations may be developed and changed autonomously which makes traditional configuration management practices inadequate for web services checking the compatibility of these programmable units turns out to be difficult task in this paper we present technique for checking compatibility of web service interfaces and implementations based on categorizing domain ontology instances of service description documents this technique is capable of both assessing the compatibility and identifying incompatibility factors of service interfaces and implementations the design details of system model for web service compatibility checking and the key operator for evaluating compatibility within the model are discussed we present simulation experiments and analyze the results to show the effectiveness and performance variations of our technique with different data source patterns
in this paper we are interested in minimizing the delay and maximizing the lifetime of event driven wireless sensor networks for which events occur infrequently in such systems most of the energy is consumed when the radios are on waiting for packet to arrive sleep wake scheduling is an effective mechanism to prolong the lifetime of these energy constrained wireless sensor networks however sleep wake scheduling could result in substantial delays because transmitting node needs to wait for its next hop relay node to wake up an interesting line of work attempts to reduce these delays by developing anycast based packet forwarding schemes where each node opportunistically forwards packet to the first neighboring node that wakes up among multiple candidate nodes in this paper we first study how to optimize the anycast forwarding schemes for minimizing the expected packet delivery delays from the sensor nodes to the sink based on this result we then provide solution to the joint control problem of how to optimally control the system parameters of the sleep wake scheduling protocol and the anycast packet forwarding protocol to maximize the network lifetime subject to constraint on the expected end to end packet delivery delay our numerical results indicate that the proposed solution can outperform prior heuristic solutions in the literature especially under practical scenarios where there are obstructions eg lake or mountain in the coverage area of the wireless sensor network
in this paper we propose sharpness dependent filter design based on the fairing of surface normal whereby the filtering algorithm automatically selects filter this may be mean filter min filter or filter ranked between these two depending on the local sharpness value and the sharpness dependent weighting function selected to recover the original shape of noisy model the algorithm selects mean filter for flat regions and min filter for distinguished sharp regions the selected sharpness dependent weighting function has gaussian laplacian or el fallah ford form that approximately fits the sharpness distribution found in all tested noisy models we use sharpness factor in the weighting function to control the degree of feature preserving the appropriate sharpness factor can be obtained by sharpness analysis based on the bayesian classification our experiment results demonstrate that the proposed sharpness dependent filter is superior to other approaches for smoothing polygon mesh as well as for preserving its sharp features
singleton kinds provide an elegant device for expressing type equality information resulting from modern module languages but they can complicate the metatheory of languages in which they appear present translation from language with singleton kinds to one without and prove this translation to be sound and complete this translation is useful for type preserving compilers generating typed target languages the proof of soundness and completeness is done by normalizing type equivalence derivations using stone and harper's type equivalence decision procedure
the paper investigates geometric properties of quasi perspective projection model in one and two view geometry the main results are as follows quasi perspective projection matrix has nine degrees of freedom dof and the parallelism along and directions in world system are preserved in images ii quasi fundamental matrix can be simplified to special form with only six dofs the fundamental matrix is invariant to any non singular projective transformation iii plane induced homography under quasi perspective model can be simplified to special form defined by six dofs the quasi homography may be recovered from two pairs of corresponding points with known fundamental matrix iv any two reconstructions in quasi perspective space are defined up to non singular quasi perspective transformation the results are validated experimentally on both synthetic and real images
this paper presents an end user oriented programming environment called mashroom major contributions herein include an end user programming model with an expressive data structure as well as set of formally defined mashup operators the data structure takes advantage of nested table and maintains the intuitiveness while allowing users to express complex data objects the mashup operators are visualized with contextual menu and formula bar and can be directly applied on the data experiments and case studies reveal that end users have little difficulty in effectively and efficiently using mashroom to build mashup applications
how to save energy is critical issue for the life time of sensor networks under continuously changing environments sensor nodes have varying sampling rates in this paper we present an online algorithm to minimize the total energy consumption while satisfying sampling rate with guaranteed probability we model the sampling rate as random variable which is estimated over finite time window an efficient algorithm eosp energy aware online algorithm to satisfy sampling rates with guaranteed probability is proposed our approach can adapt the architecture accordingly to save energy experimental results demonstrate the effectiveness of our approach
policies in modern systems and applications play an essential role we argue that decisions based on policy rules should take into account the possibility for the users to enable specific policy rules by performing actions at the time when decisions are being rendered and or by promising to perform other actions in the future decisions should also consider preferences among different sets of actions enabling different rules we adopt formalism and mechanism devised for policy rule management in this context and investigate in detail the notion of obligations which are those actions users promise to perform in the future upon firing of specific policy rule we also investigate how obligations can be monitored and how the policy rules should be affected when obligations are either fulfilled or defaulted
vision based human action recognition provides an advanced interface and research in the field of human action recognition has been actively carried out however an environment from dynamic viewpoint where we can be in any position any direction etc must be considered in our living space in order to overcome the viewpoint dependency we propose volume motion template vmt and projected motion template pmt the proposed vmt method is an extension of the motion history image mhi method to space the pmt is generated by projecting the vmt into plane that is orthogonal to an optimal virtual viewpoint where the optimal virtual viewpoint is viewpoint from which an action can be described in greatest detail in space from the proposed method any actions taken from different viewpoints can be recognized independent of the viewpoints the experimental results demonstrate the accuracies and effectiveness of the proposed vmt method for view independent human action recognition
in blaze bleumer and strauss bbs proposed an application called atomic proxy re encryption in which semitrusted proxy converts ciphertext for alice into ciphertext for bob without seeing the underlying plaintext we predict that fast and secure re encryption will become increasingly popular as method for managing encrypted file systems although efficiently computable the wide spread adoption of bbs re encryption has been hindered by considerable security risks following recent work of dodis and ivan we present new re encryption schemes that realize stronger notion of security and demonstrate the usefulness of proxy re encryption as method of adding access control to secure file system performance measurements of our experimental file system demonstrate that proxy re encryption can work effectively in practice
advances in biological experiments such as dna microarrays have produced large multidimensional data sets for examination and retrospective analysis scientists however heavily rely on existing biomedical knowledge in order to fully analyze and comprehend such datasets our proposed framework relies on the gene ontology for integrating priori biomedical knowledge into traditional data analysis approaches we explore the impact of considering each aspect of the gene ontology individually for quantifying the biological relatedness between gene products we discuss two figure of merit scores for quantifying the pair wise biological relatedness between gene products and the intra cluster biological coherency of groups of gene products finally we perform cluster deterioration simulation experiments on well scrutinized saccharomyces cerevisiae data set consisting of hybridization measurements the results presented illustrate strong correlation between the devised cluster coherency figure of merit and the randomization of cluster membership
modern business process management expands to cover the partner organisations business processes across organisational boundaries and thereby supports organisations to coordinate the flow of information among organisations and link their business processes with collaborative business processes organisations can create dynamic and flexible collaborations to synergically adapt to the changing conditions and stay competitive in the global market due to its significant potential and value collaborative business processes are now turning to be an important issue of contemporary business process management and attracts lots of attention and efforts from both academic and industry sides in this paper we review the development of bb collaboration and collaborative business processes provide an overview of related issues in managing collaborative business processes and discuss some emerging technologies and their relationships to collaborative business processes finally we introduce the papers that are published in this special issue
we apply an extension of the nelson oppen combination method to develop decision procedure for the non disjoint union of theories modeling data structures with counting operator and fragments of arithmetic we present some data structures and some fragments of arithmetic for which the combination method is complete and effective to achieve effectiveness the combination method relies on particular procedures to compute sets that are representative of all the consequences over the shared theory we show how to compute these sets by using superposition calculus for the theories of the considered data structures and various solving and reduction techniques for the fragments of arithmetic we are interested in including gauss elimination fourier motzkin elimination and groebner bases computation
we present the first to our knowledge approximation algorithm for tensor clustering powerful generalization to basic clustering tensors are increasingly common in modern applications dealing with complex heterogeneous data and clustering them is fundamental tool for data analysis and pattern discovery akin to their cousins common tensor clustering formulations are np hard to optimize but unlike the case no approximation algorithms seem to be known we address this imbalance and build on recent co clustering work to derive tensor clustering algorithm with approximation guarantees allowing metrics and divergences eg bregman as objective functions therewith we answer two open questions by anagnostopoulos et al our analysis yields constant approximation factor independent of data size worst case example shows this factor to be tight for euclidean co clustering however empirically the approximation factor is observed to be conservative so our method can also be used in practice
many multicast overlay networks maintain application specific performance goals by dynamically adapting the overlay structure when the monitored performance becomes inadequate this adaptation results in an unstructured overlay where no neighbor selection constraints are imposed although such networks provide resilience to benign failures they are susceptible to attacks conducted by adversaries that compromise overlay nodes previous defense solutions proposed to address attacks against overlay networks rely on strong organizational constraints and are not effective for unstructured overlays in this work we identify demonstrate and mitigate insider attacks against measurement based adaptation mechanisms in unstructured multicast overlay networks we propose techniques to decrease the number of incorrect adaptations by using outlier detection and limit the impact of malicious nodes by aggregating local information to derive global reputation for each node we demonstrate the attacks and mitigation techniques through real life deployments of mature overlay multicast system
due to the rapid development in mobile communication technologies the usage of mobile devices such as cell phone or pda has increased significantly as different devices require different applications various new services are being developed to satisfy the needs one of the popular services under heavy demand is the location based service lbs that exploits the spatial information of moving objects per temporal changes in order to support lbs well in this paper we investigate how spatio temporal information of moving objects can be efficiently stored and indexed in particular we propose novel location encoding method based on hierarchical administrative district information our proposal is different from conventional approaches where moving objects are often expressed as geometric points in two dimensional space instead in ours moving objects are encoded as one dimensional points by both administrative district as well as road information our method becomes especially useful for monitoring traffic situation or tracing location of moving objects through approximate spatial queries
as current trends in software development move toward more complex object oriented programming inlining has become vital optimization that provides substantial performance improvements to and java programs yet the aggressiveness of the inlining algorithm must be carefully monitored to effectively balance performance and code size the state of the art is to use profile information associated with call edges to guide inlining decisions in the presence of virtual method calls profile information for one call edge may not be sufficient for making effectual inlining decisions therefore we explore the use of profiling data with additional levels of context sensitivity in addition to exploring fixed levels of context sensitivity we explore several adaptive schemes that attempt to find the ideal degree of context sensitivity for each call site our techniques are evaluated on the basis of runtime performance code size and dynamic compilation time on average we found that with minimal impact on performance context sensitivity can enable reductions in compiled code space and compile time performance on individual programs varied from minus to while reductions in compile time and code space of up to and respectively were obtained
suffix trees are by far the most important data structure in stringology with myriads of applications in fields like bioinformatics and information retrieval classical representations of suffix trees require log bits of space for string of size this is considerably more than the log bits needed for the string itself where is the alphabet size the size of suffix trees has been barrier to their wider adoption in practice recent compressed suffix tree representations require just the space of the compressed string plus extra bits this is already spectacular but still unsatisfactory when is small as in dna sequences in this paper we introduce the first compressed suffix tree representation that breaks this linear space barrier our representation requires sublinear extra space and supports large set of navigational operations in logarithmic time an essential ingredient of our representation is the lowest common ancestor lca query we reveal important connections between lca queries and suffix tree navigation
approximation algorithms for clustering points in metric spaces is flourishing area of research with much research effort spent on getting better understanding of the approximation guarantees possible for many objective functions such as median means and min sum clustering this quest for better approximation algorithms is further fueled by the implicit hope that these better approximations also yield more accurate clusterings eg for many problems such as clustering proteins by function or clustering images by subject there is some unknown correct target clustering and the implicit hope is that approximately optimizing these objective functions will in fact produce clustering that is close pointwise to the truth in this paper we show that if we make this implicit assumption explicit that is if we assume that any approximation to the given clustering objective phi is epsilon close to the target then we can produce clusterings that are epsilon close to the target even for values for which obtaining approximation is np hard in particular for median and means objectives we show that we can achieve this guarantee for any constant and for the min sum objective we can do this for any constant our results also highlight surprising conceptual difference between assuming that the optimal solution to say the median objective is epsilon close to the target and assuming that any approximately optimal solution is epsilon close to the target even for approximation factor say in the former case the problem of finding solution that is epsilon close to the target remains computationally hard and yet for the latter we have an efficient algorithm
decoupled software pipelining dswp is one approach to automatically extract threads from loops it partitions loops into long running threads that communicate in pipelined manner via inter core queues this work recognizes that dswp can also be an enabling transformation for other loop parallelization techniques this use of dswp called dswp splits loop into new loops with dependence patterns amenable to parallelization using techniques that were originally either inapplicable or poorly performing by parallelizing each stage of the dswp pipeline using potentially different techniques not only is the benefit of dswp increased but the applicability and performance of other parallelization techniques are enhanced this paper evaluates dswp as an enabling framework for other transformations by applying it in conjunction with doall localwrite and specdoall to individual stages of the pipeline this paper demonstrates significant performance gains on commodity core multicore machine running variety of codes transformed with dswp
insufficiency of labeled training data is major obstacle for automatic video annotation semi supervised learning is an effective approach to this problem by leveraging large amount of unlabeled data however existing semi supervised learning algorithms have not demonstrated promising results in large scale video annotation due to several difficulties such as large variation of video content and intractable computational cost in this paper we propose novel semi supervised learning algorithm named semi supervised kernel density estimation sskde which is developed based on kernel density estimation kde approach while only labeled data are utilized in classical kde in sskde both labeled and unlabeled data are leveraged to estimate class conditional probability densities based on an extended form of kde it is non parametric method and it thus naturally avoids the model assumption problem that exists in many parametric semi supervised methods meanwhile it can be implemented with an efficient iterative solution process so this method is appropriate for video annotation furthermore motivated by existing adaptive kde approach we propose an improved algorithm named semi supervised adaptive kernel density estimation ssakde it employs local adaptive kernels rather than fixed kernel such that broader kernels can be applied in the regions with low density in this way more accurate density estimates can be obtained extensive experiments have demonstrated the effectiveness of the proposed methods
acquiring models of intricate objects like tree branches bicycles and insects is challenging task due to severe self occlusions repeated thin structures and surface discontinuities in theory shape from silhouettes sfs approach can overcome these difficulties and reconstruct visual hulls that are close to the actual shapes regardless of the complexity of the object in practice however sfs is highly sensitive to errors in silhouette contours and the calibration of the imaging system and has therefore not been used for obtaining accurate shapes with large number of views in this work we present practical approach to sfs using novel technique called coplanar shadowgram imaging that allows us to use dozens to even hundreds of views for visual hull reconstruction point light source is moved around an object and the shadows silhouettes cast onto single background plane are imaged we characterize this imaging system in terms of image projection reconstruction ambiguity epipolar geometry and shape and source recovery the coplanarity of the shadowgrams yields unique geometric properties that are not possible in traditional multi view camera based imaging systems these properties allow us to derive robust and automatic algorithm to recover the visual hull of an object and the positions of the light source simultaneously regardless of the complexity of the object we demonstrate the acquisition of several intricate shapes with severe occlusions and thin structures using to views
the unix fast file system ffs is probably the most widely used file system for performance comparisons however such comparisons frequently overlook many of the performance enhancements that have been added over the past decade in this paper we explore the two most commonly used approaches for improving the performance of meta data operations and recovery journaling and soft updates journaling systems use an auxiliary log to record meta data operations and soft updates uses ordered writes to ensure metadata consistency the commercial sector has moved en masse to journaling file systems as evidenced by their presence on nearly every server platform available today solaris aix digital unix hp ux irix and windows nt on all but solaris the default file system uses journaling in the meantime soft updates holds the promise of providing stronger reliability guarantees than journaling with faster recovery and superior performance in certain boundary cases in this paper we explore the benefits of soft updates and journaling comparing their behavior on both microbenchmarks and workload based macrobenchmarks we find that journaling alone is not sufficient to solve the meta data update problem if synchronous semantics are required ie meta data operations are durable once the system call returns then the journaling systems cannot realize their full potential only when this synchronicity requirement is relaxed can journaling systems approach the performance of systems like soft updates which also relaxes this requirement our asynchronous journaling and soft updates systems perform comparably in most cases while soft updates excels in some meta data intensive microbenchmarks the macrobenchmark results are more ambiguous in three cases soft updates and journaling are comparable in file intensive news workload journaling prevails and in small isp workload soft updates prevails
currently there are no known explicit algorithms for the great majority of graph problems in the dynamic distributed message passing model instead most state of the art dynamic distributed algorithms are constructed by composing static algorithm for the problem at hand with simulation technique that converts static algorithms to dynamic ones we argue that this powerful methodology does not provide satisfactory solutions for many important dynamic distributed problems and this necessitates developing algorithms for these problems from scratch in this paper we develop fully dynamic distributed algorithm for maintaining sparse spanners our algorithm improves drastically the quiescence time of the state of the art algorithm for the problem moreover we show that the quiescence time of our algorithm is optimal up to small constant factor in addition our algorithm improves significantly upon the state of the art algorithm in all efficiency parameters specifically it has smaller quiescence message and space complexities and smaller local processing time finally our algorithm is self contained and fairly simple and is consequently amenable to implementation on unsophisticated network devices
modeling spatiotemporal data in particular fuzzy and complex spatial objects representing geographic entities and relations is topic of great importance in geographic information systems computer vision environmental data management systems etc because of complex requirements it is challenging to represent spatiotemporal data and its features in databases and to effectively query them this article presents new approach to model and query the spatiotemporal data of fuzzy spatial and complex objects and or spatial relations in our case study we use meteorological database application in an intelligent database architecture which combines an object oriented database with knowledgebase for modeling and querying spatiotemporal objects
we present synchroscalar tile based architecture forembedded processing that is designed to provide the flexibilityof dsps while approaching the power efficiency ofasics we achieve this goal by providing high parallelismand voltage scaling while minimizing control and communicationcosts specifically synchroscalar uses columnsof processor tiles organized into statically assignedfrequency voltage domains to minimize power consumptionfurthermore while columns use simd control to minimizeoverhead data dependent computations can besupported by extremely flexible statically scheduled communicationbetween columnswe provide detailed evaluation of synchroscalar includingspice simulation wire and device models synthesisof key components cycle level simulation andcompiler and hand optimized signal processing applicationswe find that the goal of meeting not exceeding performancetargets with data parallel applications leads todesigns that depart significantly from our intuitions derivedfrom general purpose microprocessor design inparticular synchronous design and substantial global interconnectare desirable in the low frequency low powerdomain this global interconnect supports parallelizationand reduces processor idle time which are critical to energyefficient implementations of high bandwidth signalprocessing overall synchroscalar provides programmabilitywhile achieving power efficiencies within ofknown asic implementations which is better thanconventional dsps in addition frequency voltage scalingin synchroscalar provides between power savingsin our application suite
adaptive object models aom are sophisticated way of building object oriented systems that let non programmers customize the behavior of the system and that are most useful for businesses that are rapidly changing although systems based on an aom are often much smaller than competitors they can be difficult to build and to learn we believe that the problems with aom are due in part to mismatch between their design and the languages that are used to build them this paper describes how to avoid this mismatch by using implicit and explicit metaclasses
this paper proposes using user level memory thread ulmt for correlation prefetching in this approach user thread runs on general purpose processor in main memory either in the memory controller chip or in dram chip the thread performs correlation prefetching in software sending the prefetched data into the cache of the main processor this approach requires minimal hardware beyond the memory processor the correlation table is software data structure that resides in main memory while the main processor only needs few modifications to its cache so that it can accept incoming prefetches in addition the approach has wide applicability as it can effectively prefetch even for irregular applications finally it is very flexible as the prefetching algorithm can be customized by the user on an application basis our simulation results show that through new design of the correlation table and prefetching algorithm our scheme delivers good results specifically nine mostly irregular applications show an average speedup of furthermore our scheme works well in combination with conventional processor side sequential prefetcher in which case the average speedup increases to finally by exploiting the customization of the prefetching algorithm we increase the average speedup to
this paper describes enhanced subquery optimizations in oracle relational database system it discusses several techniques subquery coalescing subquery removal using window functions and view elimination for group by queries these techniques recognize and remove redundancies in query structures and convert queries into potentially more optimal forms the paper also discusses novel parallel execution techniques which have general applicability and are used to improve the scalability of queries that have undergone some of these transformations it describes new variant of antijoin for optimizing subqueries involved in the universal quantifier with columns that may have nulls it then presents performance results of these optimizations which show significant execution time improvements
the replica placement problem rpp aims at creating set of duplicated data objects across the nodes of distributed system in order to optimize certain criteria typically rpp formulations fall into two categories static and dynamic the first assumes that access statistics are estimated in advance and remain static and therefore one time replica distribution is sufficient irpp in contrast dynamic methods change the replicas in the network potentially upon every request this paper proposes an alternative technique named continuous replica placement problem crpp which falls between the two extreme approaches crpp can be defined as given an already implemented replication scheme and estimated access statistics for the next time period define new replication scheme subject to optimization criteria and constraints as we show in the problem formulation crpp is different in that the existing heuristics in the literature cannot be used either statically or dynamically to solve the problem in fact even with the most careful design their performance will be inferior since crpp embeds scheduling problem to facilitate the proposed mechanism we provide insight on the intricacies of crpp and propose various heuristics
software classification models have been regarded as an essential support tool in performing measurement and analysis processes most of the established models are single cycled in the model usage stage and thus require the measurement data of all the model's variables to be simultaneously collected and utilized for classifying an unseen case within only single decision cycle conversely the multi cycled model allows the measurement data of all the model's variables to be gradually collected and utilized for such classification within more than one decision cycle and thus intuitively seems to have better classification efficiency but poorer classification accuracy software project managers often have difficulties in choosing an appropriate classification model that is better suited to their specific environments and needs however this important topic is not adequately explored in software measurement and analysis literature by using an industrial software measurement dataset of nasa kc this paper explores the quantitative performance comparisons of the classification accuracy and efficiency of the discriminant analysis da and logistic regression lr based single cycled models and the decision tree dt based and echaid algorithms multi cycled models the experimental results suggest that the re appraisal cost of the type mr the software failure cost of type ii mr and the data collection cost of software measurements should be considered simultaneously when choosing an appropriate classification model
cyber physical systems increasingly rely on dynamically adaptive programs to respond to changes in their physical environment examples include ecosystem monitoring and disaster relief systems these systems are considered high assurance since errors during execution could result in injury loss of life environmental impact and or financial loss in order to facilitate the development and verification of dynamically adaptive systems we separate functional concerns from adaptive concerns specifically we model dynamically adaptive program as collection of non adaptive steady state programs and set of adaptations that realize transitions among steady state programs in response to environmental changes we use linear temporal logic ltl to specify properties of the non adaptive portions of the system and we use ltl an adapt operator extension toltl to concisely specify properties that hold during the adaptation process model checking offers an attractive approach to automatically analyzing models for adherence to formal properties and thus providing assurance however currently model checkers are unable to verify properties specified using ltl moreover as the number of steady state programs and adaptations increase the verification costs in terms of space and time potentially become unwieldy to address these issues we propose modular model checking approach to verifying that formal model of an adaptive program satisfies its requirements specified in ltl and ltl respectively
as the size of an rfid tag becomes smaller and the price of the tag gets lower rfid technology has been applied to wide range of areas recently rfid has been adopted in the business area such as supply chain management since companies can get movement information for products easily using the rfid technology it is expected to revolutionize supply chain management however the amount of rfid data in supply chain management is huge therefore it requires much time to extract valuable information from rfid data for supply chain management in this paper we define query templates for tracking queries and path oriented queries to analyze the supply chain we then propose an effective path encoding scheme to encode the flow information for products to retrieve the time information for products efficiently we utilize numbering scheme used in the xml area based on the path encoding scheme and the numbering scheme we devise storage scheme to process tracking queries and path oriented queries efficiently finally we propose method which translates the queries to sql queries experimental results show that our approach can process the queries efficiently on the average our approach is about times better than recent technique in terms of query performance
we review two areas of recent research linking proportional fairness with product form networks the areas concern respectively the heavy traffic and the large deviations limiting regimes for the stationary distribution of flow model where the flow model is stochastic process representing the randomly varying number of document transfers present in network sharing capacity according to the proportional fairness criterion in these two regimes we postulate the limiting form of the stationary distribution by comparison with several variants of the fairness criterion we outline how product form results can help provide insight into the performance consequences of resource pooling
large scale supercomputing is revolutionizing the way science is conducted growing challenge however is understanding the massive quantities of data produced by large scale simulations the data typically time varying multivariate and volumetric can occupy from hundreds of gigabytes to several terabytes of storage space transferring and processing volume data of such sizes is prohibitively expensive and resource intensive although it may not be possible to entirely alleviate these problems data compression should be considered as part of viable solution especially when the primary means of data analysis is volume rendering in this paper we present our study of multivariate compression which exploits correlations among related variables for volume rendering two configurations for multidimensional compression based on vector quantization are examined we emphasize quality reconstruction and interactive rendering which leads us to solution using graphics hardware to perform on the fly decompression during rendering
abstract in this paper we extend the standard for object oriented databases odmg with reactive features by proposing language for specifying triggers and defining its semantics this extension has several implications thus this work makes three different specific contributions first the definition of declarative data manipulation language for odmg which is missing in the current version of the standard such definition requires revisiting data manipulation in odmg and also addressing issues related to set oriented versus instance oriented computation then the definition of trigger language for odmg unifying also the sql proposal and providing support for trigger inheritance and overriding finally the development of formal semantics for the proposed data manipulation and trigger languages
most future large scale sensor networks are expected to follow two tier architecture which consists of resource rich master nodes at the upper tier and resource poor sensor nodes at the lower tier sensor nodes submit data to nearby master nodes which then answer the queries from the network owner on behalf of sensor nodes relying on master nodes for data storage and query processing raises severe concerns about data confidentiality and query result correctness when the sensor network is deployed in hostile environments in particular compromised master node may leak hosted sensitive data to the adversary it may also return juggled or incomplete query results to the network owner this paper for the first time in the literature presents suite of novel schemes to secure multidimensional range queries in tiered sensor networks the proposed schemes can ensure data confidentiality against master nodes and also enable the network owner to verify with very high probability the authenticity and completeness of any query result by inspecting the spatial and temporal relationships among the returned data detailed performance evaluations confirm the high efficacy and efficiency of the proposed schemes
applications ranging from location based services to multi player online gaming require continuous query support to monitor track and detect events of interest among sets of moving objects examples are alerting capabilities for detecting whether the distance the travel cost or the travel time among set of moving objects exceeds threshold these types of queries are driven by continuous streams of location updates simultaneously evaluated over many queries in this paper we define three types of proximity relations that induce location constraints to model continuous spatio temporal queries among sets of moving objects in road networks our focus lies on evaluating large number of continuous queries simultaneously we introduce novel moving object indexing technique that together with novel road network partitioning scheme restricts computations within the partial road network these techniques reduce query processing overhead by more than experiments over real world data sets show that our approach is twenty times faster than baseline algorithm
wireless sensor networks produce large amount of data that needs to be processed delivered and assessed according to the application objectives the way these data are manipulated by the sensor nodes is fundamental issue information fusion arises as response to process data gathered by sensor nodes and benefits from their processing capability by exploiting the synergy among the available data information fusion techniques can reduce the amount of data traffic filter noisy measurements and make predictions and inferences about monitored entity in this work we survey the current state of the art of information fusion by presenting the known methods algorithms architectures and models of information fusion and discuss their applicability in the context of wireless sensor networks
the technique of latent semantic indexing is used in wide variety of commercial applications in these applications the processing time and ram required for svd computation and the processing time and ram required during lsi retrieval operations are all roughly linear in the number of dimensions chosen for the lsi representation space in large scale commercial lsi applications reducing values could be of significant value in reducing server costs this paper explores the effects of varying dimensionality the approach taken here focuses on term comparisons pairs of terms are considered which have strong real world associations the proximities of members of these pairs in the lsi space are compared at multiple values of the testing is carried out for collections of from one to five million documents for the five million document collection value of provides the best performance the results suggest that there is something of an island of stability in the to range the results also indicate that there is relatively little room to employ values outside of this range without incurring significant distortions in at least some term term correlations
traditional content based image retrieval systems typically compute single descriptor per image based for example on color histograms the result of query is in general the images from the database whose descriptors are the closest to the descriptor of the query image systems built this way are able to return images that are globally similar to the query image but can not return images that contain some of the objects that are in the query as opposed to this traditional coarse grain recognition scheme recent advances in image processing make fine grain image recognition possible notably by computing local descriptors that can detect similar objects in different images obviously powerful fine grain recognition in images also changes the retrieval process instead of submitting single query to retrieve similar images multiple queries must be submitted and their partial results must be post processed before delivering the answer this paper first presents family of local descriptors supporting fine grain image recognition these descriptors enforce robust recognition despite image rotations and translations illumination variations and partial occlusions many multi dimensional indexes have been proposed to speed up the retrieval process these indexes however have been mostly designed for and evaluated against databases where each image is described by single descriptor while this paper does not present any new indexing scheme it shows that the three most efficient indexing techniques known today are still too slow to be used in practice with local descriptors because of the changes in the retrieval process
the advance of the web has significantly and rapidly changed the way of information organization sharing and distribution the next generation of the web the semantic web seeks to make information more usable by machines by introducing more rigorous structure based on ontologies in this context we try to propose novel and integrated approach for semi automated extraction of ontology based semantic web from data intensive web application and thus make the web content machine understandable our approach is based on the idea that semantics can be extracted by applying reverse engineering technique on the structures and the instances of html forms which are the most convenient interface to communicate with relational databases on the current data intensive web application this semantics is exploited to produce over several steps personalised ontology
we consider the problem of flow coordination in distributed multimedia applications most transport level protocols are designed to operate independently and lack mechanisms for sharing information with other flows and coordinating data transport in various ways this limitation becomes problematic in distributed applications that employ numerous flows between two computing clusters sharing the same intermediary forwarding path across the internet in this article we propose an open architecture that supports the sharing of network state information peer flow information and application specific information called simply the coordination protocol cp the scheme facilitates coordination of network resource usage across flows belonging to the same application as well as aiding other types of coordination the effectiveness of our approach is illustrated in the context of multistreaming in tele immersion where consistency of network information across flows both greatly improves frame transport synchrony and minimizes buffering delay
nodes in the hexagonal mesh and torus network are placed at the vertices of regular triangular tessellation so that each node has up to six neighbors the routing algorithm for the hexagonal torus is very complicated and it is an open problem by now hexagonal mesh and torus are known to belong to the class of cayley digraphs in this paper we use cayley formulations for the hexagonal torus along with some result on subgraphs and coset graphs to develop the optimal routing algorithm for the hexagonal torus and then we draw conclusions to the network diameter of the hexagonal torus
deformable isosurfaces implemented with level set methods have demonstrated great potential in visualization and computer graphics for applications such as segmentation surface processing and physically based modeling their usefulness has been limited however by their high computational cost and reliance on signi pound cant parameter tuning this paper presents solution to these challenges by describing graphics processor gpu based algorithms for solving and visualizing level set solutions at interactive rates the proposed solution is based on new streaming implementation of the narrow band algorithm the new algorithm packs the level set isosurface data into texture memory via multi dimensional virtual memory system as the level set moves this texture based representation is dynamically updated via novel gpu to cpu message passing scheme by integrating the level set solver with real time volume renderer user can visualize and intuitively steer the level set surface as it evolves we demonstrate the capabilities of this technology for interactive volume segmentation and visualization
usage data captured and logged by computers has long been an essential source of information for software developers support services personnel usability designers and learning researchers whether from mainframes file servers network devices or workstations the user event data logged in its many forms has served as an essential source of information for those who need to improve software analyze problems monitor security track workflow report on resource usage evaluate learning activities etc with today's generation of open and community source web based frameworks however new challenges arise as to how where and when user activity gets captured and analyzed these frameworks flexibility in allowing easy integration of different applications presentation technologies middleware and data sources has side effects on usage data fragmented logs in wide range of formats often bestrewn across many locations this paper focuses on common issues faced especially by academic computing support personnel who need to gather and analyze user activity information within heterogeneous distributed open source web frameworks like sakai and uportal as described in this paper these kinds of challenges can be met by drawing upon techniques for coordinated distributed event monitoring along with some basic data mining and data visualization approaches in particular this paper describes work in progress to develop an approach towards building distributed capture and analysis systems for large production deployment of the sakai collaboration and learning environment in order to meet wide range of tracking monitoring and reporting log analysis in one university setting
in the sponsored search model search engines are paid by businesses that are interested in displaying ads for their site alongside the search results businesses bid for keywords and their ad is displayed when the keyword is queried to the search engine an important problem in this process is keyword generation given business that is interested in launching campaign suggest keywords that are related to that campaign we address this problem by making use of the query logs of the search engine we identify queries related to campaign by exploiting the associations between queries and urls as they are captured by the user's clicks these queries form good keyword suggestions since they capture the wisdom of the crowd as to what is related to site we formulate the problem as semi supervised learning problem and propose algorithms within the markov random field model we perform experiments with real query logs and we demonstrate that our algorithms scale to large query logs and produce meaningful results
tracked objects rarely move alone they are often temporarily accompanied by other objects undergoing similar motion we propose novel tracking algorithm called sputnik tracker it is capable of identifying which image regions move coherently with the tracked object this information is used to stabilize tracking in the presence of occlusions or fluctuations in the appearance of the tracked object without the need to model its dynamics in addition sputnik tracker is based on novel template tracker integrating foreground and background appearance cues the time varying shape of the target is also estimated in each video frame together with the target position the time varying shape is used as another cue when estimating the target position in the next frame
in this position paper we discuss number of issues relating to model metrics with particular emphasis on metrics for uml models our discussion is presented as series of nine observations where we examine some of the existing work on applying metrics to uml models present some of our own work in this area and specify some topics for future research that we regard as important furthermore we identify three categories of challeges for model metrics and describe how our nine observations can be partitioned into these categories
relatively small set of static instructions has significant leverage on program execution performance these problem instructions contribute disproportionate number of cache misses and branch mispredictions because their behavior cannot be accurately anticipated using existing prefetching or branch prediction mechanisms the behavior of many problem instructions can be predicted by executing small code fragment called speculative slice if speculative slice is executed before the corresponding problem instructions are fetched then the problem instructions can move smoothly through the pipeline because the slice has tolerated the latency of the memory hierarchy for loads or the pipeline for branches this technique results in speedups up to percent over an aggressive baseline machine to benefit from branch predictions generated by speculative slices the predictions must be bound to specific dynamic branch instances we present technique that invalidates predictions when it can be determined by monitoring the program's execution path that they will not be used this enables the remaining predictions to be correctly correlated
for any outsourcing service privacy is major concern this paper focuses on outsourcing frequent itemset mining and examines the issue on how to protect privacy against the case where the attackers have precise knowledge on the supports of some items we propose new approach referred to as support anonymity to protect each sensitive item with other items of similar support to achieve support anonymity we introduce pseudo taxonomy tree and have the third party mine the generalized frequent itemsets under the corresponding generalized association rules instead of association rules the pseudo taxonomy is construct to facilitate hiding of the original items where each original item can map to either leaf node or an internal node in the taxonomy tree the rationale for this approach is that with taxonomy tree the nodes to satisfy the support anonymity may be any nodes in the taxonomy tree with the appropriate supports so this approach can provide more candidates for support anonymity with limited fake items as only the leaf nodes not the internal nodes of the taxonomy tree need to appear in the transactions otherwise for the association rule mining the nodes to satisfy the support anonymity have to correspond to the leaf nodes in the taxonomy tree this is far more restricted the challenge is thus on how to generate the pseudo taxonomy tree to facilitate support anonymity and to ensure the conservation of original frequent itemsets the experimental results showed that our methods of support anonymity can achieve very good privacy protection with moderate storage overhead
topical crawlers are increasingly seen as way to address the scalability limitations of universal search engines by distributing the crawling process across users queries or even client computers the context available to such crawlers can guide the navigation of links with the goal of efficiently locating highly relevant target pages we developed framework to fairly evaluate topical crawling algorithms under number of performance metrics such framework is employed here to evaluate different algorithms that have proven highly competitive among those proposed in the literature and in our own previous research in particular we focus on the tradeoff between exploration and exploitation of the cues available to crawler and on adaptive crawlers that use machine learning techniques to guide their search we find that the best performance is achieved by novel combination of explorative and exploitative bias and introduce an evolutionary crawler that surpasses the performance of the best nonadaptive crawler after sufficiently long crawls we also analyze the computational complexity of the various crawlers and discuss how performance and complexity scale with available resources evolutionary crawlers achieve high efficiency and scalability by distributing the work across concurrent agents resulting in the best performance cost ratio
this paper argues for set of requirements that an architectural style for self healing systems should satisfy adaptability dynamicity awareness autonomy robustness distributability mobility and traceability support for these requirements is discussed along five dimensions we have identified as distinguishing characteristics of architectural styles external structure topology rules behavior interaction and data flow as an illustration these requirements are used to assess an existing architectural style while this initial formulation of the requirements appears to have utility much further work remains to be done in order to apply it in evaluating and comparing architectural styles for self healing systems
leakage energy will be the major energy consumer in futuredeep sub micron designs especially the memory sub systemof future socs will be negatively affected by this trend inorder to reduce the leakage energy memory banks are transitionedto low energy state when possible this transitionitself costs some energy which is termed as the transition energyin this paper we present as the first approach of its kind novel energy saving replacement policy called lru seq forinstruction caches evaluation of the policy on various architecturesin system level environment has shown that upto energy savings can be obtained considering the negligiblehardware impact lru seq offers viable choice foran energy saving policy
in this work we propose method to reduce the impact of process variations by adapting the application's algorithm at the software layer we introduce the concept of hardware signatures as the measured post manufacturing hardware characteristics that can be used to drive software adaptation across different die using encoding as an example we demonstrate significant yield improvements as much as points at over design reduction in over design by as much as points at yield as well as application quality improvements about db increase in average psnr at yield further we investigate implications of limited information exchange ie signature measurement granularity on yield and quality we show that our proposed technique for determining optimal signature measurement points results in an improvement in psnr of about db over naive sampling for the encoder we conclude that hardware signature based application adaptation is an easy and inexpensive to implement better informed by actual application requirements and ffective way to manage yield cost quality tradeoffs in application implementation design flows
modern embedded consumer devices execute complex network and multimedia applications that require high performance and low energy consumption for implementing complex applications on network on chips nocs design methodology is needed for performing exploration at noc system level in order to select the optimal application specific noc architecture serving the application requirements in the best way the design methodology we present in this paper is based on the exploration of different noc characteristics and is supported by flexible noc simulator which provides the essential evaluation metrics in order to select the optimal communication parameters of the noc architectures we illustrated that it is possible with the evaluation metrics provided by the simulator we present to perform exploration of several noc aspects and select the optimal communication characteristics for noc platforms having network and multimedia applications as the target domains with our methodology we can achieve gain of in the energy times delay product on average
in order to manage service to meet the agreed upon sla it is important to design service of the required capacity and to monitor the service thereafter for violations at runtime this objective can be achieved by translating slos specified in the sla into lower level policies that can then be used for design and enforcement purposes such design and operational policies are often constraints on thresholds of lower level metrics in this paper we propose systematic and practical approach that combines fine grained performance modeling with regression analysis to translate service level objectives into design and operational policies for multi tier applications we demonstrate that our approach can handle both request based and session based workloads and deal with workload changes in terms of both request volume and transaction mix we validate our approach using both the rubis commerce benchmark and trace driven simulation of business critical enterprise application these results show the effectiveness of our approach
classical compiler optimizations assume fixed cache architecture and modify the program to take best advantage of it in some cases this may not be the best strategy because each nest might work best with different cache configuration and transforming nest for given fixed cache configuration may not be possible due to data and control dependences working with fixed cache configuration can also increase energy consumption in loops where the best required configuration is smaller than the default fixed one in this paper we take an alternate approach and modify the cache configuration for each nest depending on the access pattern exhibited by the nest we call this technique compiler directed cache polymorphism cdcp more specifically in this paper we make the following contributions first we present an approach for analyzing data reuse properties of loop nests second we give algorithms to simulate the footprints of array references in their reuse space third based on our reuse analysis we present an optimization algorithm to compute the cache configurations for each loop nest our experimental results show that cdcp is very effective in finding the near optimal data cache configurations for different nests in array intensive applications
it is envisaged that the application of the multilevel security mls scheme will enhance flexibility and effectiveness of authorization policies in shared enterprise databases and will replace cumbersome authorization enforcement practices through complicated view definitions on per user basis however as advances in this area are being made and ideas crystallized the concomitant weaknesses of the mls databases are also surfacing we insist that the critical problem with the current model is that the belief at higher security level is cluttered with irrelevant or inconsistent data as no mechanism for attenuation is supported critics also argue that it is imperative for mls database users to theorize about the belief of others perhaps at different security levels an apparatus that is currently missing and the absence of which is seriously felt the impetus for our current research is this need to provide an adequate framework for belief reasoning in mls databases we demonstrate that prudent application of the concept of inheritance in deductive database setting will help capture the notion of declarative belief and belief reasoning in mls databases in an elegant way to this end we develop function to compute belief in multiple modes which can be used to reason about the beliefs of other users we strive to develop poised and practical logical characterization of mls databases for the first time based on the inherently difficult concept of non monotonic inheritance we present an extension of the acclaimed datalog language called the multilog and show that datalog is special case of our language we also suggest an implementation scheme for multilog as front end for coral
in this paper we propose formal analysis approach to estimate the expected average data cache access time of an application across all possible program inputs towards this goal we introduce the notion of probabilistic access history that intuitively summarizes the history of data memory accesses along different program paths to reach particular program point and their associated probabilities an efficient static program analysis technique has been developed to compute the access history at all program points we estimate the cache hit miss probabilities and hence the expected access time of each data memory reference from the access history our experimental evaluation confirms the accuracy and viability of the probabilistic data cache modeling approach
this paper presents new static type system for multithreaded programs well typed programs in our system are guaranteed to be free of data races and deadlocks our type system allows programmers to partition the locks into fixed number of equivalence classes and specify partial order among the equivalence classes the type checker then statically verifies that whenever thread holds more than one lock the thread acquires the locks in the descending orderour system also allows programmers to use recursive tree based data structures to describe the partial order for example programmers can specify that nodes in tree must be locked in the tree order our system allows mutations to the data structure that change the partial order at runtime the type checker statically verifies that the mutations do not introduce cycles in the partial order and that the changing of the partial order does not lead to deadlocks we do not know of any other sound static system for preventing deadlocks that allows changes to the partial order at runtimeour system uses variant of ownership types to prevent data races and deadlocks ownership types provide statically enforceable way of specifying object encapsulation ownership types are useful for preventing data races and deadlocks because the lock that protects an object can also protect its encapsulated objects this paper describes how to use our type system to statically enforce object encapsulation as well as prevent data races and deadlocks the paper also contains detailed discussion of different ownership type systems and the encapsulation guarantees they provide
distributed information brokering system dibs is peer to peer overlay network that comprises diverse data servers and brokering components helping client queries locate the data server many existing information brokering systems adopt server side access control deployment and honest assumptions on brokers however little attention has been drawn on privacy of data and metadata stored and exchanged within dibs in this paper we address privacy preserving information sharing via on demand information access we propose flexible and scalable system using broker coordinator overlay network through an innovative automaton segmentation scheme distributed access control enforcement and query segment encryption our system integrates security enforcement and query forwarding while preserving system wide privacy we present the automaton segmentation approach analyze privacy preservation in details and finally examine the end to end performance and scalability through experiments and analysis
we study the problem of finding in given word all maximal gapped palindromes verifying two types of constraints that we call long armed and length constrained palindromes for each of the two classes we propose an algorithm that runs in time for constant size alphabet where is the number of output palindromes both algorithms can be extended to compute biological gapped palindromes within the same time bound
many of the existing approaches in software comprehension focus on program structure or external documentation however by analyzing formal information the informal semantics contained in the vocabulary of source code are overlooked to understand software as whole we need to enrich software analysis with the developer knowledge hidden in the code naming this paper proposes the use of information retrieval to exploit linguistic information found in source code such as identifier names and comments we introduce semantic clustering technique based on latent semantic indexing and clustering to group source artifacts that use similar vocabulary we call these groups semantic clusters and we interpret them as linguistic topics that reveal the intention of the code we compare the topics to each other identify links between them provide automatically retrieved labels and use visualization to illustrate how they are distributed over the system our approach is language independent as it works at the level of identifier names to validate our approach we applied it on several case studies two of which we present in this paper note some of the visualizations presented make heavy use of colors please obtain color copy of the article for better understanding
existing work on software connectors shows significant disagreement on both their definition and their relationships with components coordinators and adaptors we propose precise characterisation of connectors discuss how they relate to the other three classes and contradict the suggestion that connectors and components are disjoint we discuss the relationship between connectors and coupling and argue the inseparability of connection models from component programming models finally we identify the class of configuration languages show how it relates to primitive connectors and outline relevant areas for future work
the area under the roc receiver operating characteristics curve or simply auc has been traditionally used in medical diagnosis since the it has recently been proposed as an alternative single number measure for evaluating the predictive ability of learning algorithms however no formal arguments were given as to why auc should be preferred over accuracy in this paper we establish formal criteria for comparing two different measures for learning algorithms and we show theoretically and empirically that auc is better measure defined precisely than accuracy we then reevaluate well established claims in machine learning based on accuracy using auc and obtain interesting and surprising new results for example it has been well established and accepted that naive bayes and decision trees are very similar in predictive accuracy we show however that naive bayes is significantly better than decision trees in auc the conclusions drawn in this paper may make significant impact on machine learning and data mining applications
this paper presents search engine architecture retin aiming at retrieving complex categories in large image databases for indexing scheme based on two step quantization process is presented to compute visual codebooks the similarity between images is represented in kernel framework such similarity is combined with online learning strategies motivated by recent machine learning developments such as active learning additionally an offline supervised learning is embedded in the kernel framework offering real opportunity to learn semantic categories experiments with real scenario carried out from the corel photo database demonstrate the efficiency and the relevance of the retin strategy and its outstanding performances in comparison to up to date strategies
automated verification is one of the most successful applications of automated reasoning in computer science in automated verification one uses algorithmic techniques to establish the correctness of the design with respect to given property automated verification is based on small number of key algorithmic ideas tying together graph theory automata theory and logic in this self contained talk will describe how this holy trinity gave rise to automated verification tools and mention some applications to planning
many data warehouse systems have been developed recently yet data warehouse practice is not sufficiently sophisticated for practical usage most data warehouse systems have some limitations in terms of flexibility efficiency and scalability in particular the sizes of these data warehouses are forever growing and becoming overloaded with data scenario that leads to difficulties in data maintenance and data analysis this research focuses on data information integration between data cubes this research might contribute to the resolution of two concerns the problem of redundancy and the problem of data cubes independent information this work presents semantic cube model which extends object oriented technology to data warehouses and which enables users to design the generalization relationship between different cubes in this regard this work's objectives are to improve the performance of query integrity and to reduce data duplication in data warehouse to deal with the handling of increasing data volume in data warehouses we discovered important inter relationships that hold among data cubes that facilitate information integration and that prevent the loss of data semantics
the vision of future electronic marketplaces markets is that of markets being populated by autonomous intelligent entities software trading agents representing their users or owners and conducting business on their behalf for this vision to materialize one fundamental issue that needs to be addressed is that of trust first users need to be able to trust that the agents will do what they say they do second they need to be confident that their privacy is protected and that the security risks involved in entrusting agents to perform transactions on their behalf are minimized finally users need to be assured that any legal issues relating to agents trading electronically are fully covered as they are in traditional trading practices in this paper we consider the barriers for the adoption of agent technology in electronic commerce commerce which pertain to trust security and legal issues we discuss the perceived risks of the use of agents in commerce and the fundamental issue of trust in this context issues regarding security and how some of these can be addressed through the use of cryptography are described the impact of the use of agent technology on the users privacy and how it can be both protected as well as hindered by it is also examined finally we discuss the legal issues that arise in agent mediated commerce and discuss the idea of attributing to software agents the status of legal persons or persons and the various implications
wireless sensor networks have been proposed for multitude of location dependent applications for such systems the cost and limitations of the hardware on sensing nodes prevent the use of range based localization schemes that depend on absolute point to point distance estimates because coarse accuracy is sufficient for most sensor network applications solutions in range free localization are being pursued as cost effective alternative to more expensive range based approaches in this paper we present apit novel localization algorithm that is range free we show that our apit scheme performs best when an irregular radio pattern and random node placement are considered and low communication overhead is desired we compare our work via extensive simulation with three state of the art range free localization schemes to identify the preferable system configurations of each in addition we study the effect of location error on routing and tracking performance we show that routing performance and tracking accuracy are not significantly affected by localization error when the error is less than times the communication radio radius
xml data can be represented by tree or graph structure and xml query processing requires the information of structural relationships among nodes the basic structural relationships are parent child and ancestor descendant and finding all occurrences of these basic structural relationships in an xml data is clearly core operation in xml query processing several node labeling schemes have been suggested to support the determination of ancestor descendant or parent child structural relationships simply by comparing the labels of nodes however the previous node labeling schemes have some disadvantages such as large number of nodes that need to be relabeled in the case of an insertion of xml data huge space requirements for node labels and inefficient processing of structural joins in this paper we propose the nested tree structure that eliminates the disadvantages and takes advantage of the previous node labeling schemes the nested tree structure makes it possible to use the dynamic interval based labeling scheme which supports xml data updates with almost no node relabeling as well as efficient structural join processing experimental results show that our approach is efficient in handling updates with the interval based labeling scheme and also significantly improves the performance of the structural join processing compared with recent methods
component programming techniques encourage abstraction and reuse through external linking some parts of program however must use concrete internally specified references so pure component system is not sufficient mechanism for structuring programs we present the combination of static internally linked module system and purely abstractive component system the latter extends our previous model of typed units to properly account for translucency and sharing we also show how units and modules can express an sml style system of structures and functors and we explore the consequences for recursive structures and functors
in this paper we kernelize conventional clustering algorithms from novel point of view based on the fully mathematical proof we first demonstrate that kernel kmeans kkmeans is equivalent to kernel principal component analysis kpca prior to the conventional kmeans algorithm by using kpca as preprocessing step we also generalize gaussian mixture model gmm to its kernel version the kernel gmm kgmm consequently conventional clustering algorithms can be easily kernelized in the linear feature space instead of nonlinear one to evaluate the newly established kkmeans and kgmm algorithms we utilized them to the problem of semantic object extraction segmentation of color images based on series of experiments carried out on set of color images we indicate that both kkmeans and kgmm can offer more elaborate output than the conventional kmeans and gmm respectively
various index structures have been proposed to speed up the evaluation of xml path expressions however existing xml path indices suffer from at least one of three limitations they focus only on indexing the structure relying on separate index for node content they are useful only for simple path expressions such as root to leaf paths or they cannot be tightly integrated with relational query processor moreover there is no unified framework to compare these index structures in this paper we present framework defining family of index structures that includes most existing xml path indices we also propose two novel index structures in this family with different space time tradeoffs that are effective for the evaluation of xml branching path expressions ie twigs with value conditions we also show how this family of index structures can be implemented using the access methods of the underlying relational database system finally we present an experimental evaluation that shows the performance tradeoff between index space and matching time the experimental results show that our novel indices achieve orders of magnitude improvement in performance for evaluating twig queries albeit at higher space cost over the use of previously proposed xml path indices that can be tightly integrated with relational query processor
this paper addresses performance bottleneck in time series subsequence matching first we analyze the disk access and cpu processing times required during the index searching and post processing steps of subsequence matching through preliminary experiments based on their results we show that the post processing step is main performance bottleneck in subsequence matching in order to resolve the performance bottleneck we propose simple yet quite effective method that processes the post processing step by rearranging the order of candidate subsequences to be compared with query sequence our method completely eliminates the redundancies of disk accesses and cpu processing occurring in the post processing step we show that our method is optimal and also does not incur any false dismissal also we justify the effectiveness of our method by extensive experiments
software maintenance is expensive and difficult because soft ware is complex and maintenance requires the understanding of code written by someone else prerequisite to maintainability is program understanding specifically understanding the control flows between software components this is especially problematic for emerging software technologies such as the world wide web because of the lack of formal development practices and because web applications comprise mix of static and dynamic content adequate representations are therefore necessary to facilitate program understanding this research proposes an approach called readable readable executable augmentable database linked environment that generates executable tabular representations that can be used to both understand and manipulate software applications controlled laboratory experiment carried out to test the efficacy of the approach demonstrates that the representations significantly enhance program understanding the results suggest that the approach and the corresponding environment may be useful to alleviate problems associated with the software maintainability of new web applications
structured texts for example dictionaries and user manuals typically have heirarchical tree like structure we describe query language for retrieving information from collections of hierarchical text the language is based on tree pattern matching notion called tree inclusion tree inclusion allows easy expression of queries that use the structure and the content of the document in using it user need not be aware of the whole structure of the database thus language based on tree inclusion is data independent property made necessary because of the great variance in the structure of the texts
this paper presents an approach to matching parts of deformable shapes multiscale salient parts of the two shapes are first identified then these parts are matched if their immediate properties are similar the same holds recursively for their subparts and the same holds for their neighbor parts the shapes are represented by hierarchical attributed graphs whose node attributes encode the photometric and geometric properties of corresponding parts and edge attributes capture the strength of neighbor and part of interactions between the parts their matching is formulated as finding the subgraph isomorphism that minimizes quadratic cost the dimensionality of the matching space is dramatically reduced by convexifying the cost experimental evaluation on the benchmark mpeg and brown datasets demonstrates that the proposed approach is robust
tree automata completion is technique for the verification of infinite state systems it has already been used for the verification of cryptographic protocols and the prototyping of java static analyzers however as for many other verification techniques the correctness of the associated tool becomes more and more difficult to guarantee it is due to the size of the implementation that constantly grows and due to optimizations which are necessary to scale up the efficiency of the tool to verify real size systems in this paper we define and develop checker for tree automata produced by completion the checker is defined using coq and its implementation is automatically extracted from its formal specification using extraction gives checker that can be run independently of the coq environment specific algorithm for tree automata inclusion checking has been defined so as to avoid the exponential blow up the obtained checker is certified in coq independent of the implementation of completion usable with any approximation performed during completion small and fast some benchmarks are given to show how efficient the tool is
shape skeletons are fundamental concepts for describing the shape of geometric objects and have found variety of applications in number of areas where geometry plays an important role two types of skeletons commonly used in geometric computations are the straight skeleton of linear polygon and the medial axis of bounded set of points in the dimensional euclidean space however exact computation of these skeletons of even fairly simple planar shapes remains an open problem in this paper we propose novel approach to construct exact or approximate continuous distance functions and the associated skeletal representations skeleton and the corresponding radius function for solid semi analytic sets that can be either rigid or undergoing topological deformations our approach relies on computing constructive representations of shapes with functions that operate on real valued halfspaces as logic operations we use our approximate distance functions to define new type of skeleton ie the skeleton which is piecewise linear for polygonal domains generalizes naturally to planar and spatial domains with curved boundaries and has attractive properties we also show that the exact distance functions allow us to compute the medial axis of any closed bounded and regular planar domain importantly our approach can generate the medial axis the straight skeleton and the skeleton of possibly deformable shapes within the same formulation extends naturally to and can be used in variety of applications such as skeleton based shape editing and adaptive motion planning
this paper offers an exploration of the attitudes of older adults to keeping in touch with people who are important to them we present findings from three focus groups with people from to years of age themes emerging from the findings suggest that older adults view the act of keeping in touch as being worthy of time and dedication but also as being something that needs to be carefully managed within the context of daily life communication is seen as means through which skill should be demonstrated and personality expressed and is understood in very different context to the lightweight interaction that is increasingly afforded by new technologies the themes that emerged are used to elicit number of design implications and to promote some illustrative design concepts for new communication devices
software has spent the bounty of moore's law by solving harder problems and exploiting abstractions such as high level languages virtual machine technology binary rewriting and dynamic analysis abstractions make programmers more productive and programs more portable but usually slow them down since moore's law is now delivering multiple cores instead of faster processors future systems must either bear relatively higher cost for abstractions or use some cores to help tolerate abstraction costs this paper presents the design implementation and evaluation of novel concurrent configurable dynamic analysis framework that efficiently utilizes multicore cache architectures it introduces cache friendly asymmetric buffering cab lock free ring buffer that implements efficient communication between application and analysis threads we guide the design and implementation of our framework with model of dynamic analysis overheads the framework implements exhaustive and sampling event processing and is analysis neutral we evaluate the framework with five popular and diverse analyses and show performance improvements even for lightweight low overhead analyses efficient inter core communication is central to high performance parallel systems and we believe the cab design gives insight into the subtleties and difficulties of attaining it for dynamic analysis and other parallel software
recent work has shown that the physical connectivity of the internet exhibits small world behavior characterizing such behavior is important not only for generating realistic internet topology but also for the proper evaluation of large scale content delivery mechanisms along this line this paper tries to understand how small world behavior arises in the internet topologies and how it impacts the performance of multicast techniques first we attribute small world behavior to two possible causes namely the variability of vertex degree and the preference for local connections for vertices we have found that both factors contribute with different relative degrees to the small world behavior of autonomous system as level and router level internet topologies for as level topology we observe that high variability of vertex degree is sufficient to cause small world behavior but for router level topology preference for local connectivity plays more important role second we propose better models to generate small world internet topologies our models incorporate both causes of small world behavior and generate graphs closely resemble real internet graphs third using simulation we demonstrate the importance of our work by studying the scaling behavior of multicast techniques we show that multicast tree size largely depends on network topology if topology generators capture only the variability of vertex degree they are likely to underestimate the benefit of multicast techniques
there is currently an abundance of social network services available on the internet in addition examples of location aware social network services are emerging the use of such services presents interesting consequences for users privacy and behaviour and ultimately the adoption of such services yet not lot of explicit knowledge is available that addresses these issues the work presented here tries to answer this by investigating the willingness to use location aware service in population of students during three week festival the main findings show that most users are willing to use such systems also on larger scale however some reservations particularly with regard to privacy are uncovered
cache memories in embedded systems play an important role in reducing the execution time of the applications various kinds of extensions have been added to cache hardware to enable software involvement in replacement decisions thus improving the run time over purely hardware managed cache novel embedded systems like intel's xscale and arm cortex processors provide the facility of locking one or more lines in cache this feature is called cache locking this paper presents the first method in the literature for instruction cache locking that is able to reduce the average case run time of the program we devise cost benefit model to discover the memory addresses which should be locked in the cache we implement our scheme inside binary rewriter thus widening the applicability of our scheme to binaries compiled using any compiler results obtained on suite of mibench and mediabench benchmarks show up to improvement in the instruction cache miss rate on average and up to improvement in the execution time on average for applications having instruction accesses as bottleneck depending on the cache configuration the improvement in execution time is as high as for some benchmarks
regular path expressions are essential for formulating queries over the semistructured data without specifying the exact structure the query pruning is an important optimization technique to avoid useless traversals in evaluating regular path expressions while the previous query pruning optimizes single regular path expression well it often fails to fully optimize multiple regular path expressions nevertheless multiple regular path expressions are very frequently used in nontrivial queries and so an effective optimization technique for them is required in this paper we present new technique called the two phase query pruning that consists of the preprocessing phase and the pruning phase our two phase query pruning is effective in optimizing multiple regular path expressions and is more scalable and efficient than the combination of the previous query pruning and post processing in that it never deals with exponentially many combinations of sub results produced from all the regular path expressions
web search engines typically provide search results without considering user interests or context we propose personalized search approach that can easily extend conventional search engine on the client side our mapping framework automatically maps set of known user interests onto group of categories in the open directory project odp and takes advantage of manually edited data available in odp for training text classifiers that correspond to and therefore categorize and personalize search results according to user interests in two sets of controlled experiments we compare our personalized categorization system pcat with list interface system list that mimics typical search engine and with nonpersonalized categorization system cat in both experiments we analyze system performances on the basis of the type of task and query length we find that pcat is preferable to list for information gathering types of tasks and for searches with short queries and pcat outperforms cat in both information gathering and finding types of tasks and for searches associated with free form queries from the subjects answers to questionnaire we find that pcat is perceived as system that can find relevant web pages quicker and easier than list and cat
the discovery of biclusters which denote groups of items that show coherent values across subset of all the transactions in data set is an important type of analysis performed on real valued data sets in various domains such as biology several algorithms have been proposed to find different types of biclusters in such data sets however these algorithms are unable to search the space of all possible biclusters exhaustively pattern mining algorithms in association analysis also essentially produce biclusters as their result since the patterns consist of items that are supported by subset of all the transactions however major limitation of the numerous techniques developed in association analysis is that they are only able to analyze data sets with binary and or categorical variables and their application to real valued data sets often involves some lossy transformation such as discretization or binarization of the attributes in this paper we propose novel association analysis framework for exhaustively and efficiently mining range support patterns from such data set on one hand this framework reduces the loss of information incurred by the binarization and discretization based approaches and on the other it enables the exhaustive discovery of coherent biclusters we compared the performance of our framework with two standard biclustering algorithms through the evaluation of the similarity of the cellular functions of the genes constituting the patterns biclusters derived by these algorithms from microarray data these experiments show that the real valued patterns discovered by our framework are better enriched by small biologically interesting functional classes also through specific examples we demonstrate the ability of the rap framework to discover functionally enriched patterns that are not found by the commonly used biclustering algorithm isa the source code and data sets used in this paper as well as the supplementary material are available at http wwwcsumnedu vk gaurav rap
in building large scale video server it is highly desirable to use heterogeneous disk subsystems for the following reasons first existing disks may fail especially in an environment with large number of disks enforcing the use of new disks second for scalable server to cope with the increasing demand of customers new disks may be needed to increase the server apos storage capacity and throughput with rapid advances in the performance of disks the newly added disks generally have higher data transfer rate and larger storage capacity than the disks originally in the system in this paper we propose novel striping scheme termed as resource based striping rbs for video servers built on heterogeneous disks rbs combines the techniques of wide striping and narrow striping so that it can obtain the optimal stripe allocation and efficiently utilize both the bandwidth and storage capacity of all disks rbs is suitable for applications whose files are not updated frequently such as course on demand and movie on demand we examine the performance of rbs via simulation experiments our results show that rbs greatly outperforms the conventional striping schemes proposed for video servers with heterogeneous or homogeneous disks in terms of the number of simultaneous streams supported and the number of files that can be stored
this paper describes the moveme interaction prototype developed in conjunction with vlab in rotterdam moveme proposes scenario for social interaction and the notion of social intimacy interaction with sensory enhanced soft pliable tactile throw able cushions afford new approaches to pleasure movement and play somatics approach to touch and kinaesthesia provides an underlying design framework the technology developed for moveme uses the surface of the cushion as an intelligent tactile interface making use of movement analysis system called laban effort shape we have developed model that provides high level interpretation of varying qualities of touch and motion trajectory we describe the notion of social intimacy and how we model it through techniques in somatics and performance practice we describe the underlying concepts of moveme and its motivations we illustrate the structural layers of interaction and related technical detail finally we discuss the related body of work in the context of evaluating our approach and conclude with plans for future work
this paper proposes set of new software test diversity measures based on control oscillations of test suites oscillation diversity uses conversion inversion and phase transformation to vary test suite amplitudes frequencies and phases resistance and inductance are defined as measures of diversification difficulty the experimental results show correlation between some oscillation diversity measures and fault detection effectiveness
this paper presents techniques and tools to transform spreadsheets into relational databases and back set of data refinement rules is introduced to map tabular datatype into relational database schema having expressed the transformation of the two data models as data refinements we obtain for free the functions that migrate the data we use well known relational database techniques to optimize and query the data because data refinements define bi directional transformations we can map such database back to an optimized spreadsheet we have implemented the data refinement rules and we constructed haskell based tools to manipulate optimize and refactor excel like spreadsheets
we introduce new acceleration to the standard splatting volume rendering algorithm our method achieves full colour bit depth sorted and shaded volume rendering significantly faster than standard splatting the speedup is due to dimensional adjacency data structure that efficiently skips transparent parts of the data and stores only the voxels that are potentially visible our algorithm is robust and flexible allowing for depth sorting of the data including correct back to front ordering for perspective projections this makes interactive splatting possible for applications such as medical visualizations that rely on structure and depth information
in this paper new method to improve the utilization of main memory systems is presented the new method is based on prestoring in main memory number of query answers each evaluated out of single memory page to this end the ideas of page answers and page traces are formally described and their properties analyzed the query model used here allows for selection projection join recursive queries as well as arbitrary combinations we also show how to apply the approach under update traffic this concept is especially useful in managing the main memories of an important class of applications this class includes the evaluation of triggers and alerters performance improvement of rule based systems integrity constraint checking and materialized views these applications are characterized by the existence at compile time of predetermined set of queries by slow but persistent update traffic and by their need to repetitively reevaluate the query set the new approach represents new type of intelligent database caching which contrasts with traditional caching primarily in that the cache elements are derived data and as consequence they overlap arbitrarily and do not have fixed length the contents of the main memory cache are selected based on the data distribution within the database the set of fixed queries to preprocess and the paging characteristics page answers and page traces are used as the smallest indivisible units in the cache an efficient heuristic to select near optimal set of page answers and page traces to populate the main memory has been developed implemented and tested finally quantitative measurements of performance benefits are reported
all frequent itemset mining algorithms rely heavily on the monotonicity principle for pruning this principle allows for excluding candidate itemsets from the expensive counting phase in this paper we present sound and complete deduction rules to derive bounds on the support of an itemset based on these deduction rules we construct condensed representation of all frequent itemsets by removing those itemsets for which the support can be derived resulting in the so called non derivable itemsets ndi representation we also present connections between our proposal and recent other proposals for condensed representations of frequent itemsets experiments on real life datasets show the effectiveness of the ndi representation making the search for frequent non derivable itemsets useful and tractable alternative to mining all frequent itemsets
programming web applications in direct style with the help of continuations is much simpler safer modular and better performing technology than the current dominating page centric technology combining cgi scripts active pages or servlets this paper discusses the use of continuations in the context of web applications the problems they solve as well as some new problems they introduce
in degree pagerank number of visits and other measures of web page popularity significantly influence the ranking of search results by modern search engines the assumption is that popularity is closely correlated with quality more elusive concept that is difficult to measure directly unfortunately the correlation between popularity and quality is very weak for newly created pages that have yet to receive many visits and or in links worse since discovery of new content is largely done by querying search engines and because users usually focus their attention on the top few results newly created but high quality pages are effectively shut out and it can take very long time before they become popularwe propose simple and elegant solution to this problem the introduction of controlled amount of randomness into search result ranking methods doing so offers new pages chance to prove their worth although clearly using too much randomness will degrade result quality and annul any benefits achieved hence there is tradeoff between exploration to estimate the quality of new pages and exploitation of pages already known to be of high quality we study this tradeoff both analytically and via simulation in the context of an economic objective function based on aggregate result quality amortized over time we show that modest amount of randomness leads to improved search results
the ever growing needs of large multimedia systems cannot be met by magnetic disks due to their high cost and low storage density consequently cheaper and denser tertiary storage systems are being integrated into the storage hierarchies of these applications although tertiary storage is cheaper the access latency is very high due to the need to load and unload media on the drives this high latency and the bursty nature of traffic result in the accumulation of requests for tertiary storage we study the problem of scheduling these requests to improve performance in particular we address the issues of scheduling across multiple tapes or disks as opposed to most other studies which consider only one or two media we focus on algorithms that minimize the number of switches and show through simulation that these result in near optimal schedules for single drive libraries an efficient algorithm that produces optimal schedules is developed for multiple drives the problem is shown to be np complete efficient and effective heuristics are presented for both single and multiple drives the scheduling policies developed achieve significant performance gains over naive policies the algorithms are simple to implement and are not restrictive the study encompasses all types of storage libraries handling removable media such as tapes and optical disks
botnets are networks of compromised computers infected with malicious code that can be controlled remotely under common command and control channel recognized as one the most serious security threats on current internet infrastructure advanced botnets are hidden not only in existing well known network applications eg irc http or peer to peer but also in some unknown or novel creative applications which makes the botnet detection challenging problem most current attempts for detecting botnets are to examine traffic content for bot signatures on selected network links or by setting up honeypots in this paper we propose new hierarchical framework to automatically discover botnets on large scale wifi isp network in which we first classify the network traffic into different application communities by using payload signatures and novel cross association clustering algorithm and then on each obtained application community we analyze the temporal frequent characteristics of flows that lead to the differentiation of malicious channels created by bots from normal traffic generated by human beings we evaluate our approach with about million flows collected over three consecutive days on large scale wifi isp network and results show the proposed approach successfully detects two types of botnet application flows ie blackenergy http bot and kaiten irc bot from about million flows with high detection rate and an acceptable low false alarm rate
in this paper we present multimodal approach for the recognition of eight emotions our approach integrates information from facial expressions body movement and gestures and speech we trained and tested model with bayesian classifier using multimodal corpus with eight emotions and ten subjects firstly individual classifiers were trained for each modality next data were fused at the feature level and the decision level fusing the multimodal data resulted in large increase in the recognition rates in comparison with the unimodal systems the multimodal approach gave an improvement of more than when compared to the most successful unimodal system further the fusion performed at the feature level provided better results than the one performed at the decision level
this study investigates level set multiphase image segmentation by kernel mapping and piecewise constant modeling of the image data thereof kernel function maps implicitly the original data into data of higher dimension so that the piecewise constant model becomes applicable this leads to flexible and effective alternative to complex modeling of the image data the method uses an active curve objective functional with two terms an original term which evaluates the deviation of the mapped image data within each segmentation region from the piecewise constant model and classic length regularization term for smooth region boundaries functional minimization is carried out by iterations of two consecutive steps minimization with respect to the segmentation by curve evolution via euler lagrange descent equations and minimization with respect to the regions parameters via fixed point iterations using common kernel function this step amounts to mean shift parameter update we verified the effectiveness of the method by quantitative and comparative performance evaluation over large number of experiments on synthetic images as well as experiments with variety of real images such as medical satellite and natural images as well as motion maps
in main memory databases the number of processor cache misses has critical impact on the performance of the system cache conscious indices are designed to improve performance by reducing the number of processor cache misses that are incurred during search operation conventional wisdom suggests that the index's node size should be equal to the cache line size in order to minimize the number of cache misses and improve performance as we show in this paper this design choice ignores additional effects such as the number of instructions executed and the number of tlb misses which play significant role in determining the overall performance to capture the impact of node size on the performance of cache conscious tree csb tree we first develop an analytical model based on the fundamental components of the search process this model is then validated with an actual implementation demonstrating that the model is accurate both the analytical model and experiments confirm that using node sizes much larger than the cache line size can result in better search performance for the csb tree
adaptive user interface composition is the ability of software system to compose its user interface at runtime according to given deployment profile and to possibly drop running components and activate better alternatives in their place in response to deployment profile modifications while adaptive behavior has gained interest for wide range of software products and services its support is very demanding requiring adoption of user interface architectural patterns from the early software design stages while previous research addressed the issue of engineering adaptive systems from scratch there is an important methodological gap since we lack processes to reform existing non adaptive systems towards adaptive behavior we present stepwise transformation process of user interface software by incrementally upgrading relevant class structures towards adaptive composition by treating adaptive behavior as cross cutting concern all our refactoring examples have emerged from real practice
with the recent dramatic increase in electronic access to documents text categorization mdash the task of assigning topics to given document mdash has moved to the center of the information sciences and knowledge management this article uses the structure that is present in the semantic space of topics in order to improve performance in text categorization according to their meaning topics can be grouped together into ldquo meta topics rdquo eg gold silver and copper are all metals the proposed architecture matches the hierarchical structure of the topic space as opposed to flat model that ignores the structure it accommodates both single and multiple topic assignments for each document its probabilistic interpretation allows its predictions to be combined in principled way with information from other sources the first level of the architecture predicts the probabilities of the meta topic groups this allows the individual models for each topic on the second level to focus on finer discriminations within the group evaluating the performance of two level implementation on the reuters testbed of newswire articles shows the most significant improvement for rare classes
unanticipated connection of independently developed componentsis one of the key issues in component oriented programming while variety of component oriented languages have been proposed none of them has achieved breakthrough yet in this paper we present scl simple language dedicated to component oriented programming scl integrates well known features such as component class component interface port or service all these well known features are presented discussed and compared to existing approaches because they vary quite widely from one language to another but these features are not enough to build component language indeed most approaches use language primitives and shared interfaces to connect components but shared interfaces are in contradiction with the philosophy of independently developed components to this issue scl provides new features such as uniform component composition model based on connectors connectors represent interactions between independently developed components scl also integrates component properties which enable connections based on component state changes with no requirements of specific code in components
we provide type system inspired by affine intuitionistic logic for the calculus of higher order mobile embedded resources homer resulting in the first process calculus combining affine linear non copyable and non linear copyable higher order mobile processes nested locations and local names the type system guarantees that linear resources are neither copied nor embedded in non linear resources during computation we exemplify the use of the calculus by modelling simplistic cash smart card system the security of which depends on the interplay between linear mobile hardware embedded non linear mobile processes and local names purely linear calculus would not be able to express that embedded software processes may be copied conversely purely non linear calculus would not be able to express that mobile hardware processes cannot be copied
cryptographic operations are essential for many security critical systems reasoning about information flow in such systems is challenging because typical noninterference based information flow definitions allow no flow from secret to public data unfortunately this implies that programs with encryption are ruled out because encrypted output depends on secret inputs the plaintext and the key however it is desirable to allow flows arising from encryption with secret keys provided that the underlying cryptographic algorithm is strong enough in this article we conservatively extend the noninterference definition to allow safe encryption decryption and key generation to illustrate the usefulness of this approach we propose and implement type system that guarantees noninterference for small imperative language with primitive cryptographic operations the type system prevents dangerous program behavior eg giving away secret key or confusing keys and nonkeys which we exemplify with secure implementations of cryptographic protocols because the model is based on standard noninterference property it allows us to develop some natural extensions in particular we consider public key cryptography and integrity which accommodate reasoning about primitives that are vulnerable to chosen ciphertext attacks
with the multiplication of xml data sources many xml data warehouse models have been proposed to handle data heterogeneity and complexity in way relational data warehouses fail to achieve however xml native database systems currently suffer from limited performances both in terms of manageable data volume and response time fragmentation helps address both these issues derived horizontal fragmentation is typically used in relational data warehouses and can definitely be adapted to the xml context however the number of fragments produced by classical algorithms is difficult to control in this paper we propose the use of means based fragmentation approach that allows to master the number of fragments through its parameter we experimentally compare its efficiency to classical derived horizontal fragmentation algorithms adapted to xml data warehouses and show its superiority
with over us science and mathematics education standards and rapid proliferation of web enabled curriculum retrieving curriculum that aligns with the standards to which teachers must teach is key objective for educational digital libraries however previous studies of such alignment use single dimensional and binary measures of the alignment concept as consequence they suffer from low inter rater reliability irr with experts agreeing about alignments only some of the time we present the results of an experiment in which the alignment variable was operationalized using the saracevic model of relevance clues taken from the everyday practice of teaching results show high irr across all clues with irr on several specific alignment dimensions significantly higher than on overall alignment in addition model of overall alignment is derived and estimated the structure and explanatory power of the model as well as the relationships between alignment clues differ significantly between alignments of curriculum found by users themselves and curriculum found by others these results illustrate the usefulness of clue based relevance measures for information retrieval and have important consequences for both the formulation of automated retrieval mechanisms and the construction of gold standard or benchmark set of standard curriculum alignments
open distributed systems are becoming increasingly popular such systems include components that may be obtained from number of different sources for example java allows run time loading of software components residing on remote machines one unfortunate side effect of this openness is the possibility that hostile software components may compromise the security of both the program and the system on which it runs java offers built in security mechanism using which programmers can give permissions to distributed components and check these permissions at run time this security model is flexible but using it is not straightforward which may lead to insufficiently tight permission checking and therefore breaches of securityin this paper we propose data flow algorithm for automated analysis of the flow of permissions in java programs our algorithm produces for given instruction in the program set of permissions that are checked on all possible executions up to this instruction this information can be used in program understanding tools or directly for checking properties that assert what permissions must always be checked before access to certain functionality is allowed the worst case complexity of our algorithm is low order polynomial in the number of program statements and permission types while comparable previous approaches have exponential costs
in this paper we present microsearch search system suitable for small devices used in ubiquitous computing environments akin to desktop search engine microsearch indexes the information inside small device and accurately resolves user queries given the very limited hardware resources conventional search engine designs and algorithms cannot be used we adopt information retrieval techniques for query resolution and propose space efficient algorithm to perform top query on limited hardware resources finally we present theoretical model of microsearch to better understand the tradeoffs in system design parameters by implementing microsearch on actual hardware for evaluation we demonstrate the feasibility of scaling down information retrieval systems onto very small devices
data cube construction has been the focus of much research due to its importance in improving efficiency of olap significant fraction of this work has been on rolap techniques which are based on relational technology existing rolap cubing solutions mainly focus on flat datasets which do not include hierarchies in their dimensions nevertheless the nature of hierarchies introduces several complications into cube construction making existing techniques essentially inapplicable in significant number of real world applications in particular hierarchies raise three main challenges the number of nodes in cube lattice increases dramatically and its shape is more involved these require new forms of lattice traversal for efficient execution the number of unique values in the higher levels of dimension hierarchy may be very small hence partitioning data into fragments that fit in memory and include all entries of particular value may often be impossible this requires new partitioning schemes the number of tuples that need to be materialized in the final cube increases dramatically this requires new storage schemes that remove all forms of redundancy for efficient space utilization in this paper we propose cure novel rolap cubing method that addresses these issues and constructs complete data cubes over very large datasets with arbitrary hierarchies cure contributes novel lattice traversal scheme an optimized partitioning method and suite of relational storage schemes for all forms of redundancy we demonstrate the effectiveness of cure through experiments on both real world and synthetic datasets among the experimental results we distinguish those that have made cure the first rolap technique to complete the construction of the cube of the highest density dataset in the apb benchmark gb cure was in fact quite efficient on this showing great promise with respect to the potential of the technique overall
in this article we present the integration of shape knowledge into variational model for level set based image segmentation and contour based pose tracking given the surface model of an object that is visible in the image of one or multiple cameras calibrated to the same world coordinate system the object contour extracted by the segmentation method is applied to estimate the pose parameters of the object vice versa the surface model projected to the image plane helps in top down manner to improve the extraction of the contour while common alternative segmentation approaches which integrate shape knowledge face the problem that an object can look very differently from various viewpoints free form model ensures that for each view the model can fit the data in the image very well moreover one additionally solves the problem of determining the object's pose in space the performance is demonstrated by numerous experiments with monocular and stereo camera system
we introduce novel representation for random access rendering of antialiased vector graphics on the gpu along with efficient encoding and rendering algorithms the representation supports broad class of vector primitives including multiple layers of semitransparent filled and stroked shapes with quadratic outlines and color gradients our approach is to create coarse lattice in which each cell contains variable length encoding of the graphics primitives it overlaps these cell specialized encodings are interpreted at runtime within pixel shader advantages include localized memory access and the ability to map vector graphics onto arbitrary surfaces or under arbitrary deformations most importantly we perform both prefiltering and supersampling within single pixel shader invocation achieving inter primitive antialiasing at no added memory bandwidth cost we present an efficient encoding algorithm and demonstrate high quality real time rendering of complex real world examples
transaction management on mobile database systems mds has to cope with number of constraints such as limited bandwidth low processing power unreliable communication and mobility etc as result of these constraints traditional concurrency control mechanisms are unable to manage transactional activities to maintain availability innovative transaction execution schemes and concurrency control mechanisms are therefore required to exploit the full potential of mds in this paper we report our investigation on multi versions transaction processing approach and deadlock free concurrency control mechanism based on multiversion two phase locking scheme integrated with timestamp approach we study the behavior of the proposed model with simulation study in mds environment we have compared our schemes using reference model to argue that such performance comparison helps to show the superiority of our model over others experimental results demonstrate that our model provide significantly higher throughput by improving degree of concurrency by reducing transaction wait time and by minimizing restarts and aborts
the cade atp system competition casc is an annual evaluation of fully automatic first order automated theorem proving atp systems casc was the thirteenth competition in the casc series twenty six atp systems and system variants competed in the various competition and demonstration divisions an outline of the competition design and commentated summary of the results are presented
this paper presents theoretical approach that has been developed to capture the computational intensity and computing resource requirements of geographical data and analysis methods these requirements are then transformed into common framework grid based representation of spatial computational domain which supports the efficient use of emerging cyberinfrastructure environments two key types of transformational functions data centric and operation centric are identified and their relationships are explained the application of the approach is illustrated using two geographical analysis methods inverse distance weighted interpolation and the spatial statistic we describe the underpinnings of these two methods present their conventional sequential algorithms and then address their latent parallelism based on spatial computational domain representation through the application of this theoretical approach the development of domain decomposition methods is decoupled from specific high performance computer architectures and task scheduling implementations which makes the design of generic parallel processing solutions feasible for geographical analyses
we present and evaluate simple yet efficient optimization technique that improves memory hierarchy performance for pointer centric applications by up to and reduces cache misses by up to this is achieved by selecting an improved ordering for the data members of pointer based data structures our optimization is applicable to all type safe programming languages that completely abstract from physical storage layout examples of such languages are java and oberon our technique does not involve programmers in the optimization process but runs fully automatically guided by dynamic profiling information that captures which paths through the program are taken with that frequencey the algorithm first strives to cluster data members that are accessed closely after one another onto the same cache line increasing spatial locality then the data members that have been mapped to particular cache line are ordered to minimize load latency in case of cache miss
we propose method for discovering the dependency relationships between the topics of documents shared in social networks using the latent social interactions attempting to answer the question given seemingly new topic from where does this topic evolve in particular we seek to discover the pair wise probabilistic dependency in topics of documents which associate social actors from latent social network where these documents are being shared by viewing the evolution of topics as markov chain we estimate markov transition matrix of topics by leveraging social interactions and topic semantics metastable states in markov chain are applied to the clustering of topics applied to the citeseer dataset collection of documents in academia we show the trends of research topics how research topics are related and which are stable we also show how certain social actors authors impact these topics and propose new ways for evaluating author impact
munin is distributed shared memory dsm system that allows shared memory parallel programs to be executed efficiently on distributed memory multiprocessors munin is unique among existing dsm systems in its use of multiple consistency protocols and in its use of release consistency in munin shared program variables are annotated with their expected access pattern and these annotations are then used by the runtime system to choose consistency protocol best suited to that access pattern release consistency allows munin to mask network latency and reduce the number of messages required to keep memory consistent munin's multiprotocol release consistency is implemented in software using delayed update queue that buffers and merges pending outgoing writes sixteen processor prototype of munin is currently operational we evaluate its implementation and describe the execution of two munin programs that achieve performance within ten percent of message passing implementations of the same programs munin achieves this level of performance with only minor annotations to the shared memory programs
this paper presents generative model for textures that uses local sparse description of the image content this model enforces the sparsity of the expansion of local texture patches on adapted atomic elements the analysis of given texture within this framework performs the sparse coding of all the patches of the texture into the dictionary of atoms conversely the synthesis of new texture is performed by solving an optimization problem that seeks for texture whose patches are sparse in the dictionary this paper explores several strategies to choose this dictionary set of hand crafted dictionaries composed of edges oscillations lines or crossings elements allows to synthesize synthetic images with geometric features another option is to define the dictionary as the set of all the patches of an input exemplar this leads to computer graphics methods for synthesis and shares some similarities with non local means filtering the last method we explore learns the dictionary by an optimization process that maximizes the sparsity of set of exemplar patches applications of all these methods to texture synthesis inpainting and classification shows the efficiency of the proposed texture model
optical flow computation is well known technique and there are important fields in which the application of this visual modality commands high interest nevertheless most real world applications require real time processing an issue which has only recently been addressed most real time systems described to date use basic models which limit their applicability to generic tasks especially when fast motion is presented or when subpixel motion resolution is required therefore instead of implementing complex optical flow approach we describe here very high frame rate optical flow processing system recent advances in image sensor technology make it possible nowadays to use high frame rate sensors to properly sample fast motion ie as low motion scene which makes gradient based approach one of the best options in terms of accuracy and consumption of resources for any real time implementation taking advantage of the regular data flow of this kind of algorithm our approach implements novel superpipelined fully parallelized architecture for optical flow processing the system is fully working and is organized into more than pipeline stages which achieve data throughput of one pixel per clock cycle this computing scheme is well suited to fpga technology and vlsi implementation the developed customized dsp architecture is capable of processing up to frames per second at resolution of pixels we discuss the advantages of high frame rate processing and justify the optical flow model chosen for the implementation we analyze this architecture measure the system resource requirements using fpga devices and finally evaluate the system's performance and compare it with other approaches described in the literature
hardware systems and reactive software systems can be described as the composition of several concurrently active processes automated reasoning based on model checking algorithms can substantially increase confidence in the overall reliability of system direct methods for model checking concurrent composition however usually suffer from the explosion in the number of program states that arises from concurrency reasoning compositionally about individual processes helps mitigate this problem number of rules have been proposed for compositional reasoning typically based on an assume guarantee reasoning paradigm reasoning with these rules can be delicate as some are syntactically circular in nature in that assumptions and guarantees are mutually dependent this is known to be source of unsoundness in this article we investigate rules for compositional reasoning from the viewpoint of completeness we show that several rules are incomplete that is there are properties whose validity cannot be established using only these rules we derive new circular reasoning rule and show it to be sound and complete we show that the auxiliary assertions needed for completeness need be defined only on the interface of the component processes we also show that the two main paradigms of circular and noncircular reasoning are closely related in that proof of one type can be transformed in straightforward manner to one of the other type these results give some insight into the applicability of compositional reasoning methods
the development of new techniques and the emergence of new high throughput tools have led to new information revolution the amount and the diversity of the information that need to be stored and processed have led to the adoption of data integration systems in order to deal with information extraction from disparate sources the mediation between traditional databases and ontologies has been recognized as cornerstone issue in bringing in legacy data with formal semantic meaning however our knowledge evolves due to the rapid scientific development so ontologies and schemata need to change in order to capture and accommodate such an evolution when ontologies change these changes should somehow be rendered and used by the pre existing data integration systems problem that most of the integration systems seem to ignore in this paper we review existing approaches for ontology schema evolution and examine their applicability in state of the art ontology based data integration setting then we show that changes in schemata differ significantly from changes in ontologies this strengthens our position that current state of the art systems are not adequate for ontology based data integration so we give the requirements for an ideal data integration system that will enable and exploit ontology evolution
abstract recurrent neural networks readily process recognize and generate temporal sequences by encoding grammatical strings as temporal sequences recurrent neural networks can be trained to behave like deterministic sequential finite state automata algorithms have been developed for extracting grammatical rules from trained networks using simple method for inserting prior knowledge or rules into recurrent neural networks we show that recurrent neural networks are able to perform rule revision rule revision is performed by comparing the inserted rules with the rules in the finite state automata extracted from trained networks the results from training recurrent neural network to recognize known non trivial randomly generated regular grammar show that not only do the networks preserve correct rules but that they are able to correct through training inserted rules which were initially incorrect by incorrect we mean that the rules were not the ones in the randomly generated grammar
recently mining from data streams has become an important and challenging task for many real world applications such as credit card fraud protection and sensor networking one popular solution is to separate stream data into chunks learn base classifier from each chunk and then integrate all base classifiers for effective classification in this paper we propose new dynamic classifier selection dcs mechanism to integrate base classifiers for effective mining from data streams the proposed algorithm dynamically selects single best classifier to classify each test instance at run time our scheme uses statistical information from attribute values and uses each attribute to partition the evaluation set into disjoint subsets followed by procedure that evaluates the classification accuracy of each base classifier on these subsets given test instance its attribute values determine the subsets that the similar instances in the evaluation set have constructed and the classifier with the highest classification accuracy on those subsets is selected to classify the test instance experimental results and comparative studies demonstrate the efficiency and efficacy of our method such dcs scheme appears to be promising in mining data streams with dramatic concept drifting or with significant amount of noise where the base classifiers are likely conflictive or have low confidence
in this work we address the issue of efficient processing of range queries in dht based pp data networks the novelty of the proposed approach lies on architectures algorithms and mechanisms for identifying and appropriately exploiting powerful nodes in such networks the existence of such nodes has been well documented in the literature and plays key role in the architecture of most successful real world pp applications however till now this heterogeneity has not been taken into account when architecting solutions for complex query processing especially in dht networks with this work we attempt to fill this gap for optimizing the processing of range queries significant performance improvements are achieved due to ensuring much smaller hop count performance for range queries and ii avoiding the dangers and inefficiencies of relying for range query processing on weak nodes with respect to processing storage and communication capacities and with intermittent connectivity we present detailed experimental results validating our performance claims
we present the auckland layout model alm constraint based technique for specifying layout as it is used for arranging the controls in graphical user interface gui most gui frameworks offer layout managers that are basically adjustable tables often adjacent table cells can be merged in the alm the focus switches from the table cells to vertical and horizontal tabulators between the cells on the lowest level of abstraction the model applies linear constraints and an optimal layout is calculated using linear programming however bare linear programming makes layout specification cumbersome and unintuitive especially for gui domain experts who are often not used to such mathematical formalisms in order to improve the usability of the model alm offers several other layers of abstraction that make it possible to define common gui layout more easily in the domain of user interfaces it is important that specifications are not over constrained therefore alm introduces soft constraints which are automatically translated to appropriate hard linear constraints and terms in the objective function guis are usually composed of rectangular areas containing controls therefore alm offers an abstraction for such areas dynamic resizing behavior is very important for guis hence areas have domain specific parameters specifying their minimum maximum and preferred sizes from such definitions hard and soft constraints are automatically derived third level of abstraction allows designers to arrange guis in tabular fashion using abstractions for columns and rows which offer additional parameters for ordering and alignment row and column definitions are used to automatically generate definitions from lower levels of abstraction such as hard and soft constraints and areas specifications from all levels of abstraction can be consistently combined offering gui developers rich set of tools that is much closer to their needs than pure linear constraints incremental computation of solutions makes constraint solving fast enough for near real time use
in the area of image retrieval from data bases and for copyright protection of large image collections there is growing demand for unique but easily computable fingerprints for images these fingerprints can be used to quickly identify every image within larger set of possibly similar images this paper introduces novel method to automatically obtain such fingerprints from an image it is based on reinterpretation of an image as riemannian manifold this representation is feasible for gray value images and color images we discuss the use of the spectrum of eigenvalues of different variants of the laplace operator as fingerprint and show the usability of this approach in several use cases contrary to existing works in this area we do not only use the discrete laplacian but also with particular emphasis the underlying continuous operator this allows better results in comparing the resulting spectra and deeper insights in the problems arising we show how the well known discrete laplacian is related to the continuous laplace beltrami operator furthermore we introduce the new concept of solid height functions to overcome some potential limitations of the method
liveness analysis is an important analysis in optimizing compilers liveness information is used in several optimizations and is mandatory during the code generation phase two drawbacks of conventional liveness analyses are that their computations are fairly expensive and their results are easily invalidated by program transformations we present method to check liveness of variables that overcomes both obstacles the major advantage of the proposed method is that the analysis result survives all program transformations except for changes in the control flow graph for common program sizes our technique is faster and consumes less memory than conventional data flow approaches thereby we heavily make use of ssa form properties which allow us to completely circumvent data flow equation solving we evaluate the competitiveness of our approach in an industrial strength compiler our measurements use the integer part of the spec benchmarks and investigate the liveness analysis used by the ssa destruction pass we compare the net time spent in liveness computations of our implementation against the one provided by that compiler the results show that in the vast majority of cases our algorithm while providing the same quality of information needs less time an average speed up of
we have developed self healing key distribution scheme for secure multicast group communications for wireless sensor network environment we present strategy for securely distributing rekeying messages and specify techniques for joining and leaving group access control in multicast system is usually achieved by encrypting the content using an encryption key known as the group key session key that is only known by the group controller and all legitimate group members in our scheme all rekeying messages except for unicast of an individual key are transmitted without any encryption using one way hash function and xor operation in our proposed scheme nodes are capable of recovering lost session keys on their own without requesting additional transmission from the group controller the proposed scheme provides both backward and forward secrecy we analyze the proposed scheme to verify that it satisfies the security and performance requirements for secure group communication
name ambiguity problem has raised an urgent demand for efficient high quality named entity disambiguation methods the key problem of named entity disambiguation is to measure the similarity between occurrences of names the traditional methods measure the similarity using the bag of words bow model the bow however ignores all the semantic relations such as social relatedness between named entities associative relatedness between concepts polysemy and synonymy between key terms so the bow cannot reflect the actual similarity some research has investigated social networks as background knowledge for disambiguation social networks however can only capture the social relatedness between named entities and often suffer the limited coverage problem to overcome the previous methods deficiencies this paper proposes to use wikipedia as the background knowledge for disambiguation which surpasses other knowledge bases by the coverage of concepts rich semantic information and up to date content by leveraging wikipedia's semantic knowledge like social relatedness between named entities and associative relatedness between concepts we can measure the similarity between occurrences of names more accurately in particular we construct large scale semantic network from wikipedia in order that the semantic knowledge can be used efficiently and effectively based on the constructed semantic network novel similarity measure is proposed to leverage wikipedia semantic knowledge for disambiguation the proposed method has been tested on the standard weps data sets empirical results show that the disambiguation performance of our method gets improvement over the traditional bow based methods and improvement over the traditional social network based methods
physically motivated method for surface reconstruction is proposed that can recover smooth surfaces from noisy and sparse data sets no orientation information is required by new technique based on regularized membrane potentials the input sample points are aggregated leading to improved noise tolerability and outlier removal without sacrificing much with respect to detail feature recovery after aggregating the sample points on volumetric grid novel iterative algorithm is used to classify grid points as exterior or interior to the surface this algorithm relies on intrinsic properties of the smooth scalar field on the grid which emerges after the aggregation step second mesh smoothing paradigm based on mass spring system is introduced by enhancing this system with bending energy minimizing term we ensure that the final triangulated surface is smoother than piecewise linear in terms of speed and flexibility the method compares favorably with respect to previous approaches most parts of the method are implemented on modern graphics processing units gpus results in wide variety of settings are presented ranging from surface reconstruction on noise free point clouds to grayscale image segmentation
we present memory management scheme for java based on thread local heaps assuming most objects are created and used by single thread it is desirable to free the memory manager from redundant synchronization for thread local objects therefore in our scheme each thread receives partition of the heap in which it allocates its objects and in which it does local garbage collection without synchronization with other threads we dynamically monitor to determine which objects are local and which are global furthermore we suggest using profiling to identify allocation sites that almost exclusively allocate global objects and allocate objects at these sites directly in global areawe have implemented the thread local heap memory manager and preliminary mechanism for direct global allocation on an ibm prototype of jdk for windows our measurements of thread local heaps with direct global allocation on way multiprocessor ibm netfinity server show that the overall garbage collection times have been substantially reduced and that most long pauses have been eliminated
the recent proliferation of location based services lbss has necessitated the development of effective indoor positioning solutions in such context wireless local area network wlan positioning is particularly viable solution in terms of hardware and installation costs due to the ubiquity of wlan infrastructures this paper examines three aspects of the problem of indoor wlan positioning using received signal strength rss first we show that due to the variability of rss features over space spatially localized positioning method leads to improved positioning results second we explore the problem of access point ap selection for positioning and demonstrate the need for further research in this area third we present kernelized distance calculation algorithm for comparing rss observations to rss training records experimental results indicate that the proposed system leads to percent improvement over the widely used nearest neighbor and histogram based methods
grids involve coordinated resource sharing and problem solving in heterogeneous dynamic environments to meet the needs of generation of researchers requiring large amounts of bandwidth and more powerful computational resources the lack of resource ownership by grid schedulers and fluctuations in resource availability require mechanisms which will enable grids to adjust themselves to cope with fluctuations the lack of central controller implies need for self adaptation grids must thus be enabled with the ability to discover monitor and manage the use of resources so they can operate autonomously two different approaches have been conceived to match the resource demands of grid applications to resource availability dynamic scheduling and adaptive scheduling however these two approaches fail to address at least one of three important issues the production of feasible schedules in reasonable amount of time in relation to that required for the execution of an application ii the impact of network link availability on the execution time of an application and iii the necessity of migrating codes to decrease the execution time of an application to overcome these challenges this paper proposes procedure for enabling grid applications composed of various dependent tasks to deal with the availability of hosts and links bandwidth this procedure involves task scheduling resource monitoring and task migration with the goal of decreasing the execution time of grid applications the procedure differs from other approaches in the literature because it constantly considers changes in resource availability especially network bandwidth availability to trigger task migration the proposed procedure is illustrated via simulation using various scenarios involving fluctuation of resource availability an additional contribution of this paper is the introduction of set of schedulers offering solutions which differ in terms of both schedule length and computational complexity the distinguishing aspect of this set of schedulers is the consideration of time requirements in the production of feasible schedules performance is then evaluated considering various network topologies and task dependencies
this paper proposes weighted power series model for face verification scores fusion essentially linear parametric power series model is adopted to directly minimize an approximated total error rate for fusion of multi modal face verification scores unlike the conventional least squares error minimization approach which involves fitting of learning model to data density and then perform threshold process for error counting this work directly formulates the required target error count rate in terms of design model parameters with closed form solution the solution is found to belong to specific setting of the weighted least squares our experiments on fusing scores from visual and infra red face images as well as on public data sets show promising results
the increasing amount of communication between individuals in formats eg email instant messaging and the web has motivated computational research in social network analysis sna previous work in sna has emphasized the social network sn topology measured by communication frequencies while ignoring the semantic information in sns in this paper we propose two generative bayesian models for semantic community discovery in sns combining probabilistic modeling with community detection in sns to simulate the generative models an enf gibbs sampling algorithm is proposed to address the efficiency and performance problems of traditional methods experimental studies on enron email corpus show that our approach successfully detects the communities of individuals and in addition provides semantic topic descriptions of these communities
how many pages are there on the web more less big bets on clusters in the clouds could be wiped out if small cache of few million urls could capture much of the value language modeling techniques are applied to msn's search logs to estimate entropy the perplexity is surprisingly small millions not billions entropy is powerful tool for sizing challenges and opportunities how hard is search how hard are query suggestion mechanisms like auto complete how much does personalization help all these difficult questions can be answered by estimation of entropy from search logs what is the potential opportunity for personalization in this paper we propose new way to personalize search personalization with backoff if we have relevant data for particular user we should use it but if we don't back off to larger and larger classes of similar users as proof of concept we use the first few bytes of the ip address to define classes the coefficients of each backoff class are estimated with an em algorithm ideally classes would be defined by market segments demographics and surrogate variables such as time and geography
we address the problem of finding best deterministic query answer to query over probabilistic database for this purpose we propose the notion of consensus world or consensus answer which is deterministic world answer that minimizes the expected distance to the possible worlds answers this problem can be seen as generalization of the well studied inconsistent information aggregation problems eg rank aggregation to probabilistic databases we consider this problem for various types of queries including spj queries top ranking queries group by aggregate queries and clustering for different distance metrics we obtain polynomial time optimal or approximation algorithms for computing the consensus answers or prove np hardness most of our results are for general probabilistic database model called and xor tree model which significantly generalizes previous probabilistic database models like tuples and block independent disjoint models and is of independent interest
this paper presents novel architectural solution to address the problem of scalable routing in very large sensor networks the control complexities of the existing sensor routing protocols both flat and with traditional hierarchy do not scale very well for large networks with potentially hundreds of thousands of embedded sensor devices this paper develops novel routing solution off network control processing oncp that achieves control scalability in large sensor networks by shifting certain amount of routing functions off network this routing approach consisting of coarse grain global routing and distributed fine grain local routing is proposed for achieving scalability by avoiding network wide control message dissemination we present the oncp architectural concepts and analytically characterize its performance in relation to both flat and traditional hierarchical sensor routing architectures we also present ns based experimental results which indicate that for very large networks the packet drop latency and energy performance of oncp can be significantly better than those for flat sensor routing protocols such as directed diffusion and cluster based traditional hierarchical protocols such as cbrp
constrained clustering has recently become an active research topic this type of clustering methods takes advantage of partial knowledge in the form of pairwise constraints and acquires significant improvement beyond the traditional unsupervised clustering however most of the existing constrained clustering methods use constraints which are selected at random recently active constrained clustering algorithms utilizing active constraints have proved themselves to be more effective and efficient in this paper we propose an improved algorithm which introduces multiple representatives into constrained clustering to make further use of the active constraints experiments on several benchmark data sets and public image data sets demonstrate the advantages of our algorithm over the referenced competitors
this paper analyzes memory access scheduling and virtual channels as mechanisms to reduce the latency of main memory accesses by the cpu and peripherals in web servers despite the address filtering effects of the cpu's cache hierarchy there is significant locality and bank parallelism in the dram access stream of web server which includes traffic from the operating system application and peripherals however sequential memory controller leaves much of this locality and parallelism unexploited as serialization and bank conflicts affect the realizable latency aggressive scheduling within the memory controller to exploit the available parallelism and locality can reduce the average read latency of the sdram however bank conflicts and the limited ability of the sdram's internal row buffers to act as cache hinder further latency reduction virtual channel sdramovercomes these limitations by providing set of channel buffers that can hold segments from rows of any internal sdram bank this paper presents memory controller policies that can make effective use of these channel buffers to further reduce the average read latency of the sdram
locating faults in program can be very time consuming and arduous and therefore there is an increased demand for automated techniques that can assist in the fault localization process in this paper code coverage based method with family of heuristics is proposed in order to prioritize suspicious code according to its likelihood of containing program bugs highly suspicious code ie code that is more likely to contain bug should be examined before code that is relatively less suspicious and in this manner programmers can identify and repair faulty code more efficiently and effectively we also address two important issues first how can each additional failed test case aid in locating program faults and second how can each additional successful test case help in locating program faults we propose that with respect to piece of code the contribution of the first failed test case that executes it in computing its likelihood of containing bug is larger than or equal to that of the second failed test case that executes it which in turn is larger than or equal to that of the third failed test case that executes it and so on this principle is also applied to the contribution provided by successful test cases that execute the piece of code tool gdebug was implemented to automate the computation of the suspiciousness of the code and the subsequent prioritization of suspicious code for locating program faults to validate our method case studies were performed on six sets of programs siemens suite unix suite space grep gzip and make data collected from the studies are supportive of the above claim and also suggest heuristics iii and of our method can effectively reduce the effort spent on fault localization
in order to achieve high performance wide issue superscalar processors have to fetch large number of instructions per cycle conditional branches are the primary impediment to increasing the fetch bandwidth because they can potentially alter the flow of control and are very frequent to overcome this problem these processors need to predict the outcome of multiple branches in cycle this paper investigates two control flow prediction schemes that predict the effective outcome of multiple branches with the help of single prediction instead of considering branches as the basic units of prediction these schemes consider subgraphs of the control flow graph of the executed program as the basic units of prediction and predict the target of an entire subgraph at time thereby allowing the superscalar fetch mechanism to go past multiple branches in cycle the first control flow prediction scheme investigated considers sequential block like subgraphs and the second scheme considers tree like subgraphs to make the control flow predictions both schemes do out of prediction as opposed to the out of prediction done by branch level prediction schemes these two schemes are evaluated using mips isa based way superscalar microarchitecture an improvement in effective fetch size of approximately percent and percent respectively is observed over identical microprocessors that use branch level prediction no appreciable difference in the prediction accuracy was observed although the control flow prediction schemes predicted out of outcomes
wireless mesh networks wmns are considered as cost effective easily deployable and capable of extending internet connectivity however one of the major challenges in deploying reliable wmns is preventing their nodes from malicious attacks which is of particular concern as attacks can severely degrade network performance when dos attack is targeted over an entire communication path it is called path based dos attack we study the performance impact of path based dos attacks by considering attack intensity medium errors physical diversity collusion and hop count we setup wireless mesh testbed and configure set of experiments to gather realistic measurements and assess the effects of different factors we find that medium errors have significant impact on the performance of wmns when path based dos attack is carried out and the impact is exacerbated by the mac layer retransmissions we show that due to physical diversity far attacker can lead to an increased performance degradation than close by attacker additionally we demonstrate that the joint impact of two colluding attackers is not as severe as the joint result of individual attacks we also discuss strategy to counter path based dos attacks which can potentially alleviate the impact of the attack significantly
current cscw applications support one or more modes of cooperative work the selection of and transition between these modes is usually placed on the users at ipsi we built the sepia cooperative hypermedia authoring environment supporting whole range of situations arising during collaborative work and the smooth transitions between them while early use of the system shows the benefits of supporting smooth transitions between different collaborative modes it also reveals some deficits regarding parallel work management of alternative documents or reuse of document parts we propose to integrate version support to overcome these limitations this leads to versioned data management and an extended user interface enabling concurrent users to select certain state of their work to be aware of related changes and to cooperate with others either asynchronously or synchronously
trust between users is an important piece of knowledge that can be exploited in search and recommendationgiven that user supplied trust relationships are usually very sparse we study the prediction of trust relationships using user interaction features in an online user generated review application context we show that trust relationship prediction can achieve better accuracy when one adopts personalized and cluster based classification methods the former trains one classifier for each user using user specific training data the cluster based method first constructs user clusters before training one classifier for each user cluster our proposed methods have been evaluated in series of experiments using two datasets from epinionscom it is shown that the personalized and cluster based classification methods outperform the global classification method particularly for the active users
due to increasing system decentralization component heterogeneity and interface complexities many trustworthiness challenges become more and more complicated and intertwined moreover there is lack of common understanding of software trustworthiness and its related development methodology this paper reports preliminary results from an ongoing collaborative research project among international research units which aims at exploring theories and methods for enhancing existing software process techniques for trustworthy software development the results consist in two parts the proposal of new concept of process trustworthiness as capability indicator to measure the relative degree of confidence for certain software processes to deliver trustworthy software and the introduction of the architecture of trustworthy process management framework tpmf toolkit for process runtime support in measuring and improving process trustworthiness in order to assess and assure software trustworthiness
the research issue of broadcasting has attracted considerable amount of attention in mobile computing system by utilizing broadcast channels server is able to continuously and repeatedly broadcast data to mobile users from these broadcast channels mobile users obtain the data of interest efficiently and only need to wait for the required data to be present on the broadcast channel given the access frequencies of data items one can design proper data allocation in the broadcast channels to reduce the average expected delay of data items in practice the data access frequencies may vary with time we explore in this paper the problem of adjusting broadcast programs to effectively respond to the changes of data access frequencies and develop an efficient algorithm dl to address this problem performance of algorithm dl is analyzed and system simulator is developed to validate our results sensitivity analysis on several parameters including the number of data items the number of broadcast disks and the variation of access frequencies is conducted it is shown by our results that the broadcast programs adjusted by algorithm dl are of very high quality and are in fact very close to the optimal ones
contemporary database applications often perform queries in hybrid data spaces hds where vectors can have mix of continuous valued and non ordered discrete valued dimensions to support efficient query processing for an hds robust indexing method is required existing indexing techniques to process queries efficiently either apply to continuous data spaces eg the tree or non ordered discrete data spaces eg the nd tree no techniques directly indexing vectors in hdss have been reported in the literature in this paper we propose new multidimensional indexing technique called the nd tree to directly index vectors in an hds to build such an index we first introduce some essential geometric concepts eg hybrid bounding rectangle in hdss the nd tree structure and the relevant tree building and query processing algorithms based on these geometric concepts in hdss are then presented strategies have been suggested to make the values in continuous dimensions and non ordered discrete dimensions comparable and controllable novel node splitting heuristics which exploit characteristics of both continuous and discrete dimensions are proposed performance of the nd tree is compared with that of linear scan tree and nd tree using range queries on hybrid data experimental results demonstrate that the nd tree is quite promising in supporting range queries in hdss
it is now well admitted that formal methods are helpful for many issues raised in the web service area in this paper we present framework for the design and the verification of wss using process algebras and their tools we define two way mapping between abstract specifications written using these calculi and executable web services written in bpelws the translation includes also compensation event and fault handlers the following choices are available design and verification in bpelws using process algebra tools or design and verification in process algebra and automatically obtaining the corresponding bpelws code the approaches can be combined process algebras are not useful only for temporal logic verification we remark the use of simulation bisimulation for verification for the hierarchical refinement design method for the service redundancy analysis in community and for replacing service with another one in composition
with the increased use of virtual machines vms as vehicles that isolate applications running on the same host it is necessary to devise techniques that enable multiple vms to share underlying resources both fairly and efficiently to that end one common approach is to deploy complex resource management techniques in the hosting infrastructure alternately in this paper we advocate the use of self adaptation in the vms themselves based on feedback about resource usage and availability consequently we define friendly vm fvm to be virtual machine that adjusts its demand for system resources so that they are both efficiently and fairly allocated to competing fvms such properties are ensured using one of many provably convergent control rules such as additive increase multiplicative decrease aimd by adopting this distributed application based approach to resource management it is not necessary to make assumptions about the underlying resources nor about the requirements of fvms competing for these resources to demonstrate the elegance and simplicity of our approach we present prototype implementation of our fvm framework in user mode linux uml an implementation that consists of less than lines of code changes to uml we present an analytic control theoretic model of fvm adaptation which establishes convergence and fairness properties these properties are also backed up with experimental results using our prototype fvm implementation
with power consumption becoming increasingly critical in interconnected systems power aware networks will become part and parcel of many single chip and multichip systems as communication links consume significant power regardless of utilization mechanism to realize such power aware networks is on off links network links that can be turned on off as function of traffic in this paper we investigate and propose self regulating power aware interconnection networks that turn their links on off in response to bursts and dips in traffic in distributed fashion we explore the design space of such on off networks outlining step design methodology along with various building block solutions at each step that can be effectively assembled to develop various on off network designs we applied our methodology to the design of two classes of on off networks with links that possess substantially different on off delays an on chip network as well as chip to chip network and show that our designs are able to adapt dynamically to variations in network traffic three specific network designs are then constructed presented and evaluated our simulations show that link power consumption can be reduced by up to percent with modest increase in network latency
fraud is serious problem that costs the worldwide economy billions of dollars annually however fraud detection is difficult as perpetrators actively attempt to masquerade their actions among typically overwhelming large volumes of legitimate activity in this paper we investigate the fraud detection problem and examine how learning classifier systems can be applied to it we describe the common properties of fraud introducing an abstract problem which can be tuned to exhibit those characteristics we report experiments on this abstract problem with popular real time learning classifier system algorithm results from our experiments demonstrating that this approach can overcome the difficulties inherent to the fraud detection problem finally we apply the algorithm to real world problem and show that it can achieve good performance in this domain
the predictability of data values is studied at fundamental level two basic predictor models are defined computational predictors perform an operation on previous values to yield predicted next values examples we study are stride value prediction which adds delta to previous value and last value prediction which performs the trivial identity operation on the previous value context based predictors match recent value history context with previous value history and predict values based entirely on previously observed patterns to understand the potential of value prediction we perform simulations with unbounded prediction tables that are immediately updated using correct data values simulations of integer spec benchmarks show that data values can be highly predictable best performance is obtained with context based predictors overall prediction accuracies are between and the context based predictor typically has an accuracy about better than the computational predictors last value and stride comparison of context based prediction and stride prediction shows that the higher accuracy of context based prediction is due to relatively few static instructions giving large improvements this suggests the usefulness of hybrid predictors among different instruction types predictability varies significantly in general load and shift instructions are more difficult to predict correctly whereas add instructions are more predictable
data clustering has been discussed extensively but almost all known conventional clustering algorithms tend to break down in high dimensional spaces because of the inherent sparsity of the data points existing subspace clustering algorithms for handling high dimensional data focus on numerical dimensions in this paper we designed an iterative algorithm called subcad for clustering high dimensional categorical data sets based on the minimization of an objective function for clustering we deduced some cluster memberships changing rules using the objective function we also designed an objective function to determine the subspace associated with each cluster we proved various properties of this objective function that are essential for us to design fast algorithm to find the subspace associated with each cluster finally we carried out some experiments to show the effectiveness of the proposed method and the algorithm
with more and more large networks becoming available mining and querying such networks are increasingly important tasks which are not being supported by database models and querying languages this paper wants to alleviate this situation by proposing data model and query language for facilitating the analysis of networks key features include support for executing external tools on the networks flexible contexts on the network each resulting in different graph primitives for querying subgraphs including paths and transforming graphs the data model provides for closure property in which the output of every query can be stored in the database and used for further querying
large real time software systems such as real time java virtual machines often use barrier protocols which work for dynamically varying number of threads without using centralized locking such barrier protocols however still suffer from priority inversion similar to centralized locking we introduce gang priority management as generic solution for avoiding unbounded priority inversion in barrier protocols our approach is either kernel assisted for efficiency or library based for portability but involves cooperation from the protocol designer for generality we implemented gang priority management in the linux kernel and rewrote the garbage collection safe point barrier protocol in ibm's websphere real time java virtual machine to exploit it we run experiments on an way smp machine in multi user and multi process environment and show that by avoiding unbounded priority inversion the maximum latency to reach barrier point is reduced by factor of and the application jitter is reduced by factor of
achieving intuitive control of animated surface deformation while observing specific style is an important but challenging task in computer graphics solutions to this task can find many applications in data driven skin animation computer puppetry and computer games in this paper we present an intuitive and powerful animation interface to simultaneously control the deformation of large number of local regions on deformable surface with minimal number of control points our method learns suitable deformation subspaces from training examples and generate new deformations on the fly according to the movements of the control points our contributions include novel deformation regression method based on kernel canonical correlation analysis cca and poisson based translation solving technique for easy and fast deformation control based on examples our run time algorithm can be implemented on gpus and can achieve few hundred frames per second even for large datasets with hundreds of training examples
the specification of schema mappings has proved to be time and resource consuming and has been recognized as critical bottleneck to the large scale deployment of data integration systems in an attempt to address this issue dataspaces have been proposed as data management abstraction that aims to reduce the up front cost required to setup data integration system by gradually specifying schema mappings through interaction with end users in pay as you go fashion as step in this direction we explore an approach for incrementally annotating schema mappings using feedback obtained from end users in doing so we do not expect users to examine mapping specifications rather they comment on results to queries evaluated using the mappings using annotations computed on the basis of user feedback we present method for selecting from the set of candidate mappings those to be used for query evaluation considering user requirements in terms of precision and recall in doing so we cast mapping selection as an optimization problem mapping annotations may reveal that the quality of schema mappings is poor we also show how feedback can be used to support the derivation of better quality mappings from existing mappings through refinement an evolutionary algorithm is used to efficiently and effectively explore the large space of mappings that can be obtained through refinement the results of evaluation exercises show the effectiveness of our solution for annotating selecting and refining schema mappings
watermarking algorithms provide way of hiding or embedding some bits of information in watermark in the case of watermarking model many algorithms employ so called indexed localization scheme in this paper we propose an optimization framework with two new steps for such watermarking algorithms to improve their capacity and invisibility the first step is to find an optimal layout of invariant units to improve capacity the second step is to rearrange the correspondence between the watermark units and the invariant units to improve invisibility experimental tests show that by using this framework the capacity and invisibility of watermarking algorithms can be greatly improved
many virtual environments and games must be populated with synthetic characters to create the desired experience these characters must move with sufficient realism so as not to destroy the visual quality of the experience yet be responsive controllable and efficient to simulate in this paper we present an approach to character motion called snap together motion that addresses the unique demands of virtual environments snap together motion stm preprocesses corpus of motion capture examples into set of short clips that can be concatenated to make continuous streams of motion the result process is simple graph structure that facilitates efficient planning of character motions user guided process selects common character poses and the system automatically synthesizes multi way transitions that connect through these poses in this manner well connected graphs can be constructed to suit particular application allowing for practical interactive control without the effort of manually specifying all transitions
as more and more documents become electronically available finding documents in large databases that fit users needs is becoming increasingly important in the past the document search problem was dealt with using the database query approach or the text based search approach in this paper we investigate this problem focusing on the sci ssci databases from isi specifically we design our search methodology based on the four fields commonly seen in scientific research document abstract title keywords and reference list of these four only the abstract field can be viewed as normal text while the other three have their own characteristics to differentiate them from texts therefore we first develop method to compute the similarity value for each field our next problem is combining the four similarity values into final value one approach is to assign weights to each and compute the weighted sum we have not adopted this simple weighting method however because it is difficult to determine appropriate weights instead we use the back propagation neural network to combine them finally extensive experiments have been carried out using real documents drawn from tkde journal and the results indicate that in all situations our method has much higher accuracy than the traditional text based search approach
denial of service dos attacks are arguably one of the most cumbersome problems in the internet this paper presents distributed information system over set of completely connected servers called chameleon which is robust to dos attacks on the nodes as well as the operations of the system in particular it allows nodes to efficiently look up and insert data items at any time despite powerful past insider adversary which has complete knowledge of the system up to some time point and can use that knowledge in order to block constant fraction of the nodes and inject lookup and insert requests to selected data this is achieved with smart randomized replication policy requiring polylogarithmic overhead only and the interplay of permanent and temporary distributed hash table all requests in chameleon can be processed in polylogarithmic time and work at every node
energy consumption is major concern in many embedded computing systems several studies have shown that cache memories account for about of the total energy consumed in these systems the performance of given cache architecture is largely determined by the behavior of the application using that cache desktop systems have to accommodate very wide range of applications and therefore the manufacturer usually sets the cache architecture as compromise given current applications technology and cost unlike desktop systems embedded systems are designed to run small range of well defined applications in this context cache architecture that is tuned for that narrow range of applications can have both increased performance as well as lower energy consumption we introduce novel cache architecture intended for embedded microprocessor platforms the cache can be configured by software to be direct mapped two way or four way set associative using technique we call way concatenation having very little size or performance overhead we show that the proposed cache architecture reduces energy caused by dynamic power compared to way shutdown cache furthermore we extend the cache architecture to also support way shutdown method designed to reduce the energy from static power that is increasing in importance in newer cmos technologies our study of programs drawn from powerstone mediabench and spec show that tuning the cache's configuration saves energy for every program compared to conventional four way set associative as well as direct mapped caches with average savings of compared to four way conventional cache
this paper describes the generation of model capturing information on how placenames co occur together the advantages of the co occurrence model over traditional gazetteers are discussed and the problem of placename disambiguation is presented as case study we begin by outlining the problem of ambiguous placenames we demonstrate how analysis of wikipedia can be used in the generation of co occurrence model the accuracy of our model is compared to handcrafted ground truth then we evaluate alternative methods of applying this model to the disambiguation of placenames in free text using the geoclef evaluation forum we conclude by showing how the inclusion of placenames in both the text and geographic parts of query provides the maximum mean average precision and outline the benefits of co occurrence model as data source for the wider field of geographic information retrieval gir
with the proliferation of mobile streaming multimedia available battery capacity constrains the end user experience since streaming applications are expected to be long running wireless network interface card's wnic energy consumption is particularly an acute problem in this work we explore various mechanisms to conserve client wnic energy consumption for popular streaming formats such as microsoft windows media real and apple quicktime first we investigate the wnic energy consumption characteristics for these popular multimedia streaming formats under varying stream bandwidth and network loss rates we show that even for high bandwidth kbps stream the wnic unnecessarily spent over of the time in idle state illustrating the potential for significant energy savingsbased on these observations we explore two mechanisms to conserve the client wnic energy consumption first we show the limitations of ieee power saving mode for multimedia streams without an understanding of the stream requirements these scheduled rendezvous mechanisms do not offer any energy savings for multimedia streams over kbps we also develop history based client side strategies to reduce the energy consumed by transitioning the wnics to lower power consuming sleep state we show that streams optimized for kbps can save over in energy consumption with data loss high bandwidth stream kbps can still save in energy consumption with less than data loss we also show that real and quicktime packets are harder to predict at the network level without understanding the packet semantics as the amount of cross traffic generated by other clients that share the same wireless segment increases the potential energy savings from our client side policies deteriorate further our work enables multimedia proxy and server developers to suitably customize the stream to lower client energy consumption
this paper describes an approach of representing shape by using set of invariant spherical harmonic sh coefficients after conformal mapping specifically genus zero mesh object is first conformally mapped onto the unit sphere by using modified discrete conformal mapping where the modification is based on mobius factorization and aims at obtaining canonical conformal mapping then sh analysis is applied to the resulting conformal spherical mesh the obtained sh coefficients are further made invariant to translation and rotation while at the same time retain the completeness thanks to which the original shape information has been faithfully preserved
active harmony is an automated runtime performance tuning system in this paper we describe parameter prioritizing tool to help focus on those parameters that are performance critical historical data is also utilized to further speed up the tuning process we first verify our proposed approaches with synthetic data and finally we verify all the improvements on real cluster based web service system taken together these changes allow the active harmony system to reduce the time spent tuning from up to and at the same time reduce the variation in performance while tuning
an overwhelming volume of news videos from different channels and languages is available today which demands automatic management of this abundant information to effectively search retrieve browse and track cross lingual news stories news story similarity measure plays critical role in assessing the novelty and redundancy among them in this paper we explore the novelty and redundancy detection with visual duplicates and speech transcripts for cross lingual news stories news stories are represented by sequence of keyframes in the visual track and set of words extracted from speech transcript in the audio track major difference to pure text documents is that the number of keyframes in one story is relatively small compared to the number of words and there exist large number of non near duplicate keyframes these features make the behavior of similarity measures different compared to traditional textual collections furthermore the textual features and visual features complement each other for news stories they can be further combined to boost the performance experiments on the trecvid cross lingual news video corpus show that approaches on textual features and visual features demonstrate different performance and measures on visual features are quite effective overall the cosine distance on keyframes is still robust measure language models built on visual features demonstrate promising performance the fusion of textual and visual features improves overall performance
the ready availability of online source code examples has fundamentally changed programming practices however current search tools are not designed to assist with programming tasks and are wholly separate from editing tools this paper proposes that embedding task specific search engine in the development environment can significantly reduce the cost of finding information and thus enable programmers to write better code more easily this paper describes the design implementation and evaluation of blueprint web search interface integrated into the adobe flex builder development environment that helps users locate example code blueprint automatically augments queries with code context presents code centric view of search results embeds the search experience into the editor and retains link between copied code and its source comparative laboratory study found that blueprint enables participants to write significantly better code and find example code significantly faster than with standard web browser analysis of three months of usage logs with users suggests that task specific search interfaces can significantly change how and when people search the web
efforts to improve application reliability can be irrelevant if the reliability of the underlying operating system on which the application resides is not seriously considered an important first step in improving the reliability of an operating system is to gain insights into why and how the bugs originate contributions of the different modules to the bugs their distribution across severities the different ways in which the bugs may be resolved and the impact of bug severities on their resolution times to acquire this insight we conducted an extensive analysis of the publicly available bug data on the linux kernel over period of seven years we also justify and explain the statistical bug occurrence trends observed from the data using the architecture of the linux kernel as an anchor the statistical analysis of the linux bug data suggests that the linux kernel may draw significant benefits from the continual reliability improvement efforts of its developers these efforts however are disproportionately targeted towards popular configurations and hardware platforms due to which the reliability of these configurations may be better than those that are not commonly used thus key finding of our study is that it may be prudent to restrict to using common configurations and platforms when using open source systems such as linux in applications with stringent reliability expectations finally our study of the architectural properties of the bugs suggests that the dependence among the modules rather than the unreliabilities of the individual modules is the primary cause of the bugs and their impact on system reliability
major obstacle in the technology transfer agenda of behavioral analysis and design methods is the need for logics or automata to express properties for control intensive systems interaction modeling notations may offer replacement or complement with practitioner appealing and lightweight flavor due partly to the subspecification of intended behavior by means of scenarios we propose novel approach consisting of engineering new formal notation of this sort based on simple compact declarative semantics vts visual timed event scenarios scenarios represent event patterns graphically depicting conditions over traces they predicate general system events and provide features to describe complex properties not expressible with msc like notations the underlying formalism supports partial orders and real time constraints the problem of checking whether timed automaton model has matching trace is proven decidable on top of this kernel we introduce notation to state properties over all system traces conditional scenarios allowing engineers to describe uniquely rich connections between antecedent and consequent portions of the scenario an undecidability result is presented for the general case of the model checking problem over dense time domains to later identify decidable yet practically relevant subclass where verification is solvable by generating antiscenarios expressed in the vts hbox rm kernel notation
data generalization is widely used to protect identities and prevent inference of sensitive information during the public release of microdata the anonymity model has been extensively applied in this context the model seeks generalization scheme such that every individual becomes indistinguishable from at least other individuals and the loss in information while doing so is kept at minimum the search is performed on domain hierarchy lattice where every node is vector signifying the level of generalization for each attribute an effort to understand privacy and data utility trade offs will require knowing the minimum possible information losses of every possible value of however this can easily lead to an exhaustive evaluation of all nodes in the hierarchy lattice in this paper we propose using the concept of pareto optimality to obtain the desired trade off information pareto optimal generalization is one in which no other generalization can provide higher value of without increasing the information loss we introduce the pareto optimal anonymization poka algorithm to traverse the hierarchy lattice and show that the number of node evaluations required to find the pareto optimal generalizations can be significantly reduced results on benchmark data set show that the algorithm is capable of identifying all pareto optimal nodes by evaluating only of nodes in the lattice
distributed simulation techniques are commonly used to improve the speed and scalability of wireless sensor network simulators however accurate simulations of dynamic interactions of sensor network applications incur large synchronization overheads and severely limit the performance of existing distributed simulators in this paper we present two novel techniques that significantly reduce such overheads by minimizing the number of sensor node synchronizations during simulations these techniques work by exploiting radio and mac specific characteristics without reducing simulation accuracy in addition we present new probing mechanism that makes it possible to exploit any potential application specific characteristics for synchronization reductions we implement and evaluate these techniques in cycle accurate distributed simulation framework that we developed based on avrora in our experiments the radio level technique achieves speedup of to times in simulating hop networks with to nodes with default backoffs themac level technique achieves speedup of to times in the best case scenarios of simulating and nodes in our multi hop flooding tests together they achieve speedup of to times in simulating networks with to nodes the experiments also demonstrate that the speedups can be significantly larger as the techniques scale with the number of processors and radio off mac backoff time
an important problem in software engineering is the automated discovery of noncrashing occasional bugs in this work we address this problem and show that mining of weighted call graphs of program executions is promising technique we mine weighted graphs with combination of structural and numerical techniques more specifically we propose novel reduction technique for call graphs which introduces edge weights then we present an analysis technique for such weighted call graphs based on graph mining and on traditional feature selection schemes the technique generalises previous graph mining approaches as it allows for an analysis of weights our evaluation shows that our approach finds bugs which previous approaches cannot detect so far our technique also doubles the precision of finding bugs which existing techniques can already localise in principle
this article presents new technique adaptive replication for automatically eliminating synchronization bottlenecks in multithreaded programs that perform atomic operations on objects synchronization bottlenecks occur when multiple threads attempt to concurrently update the same object it is often possible to eliminate synchronization bottlenecks by replicating objects each thread can then update its own local replica without synchronization and without interacting with other threads when the computation needs to access the original object it combines the replicas to produce the correct values in the original object one potential problem is that eagerly replicating all objects may lead to performance degradation and excessive memory consumptionadaptive replication eliminates unnecessary replication by dynamically detecting contention at each object to find and replicate only those objects that would otherwise cause synchronization bottlenecks we have implemented adaptive replication in the context of parallelizing compiler for subset of given an unannotated sequential program written in the compiler automatically extracts the concurrency determines when it is legal to apply adaptive replication and generates parallel code that uses adaptive replication to efficiently eliminate synchronization bottlenecksin addition to automatic parallelization and adaptive replication our compiler also implements lock coarsening transformation that increases the granularity at which the computation locks objects the advantage is reduction in the frequency with which the computation acquires and releases locks the potential disadvantage is the introduction of new synchronization bottlenecks caused by increases in the sizes of the critical sections because the adaptive replication transformation takes place at lock acquisition sites there is synergistic interaction between lock coarsening and adaptive replication lock coarsening drives down the overhead of using adaptive replication and adaptive replication eliminates synchronization bottlenecks associated with the overaggressive use of lock coarseningour experimental results show that for our set of benchmark programs the combination of lock coarsening and adaptive replication can eliminate synchronization bottlenecks and significantly reduce the synchronization and replication overhead as compared to versions that use none or only one of the transformations
in this paper we investigate reduced representations for the emerging cube we use the borders classical in data mining for the emerging cube these borders can support classification tasks to know whether trend is emerging or not however the borders do not make possible to retrieve the measure values this is why we introduce two new and reduced representations without measure loss the emerging closed cube and emerging quotient cube we state the relationship between the introduced representations experiments performed on various data sets are intended to measure the size of the three reduced representations
even great efforts have been made for decades the recognition of human activities is still an unmature technology that attracted plenty of people in computer vision in this paper system framework is presented to recognize multiple kinds of activities from videos by an svm multi class classifier with binary tree architecture the framework is composed of three functionally cascaded modules detecting and locating people by non parameter background subtraction approach extracting various of features such as local ones from the minimum bounding boxes of human blobs in each frames and newly defined global one contour coding of the motion energy image ccmei and recognizing activities of people by svm multi class classifier whose structure is determined by clustering process the thought of hierarchical classification is introduced and multiple svms are aggregated to accomplish the recognition of actions each svm in the multi class classifier is trained separately to achieve its best classification performance by choosing proper features before they are aggregated experimental results both on home brewed activity data set and the public schuldt's data set show the perfect identification performance and high robustness of the system
the ability to walk up to any computer personalize it and use it as one's own has long been goal of mobile computing research we present soulpad new approach based on carrying an auto configuring operating system along with suspended virtual machine on small portable device with this approach the computer boots from the device and resumes the virtual machine thus giving the user access to his personal environment including previously running computations soulpad has minimal infrastructure requirements and is therefore applicable to wide range of conditions particularly in developing countries we report our experience implementing soulpad and using it on variety of hardware configurations we address challenges common to systems similar to soulpad and show that the soulpad model has significant potential as mobility solution
data races occur when multiple threads are about to access the same piece of memory and at least one of those accesses is write such races can lead to hard to reproduce bugs that are time consuming to debug and fix we present relay static and scalable race detection analysis in which unsoundness is modularized to few sources we describe the analysis and results from our experiments using relay to find data races in the linux kernel which includes about million lines of code
one popular approach to object design proposes to identify responsibilities from software contracts apply number of principles to assign them to objects and finally construct an object interaction that realizes the contract this three step activity is currently manual process that is time consuming and error prone and is among the most challenging activities in object oriented development in this paper we present model transformation that partially automates this activity such transformation is modularized in three stages the first stage automatically transforms software contract to trace of state modification actions in the second stage the designer manually extends the trace with design decisions finally the extended trace is automatically transformed to an object interaction in the third stage prototype of the whole transformation was developed and successfully applied to case study from the literature our technique allows the extraction of valuable information from software contracts provides bridge between analysis and design artifacts and significantly reduces the effort of interaction design
operational transformation ot is an optimistic concurrency control method that has been well established in realtime group editors and has drawn significant research attention in the past decade it is generally believed that the use of ot automatically achieves high local responsiveness in group editors however no performance study has been reported previously on ot algorithms to the best of our knowledge this paper extends recent ot algorithm and studies its performance by theoretical analyses and performance experiments this paper proves that the worst case execution time of ot only appears in rare cases and shows that local responsiveness of ot based group editors in fact depends on number of factors such as the size of the operation log the paper also reveals that these two results have general implications on ot algorithms and hence the design of ot based group editors must pay attention to performance issues
addressed in this paper is the issue of email data cleaning for text mining many text mining applications need take emails as input email data is usually noisy and thus it is necessary to clean it before mining several products offer email cleaning features however the types of noises that can be eliminated are restricted despite the importance of the problem email cleaning has received little attention in the research community thorough and systematic investigation on the issue is thus needed in this paper email cleaning is formalized as problem of non text filtering and text normalization in this way email cleaning becomes independent from any specific text mining processing cascaded approach is proposed which cleans up an email in four passes including non text filtering paragraph normalization sentence normalization and word normalization as far as we know non text filtering and paragraph normalization have not been investigated previously methods for performing the tasks on the basis of support vector machines svm have also been proposed in this paper features in the models have been defined experimental results indicate that the proposed svm based methods can significantly outperform the baseline methods for email cleaning the proposed method has been applied to term extraction typical text mining processing experimental results show that the accuracy of term extraction can be significantly improved by using the data cleaning method
due to the reliance on the textual information associated with an image image search engines on the web lack the discriminative power to deliver visually diverse search results the textual descriptions are key to retrieve relevant results for given user query but at the same time provide little information about the rich image content in this paper we investigate three methods for visual diversification of image search results the methods deploy lightweight clustering techniques in combination with dynamic weighting function of the visual features to best capture the discriminative aspects of the resulting set of images that is retrieved representative image is selected from each cluster which together form diverse result set based on performance evaluation we find that the outcome of the methods closely resembles human perception of diversity which was established in an extensive clustering experiment carried out by human assessors
inspired by the surprising discovery of several recurring structures in various complex networks in recent years number of related works treated software systems as complex network and found that software systems might expose the small world effects and follow scale free degree distributions different from the research perspectives adopted in these works the work presented in this paper treats software execution processes as an evolving complex network for the first time the concept of software mirror graph is introduced as new model of complex networks to incorporate the dynamic information of software behavior the experimentation paradigm with statistical repeatability was applied to three distinct subject programs to conduct several software experiments the corresponding experimental results are analyzed by treating the software execution processes as an evolving directed topological graph as well as an evolving software mirror graph this results in several new findings while the software execution processes may demonstrate as small world complex network in the topological sense they no longer expose the small world effects in the temporal sense further the degree distributions of the software execution processes may follow power law however they may also follow an exponential function or piecewise power law
we overview the development of first order automated reasoning systems starting from their early years based on the analysis of current and potential applications of such systems we also try to predict new trends in first order automated reasoning our presentation will be centered around two main motives efficiency and usefulness for existing and future potential applications
dynamic slicing algorithms have been considered to aid in debugging for many years however as far as we know no detailed studies on evaluating the benefits of using dynamic slicing for locating real faults present in programs have been carried out in this paper we study the effectiveness of fault location using dynamic slicing for set of real bugs reported in some widely used software programs our results show that of the faults studied faults were captured by data slices required the use of full slices and none of them required the use of relevant slices moreover it was observed that dynamic slicing considerably reduced the subset of program statements that needed to be examined to locate faulty statements interestingly we observed that all of the memory bugs in the faulty versions were captured by data slices the dynamic slices that captured faulty code included to of statements that were executed at least once
video on demand vod service offers large selection of videos from which customers can choose designers of vod systems strive to achieve low access latency for customers one approach that has been investigated by several researchers allows the server to batch clients requesting the same video and to serve clients in the same batch with one multicast video stream this approach has the advantage that it can save server resources as well as server access and network bandwidth thus allowing the server to handle large number of customers without sacrificing access latency vod server replication is another approach that can allow vod service to handle large number of clients albeit at the additional cost of providing more servers while replication is an effective way to increase the service capacity it needs to be coupled with appropriate selection techniques in order to make efficient use of the increased capacity in this paper we investigate the design of server selection techniques for system of replicated batching vod servers we design and evaluate range of selection algorithms as they would be applied to three batching approaches batching with persistent channel allocation patching and hierarchical multicast stream merging hmsm we demonstrate that server replication combined with appropriate server selection scheme can indeed be used to increase the capacity of the service leading to improved performance
this work addresses data warehouse maintenance ie how changes to autonomous heterogeneous and distributed sources should be detected and propagated to warehouse the research community has mainly addressed issues relating to the internal operation of data warehouse servers work related to data warehouse maintenance has received less attention and only limited set of maintenance alternatives are considered while ignoring the autonomy and heterogeneity of sourcesin this paper we extend work on single source view maintenance to views with multiple heterogeneous sources we present tool pam which allows for comparison of large number of relevant maintenance policies under different configurations based on such analysis and previous studies we propose set of heuristics to guide in policy selection the quality of these heuristics is evaluated empirically using test bed developed for this purpose this is done for number of different criteria and for different data sources and computer systems the performance gained using the policy selected through the heuristics is compared with the performance of all identified policies based on these experiments we claim that heuristic based selections are good
massive data streams are now fundamental to many data processing applications for example internet routers produce large scale diagnostic data streams such streams are rarely stored in traditional databases and instead must be processed on the fly as they are produced similarly sensor networks produce multiple data streams of observations from their sensors there is growing focus on manipulating data streams and hence there is need to identify basic operations of interest in managing data streams and to support them efficiently we propose computation of the hamming norm as basic operation of interest the hamming norm formalises ideas that are used throughout data processing when applied to single stream the hamming norm gives the number of distinct items that are present in that data stream which is statistic of great interest in databases when applied to pair of streams the hamming norm gives an important measure of dis similarity the number of unequal item counts in the two streams hamming norms have many uses in comparing data streams we present novel approximation technique for estimating the hamming norm for massive data streams this relies on what we call the sketch and we prove its accuracy we test our approximation method on large quantity of synthetic and real stream data and show that the estimation is accurate to within few percentage points
we present framework for specification and security analysis of communication protocols for mobile wireless networks this setting introduces new challenges which are not being addressed by classical protocol analysis techniques the main complication stems from the fact that the actions of intermediate nodes and their connectivity can no longer be abstracted into single unstructured adversarial environment as they form an inherent part of the system's security in order to model this scenario faithfully we present broadcast calculus which makes clear distinction between the protocol processes and the network's connectivity graph which may change independently from protocol actions we identify property characterising an important aspect of security in this setting and express it using behavioural equivalences of the calculus we complement this approach with control flow analysis which enables us to automatically check this property on given network and attacker specification
characterizing the communication behavior of large scale applications is difficult and costly task due to code system complexity and long execution times while many tools to study this behavior have been developed these approaches either aggregate information in lossy way through high level statistics or produce huge trace files that are hard to handle we contribute an approach that provides orders of magnitude smaller if not near constant size communication traces regardless of the number of nodes while preserving structural information we introduce intra and inter node compression techniques of mpi events that are capable of extracting an application's communication structure we further present replay mechanism for the traces generated by our approach and discuss results of our implementation for bluegene given this novel capability we discuss its impact on communication tuning and beyond to the best of our knowledge such concise representation of mpi traces in scalable manner combined with deterministic mpi call replay is without any precedent
over the last decade feature creep and the convergence of multiple devices have increased the complexity of both design and use one way to reduce the complexity of device without sacrificing its features is to design the ui consistently however designing consistent user interface of multifunction device often becomes formidable task especially when the logical interaction is concerned this paper presents systematic method for consistent design of user interaction called cuid consistent user interaction design and validates its usefulness through case study cuid focusing on ensuring consistency of logical interaction rather than physical or visual interfaces employs constraint based interactive approach it strives for consistency as the main goal but also considers efficiency and safety of use cuid will reduce the cognitive complexity of the task of interaction design to help produce devices that are easier to learn and use
the web graph is giant social network whose properties have been measured and modeled extensively in recent years most such studies concentrate on the graph structure alone and do not consider textual properties of the nodes consequently web communities have been characterized purely in terms of graph structure and not on page content we propose that topic taxonomy such as yahoo or the open directory provides useful framework for understanding the structure of content based clusters and communities in particular using topic taxonomy and an automatic classifier we can measure the background distribution of broad topics on the web and analyze the capability of recent random walk algorithms to draw samples which follow such distributions in addition we can measure the probability that page about one broad topic will link to another broad topic extending this experiment we can measure how quickly topic context is lost while walking randomly on the web graph estimates of this topic mixing distance may explain why global pagerank is still meaningful in the context of broad queries in general our measurements may prove valuable in the design of community specific crawlers and link based ranking systems
current techniques and tools for automated termination analysis of term rewrite systems trss are already very powerful however they fail for algorithms whose termination is essentially due to an inductive argument therefore we show how to couple the dependency pair method for trs termination with inductive theorem proving as confirmed by the implementation of our new approach in the tool aprove now trs termination techniques are also successful on this important class of algorithms
we present freeform modeling framework for unstructured triangle meshes which is based on constraint shape optimization the goal is to simplify the user interaction even for quite complex freeform or multiresolution modifications the user first sets various boundary constraints to define custom tailored abstract basis function which is adjusted to given design task the actual modification is then controlled by moving one single dof manipulator object the technique can handle arbitrary support regions and piecewise boundary conditions with smoothness ranging continuously from to to more naturally adapt the modification to the shape of the support region the deformed surface can be tuned to bend with anisotropic stiffness we are able to achieve real time response in an interactive design session even for complex meshes by precomputing set of scalar valued basis functions that correspond to the degrees of freedom of the manipulator by which the user controls the modification
balancing peer to peer graphs including zone size distributions has recently become an important topic of peer to peer pp research to bring analytical understanding into the various peer join mechanisms we study how zone balancing decisions made during the initial sampling of the peer space affect the resulting zone sizes and derive several asymptotic results for the maximum and minimum zone sizes that hold with high probability
